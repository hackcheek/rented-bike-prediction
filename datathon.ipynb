{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6_ZRx_AKhSyI",
        "vl9EKQJyGI5V",
        "sVmJpNHfn07-",
        "S3pdheo9nZKs",
        "rzarbbifnfQO",
        "ddPVwn4BpjeH",
        "LuDN4AF3rjdT",
        "Ll-NnmiLq2L5",
        "PkklKNWOOCql",
        "uM3cds3yPoE3",
        "--w4kfvNQCyT",
        "qnqTyCr5QxZW",
        "5LvbmhTfuQL7",
        "jKfTPWjX6f6o"
      ],
      "authorship_tag": "ABX9TyOoRwFXNu7rQBYSCWi02RmH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hackcheek/rented-bike-prediction/blob/main/datathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import xgboost as xgb\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "metadata": {
        "id": "-pCa0V_VhQp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract data**"
      ],
      "metadata": {
        "id": "beX_WBdwhBr2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afqdnSUcaPU-",
        "outputId": "1a1d9732-a9b3-4547-8692-6009c879ad3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rented-bike-prediction'...\n",
            "remote: Enumerating objects: 130, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 130 (delta 8), reused 33 (delta 5), pack-reused 89\u001b[K\n",
            "Receiving objects: 100% (130/130), 32.71 MiB | 12.69 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://@github.com/hackcheek/rented-bike-prediction.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "72yQTNzrYHmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "E_data_train = pd.read_excel('rented-bike-prediction/bike_train.xlsx')\n",
        "E_data_test = pd.read_excel('rented-bike-prediction/bike_test.xlsx')"
      ],
      "metadata": {
        "id": "CT27Vjqsh30W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E_data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "lHe4Jpflu37r",
        "outputId": "8df004ae-5748-47c3-bfd0-514e366ed8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       instant     dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
              "0            1 2011-01-01       1   0     1   0        0        6           0   \n",
              "1            2 2011-01-01       1   0     1   1        0        6           0   \n",
              "2            3 2011-01-01       1   0     1   2        0        6           0   \n",
              "3            4 2011-01-01       1   0     1   3        0        6           0   \n",
              "4            5 2011-01-01       1   0     1   4        0        6           0   \n",
              "...        ...        ...     ...  ..   ...  ..      ...      ...         ...   \n",
              "11994    11995 2012-05-19       2   1     5  23        0        6           0   \n",
              "11995    11996 2012-05-20       2   1     5   0        0        0           0   \n",
              "11996    11997 2012-05-20       2   1     5   1        0        0           0   \n",
              "11997    11998 2012-05-20       2   1     5   2        0        0           0   \n",
              "11998    11999 2012-05-20       2   1     5   3        0        0           0   \n",
              "\n",
              "       weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  dy  \n",
              "0               1  0.24  0.2879  0.81     0.0000       3          13   16   1  \n",
              "1               1  0.22  0.2727  0.80     0.0000       8          32   40   1  \n",
              "2               1  0.22  0.2727  0.80     0.0000       5          27   32   1  \n",
              "3               1  0.24  0.2879  0.75     0.0000       3          10   13   1  \n",
              "4               1  0.24  0.2879  0.75     0.0000       0           1    1   1  \n",
              "...           ...   ...     ...   ...        ...     ...         ...  ...  ..  \n",
              "11994           1  0.60  0.6212  0.56     0.1642      71         168  239  19  \n",
              "11995           1  0.58  0.5455  0.53     0.1045      42         128  170  20  \n",
              "11996           1  0.56  0.5303  0.52     0.0000      28         102  130  20  \n",
              "11997           1  0.56  0.5303  0.52     0.0000      36          62   98  20  \n",
              "11998           1  0.54  0.5152  0.56     0.0896      26          40   66  20  \n",
              "\n",
              "[11999 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b5a75b8-9728-4b9e-a719-b96321fe0145\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "      <th>dy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11994</th>\n",
              "      <td>11995</td>\n",
              "      <td>2012-05-19</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.6212</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>71</td>\n",
              "      <td>168</td>\n",
              "      <td>239</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11995</th>\n",
              "      <td>11996</td>\n",
              "      <td>2012-05-20</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.5455</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.1045</td>\n",
              "      <td>42</td>\n",
              "      <td>128</td>\n",
              "      <td>170</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11996</th>\n",
              "      <td>11997</td>\n",
              "      <td>2012-05-20</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>28</td>\n",
              "      <td>102</td>\n",
              "      <td>130</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11997</th>\n",
              "      <td>11998</td>\n",
              "      <td>2012-05-20</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>36</td>\n",
              "      <td>62</td>\n",
              "      <td>98</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11998</th>\n",
              "      <td>11999</td>\n",
              "      <td>2012-05-20</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.5152</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.0896</td>\n",
              "      <td>26</td>\n",
              "      <td>40</td>\n",
              "      <td>66</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11999 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b5a75b8-9728-4b9e-a719-b96321fe0145')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b5a75b8-9728-4b9e-a719-b96321fe0145 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b5a75b8-9728-4b9e-a719-b96321fe0145');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(data=lambda x: x.copy(), n_components=None, no_consider=list()) -> pd.DataFrame:\n",
        "    \n",
        "    data['dy'] = data.dteday.dt.day\n",
        "\n",
        "    _features = {\n",
        "       'yr', \n",
        "       'mnth', \n",
        "       'dy',\n",
        "       'hr', \n",
        "       'weekday',\n",
        "       'temp', \n",
        "       'atemp', \n",
        "       'hum', \n",
        "       'windspeed',\n",
        "    }\n",
        "    _categorical = {\n",
        "       'season', \n",
        "       'holiday', \n",
        "       'workingday', \n",
        "       'weathersit', \n",
        "        \n",
        "    }\n",
        "    features = list(_features.difference(no_consider))\n",
        "    categorical = list(_categorical.difference(no_consider))\n",
        "\n",
        "    # Get standarscaler \n",
        "\n",
        "    if n_components == 'no':\n",
        "        data[list(_features)] = StandardScaler().fit_transform(data[list(_features)])\n",
        "        data = data.astype({k:'int' for k in _categorical})\n",
        "        all_feat = list(_features) + ['cnt'] if 'cnt' in data.columns else list(_features)\n",
        "        return data[all_feat + list(_categorical)]\n",
        "    else:\n",
        "        # X = StandardScaler().fit_transform(data[features])\n",
        "        X = MinMaxScaler().fit_transform(data[features])\n",
        "        data = data.astype({k:'int' for k in _categorical})\n",
        "        X = pd.concat(X, data, axis=1)\n",
        "        \n",
        "\n",
        "    if no_consider:\n",
        "        data[no_consider] = StandardScaler().fit_transform(data[no_consider])\n",
        "\n",
        "\n",
        "    # Data reductor with PCA\n",
        "    components = n_components if n_components else int(len(features) / 2)\n",
        "    pca = PCA(n_components=components)\n",
        "    principal_components = pca.fit_transform(X)\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        principal_components,\n",
        "        columns=[f'component {i}' for i in range(1, components+1)]\n",
        "    )\n",
        "    for i in no_consider:\n",
        "        df[i] = data[i]\n",
        "\n",
        "    # Add cnt for training data\n",
        "    if 'cnt' in data.columns:\n",
        "        df['cnt'] = data['cnt']\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# data_train = preprocessing(E_data_train, no_consider=['hr', 'temp', 'atemp', 'hum'])\n",
        "# data_test = preprocessing(E_data_test, no_consider=['hr', 'temp', 'atemp', 'hum'])\n",
        "\n",
        "data_train = preprocessing(E_data_train, n_components='no')\n",
        "data_test = preprocessing(E_data_test, n_components='no')"
      ],
      "metadata": {
        "id": "ts_EjffApwPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dylrb8GbtuTF",
        "outputId": "ba033dd6-0db8-4bfe-df4a-5c57268df5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11999 entries, 0 to 11998\n",
            "Data columns (total 14 columns):\n",
            " #   Column      Non-Null Count  Dtype   \n",
            "---  ------      --------------  -----   \n",
            " 0   hr          11999 non-null  float64 \n",
            " 1   mnth        11999 non-null  float64 \n",
            " 2   dy          11999 non-null  float64 \n",
            " 3   atemp       11999 non-null  float64 \n",
            " 4   yr          11999 non-null  float64 \n",
            " 5   windspeed   11999 non-null  float64 \n",
            " 6   hum         11999 non-null  float64 \n",
            " 7   weekday     11999 non-null  float64 \n",
            " 8   temp        11999 non-null  float64 \n",
            " 9   cnt         11999 non-null  int64   \n",
            " 10  season      11999 non-null  category\n",
            " 11  workingday  11999 non-null  category\n",
            " 12  holiday     11999 non-null  category\n",
            " 13  weathersit  11999 non-null  category\n",
            "dtypes: category(4), float64(9), int64(1)\n",
            "memory usage: 985.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "OGdGaMTW2qI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deploy(model):\n",
        "    model.fit(data_train.drop('cnt', axis=1), data_train['cnt'])\n",
        "    pred = model.predict(data_test)\n",
        "    pred = np.array(pred).tolist()\n",
        "    print(pred)\n",
        "    \n",
        "    pd.DataFrame({'pred': pred}).to_csv('hackcheek.csv', index=False)"
      ],
      "metadata": {
        "id": "-KwUdKuDQD61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide metrics for scoring\n",
        "def rmse(y_test, pred):\n",
        "    return mean_squared_error(y_test, pred, squared=False)\n",
        "    \n",
        "\n",
        "def r2(y_test, pred):\n",
        "    return r2_score(y_test, pred)\n",
        "\n",
        "\n",
        "def grid_scorer():\n",
        "    return make_scorer(r2, greater_is_better=True)\n",
        "\n",
        "\n",
        "def evaluate(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    RMSE = rmse(y_test, predictions)\n",
        "    R2 = r2(y_test, predictions)\n",
        "    print('Model Performance')\n",
        "    print(f'RMSE: {RMSE}')\n",
        "    print(f'R2: {R2}')"
      ],
      "metadata": {
        "id": "4hpDkWWcIh1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning"
      ],
      "metadata": {
        "id": "6_ZRx_AKhSyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tran test split and get features and target on torch"
      ],
      "metadata": {
        "id": "RBXkQC6jntTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop('cnt', axis=1).to_numpy()\n",
        "y = data_train['cnt'].to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "features = torch.from_numpy(X_train)\n",
        "target = torch.from_numpy(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "bYUWG_4zmckc",
        "outputId": "9aa8c59b-48fc-4bc2-8142-610cc92b10cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-074b4b1f11ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cnt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron for regression.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(13, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      Forward pass\n",
        "    '''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "kXBvnSdDTRFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "vl9EKQJyGI5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop('cnt', axis=1)\n",
        "y = data_train['cnt']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "features = X_train\n",
        "target = y_train"
      ],
      "metadata": {
        "id": "zSgzZcG0H-ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing depths"
      ],
      "metadata": {
        "id": "sVmJpNHfn07-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing depths -> 17\n",
        "for d in range(1, 21):\n",
        "    RF = RandomForestRegressor(n_estimators=100, max_depth=d, random_state=42, criterion='mse')\n",
        "    RF.fit(features, target)\n",
        "    print(f'\\nDEPTH {d}\\n')\n",
        "    evaluate(RF, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIoqRr7zZz2E",
        "outputId": "eb291f98-2cf6-4116-e097-45d913bce102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 1\n",
            "\n",
            "Model Performance\n",
            "RMSE: 127.35527165428084\n",
            "R2: 0.3066730100753333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 2\n",
            "\n",
            "Model Performance\n",
            "RMSE: 116.58320102443848\n",
            "R2: 0.41899990308182733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 3\n",
            "\n",
            "Model Performance\n",
            "RMSE: 109.602568975904\n",
            "R2: 0.4864937650878651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 4\n",
            "\n",
            "Model Performance\n",
            "RMSE: 99.82747470575232\n",
            "R2: 0.5740050763642921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 5\n",
            "\n",
            "Model Performance\n",
            "RMSE: 90.61121038133363\n",
            "R2: 0.6490315181060577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 6\n",
            "\n",
            "Model Performance\n",
            "RMSE: 80.15119641618878\n",
            "R2: 0.7253849892214456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 7\n",
            "\n",
            "Model Performance\n",
            "RMSE: 66.73703226048298\n",
            "R2: 0.8096126741941274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 8\n",
            "\n",
            "Model Performance\n",
            "RMSE: 55.96474157079862\n",
            "R2: 0.8661145881107593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 9\n",
            "\n",
            "Model Performance\n",
            "RMSE: 49.211526138709615\n",
            "R2: 0.896476744526575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 10\n",
            "\n",
            "Model Performance\n",
            "RMSE: 44.74076902824292\n",
            "R2: 0.9144320469175038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 11\n",
            "\n",
            "Model Performance\n",
            "RMSE: 41.534511934682\n",
            "R2: 0.9262567155529364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 12\n",
            "\n",
            "Model Performance\n",
            "RMSE: 39.68975074732837\n",
            "R2: 0.9326618785221638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 13\n",
            "\n",
            "Model Performance\n",
            "RMSE: 38.5725754615348\n",
            "R2: 0.9363993537751486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 14\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.98784286219396\n",
            "R2: 0.9383130185298749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 15\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.761366877973984\n",
            "R2: 0.9390463570803332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 16\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.496546550789716\n",
            "R2: 0.9398982945988247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 17\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 18\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.5898049306822\n",
            "R2: 0.9395989626161823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 19\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.72262919286955\n",
            "R2: 0.9391713521265725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEPTH 20\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.68553876894466\n",
            "R2: 0.9392909117230445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing estimators"
      ],
      "metadata": {
        "id": "S3pdheo9nZKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing estimators -> 600\n",
        "for n in np.arange(100, 1001, 100): \n",
        "    RF = RandomForestRegressor(n_estimators=n, max_depth=17, random_state=42, criterion='mse')\n",
        "    RF.fit(features, target)\n",
        "    print(f'\\nESTIMATORS {n}\\n')\n",
        "    evaluate(RF, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz8TxHT2jVyB",
        "outputId": "c5376063-1265-435d-8cdb-0ab26e293163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 100\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 200\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.31330884285308\n",
            "R2: 0.9404842680228137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 300\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.23386814870298\n",
            "R2: 0.9407374183458806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 400\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.11899540395016\n",
            "R2: 0.9411025243299117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 500\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.038098988185595\n",
            "R2: 0.9413589646077298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 600\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.008299691314235\n",
            "R2: 0.9414532868706509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 700\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.02488090200559\n",
            "R2: 0.9414008125409982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 800\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.01664434103828\n",
            "R2: 0.9414268816103668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 900\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.0209181489984\n",
            "R2: 0.9414133555484996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESTIMATORS 1000\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.056431039065984\n",
            "R2: 0.9413009013424591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Min leaf"
      ],
      "metadata": {
        "id": "rzarbbifnfQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing min_samples_leaf -> 1\n",
        "for l in np.arange(1, 10, 1): \n",
        "    RF = RandomForestRegressor(\n",
        "        n_estimators=100, \n",
        "        max_depth=17, \n",
        "        min_samples_leaf=l,\n",
        "        random_state=42, \n",
        "        criterion='mse'\n",
        "    )\n",
        "    RF.fit(features, target)\n",
        "    print(f'\\nMIN LEAF {l}\\n')\n",
        "    evaluate(RF, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGu49QionmCh",
        "outputId": "3a702763-3195-4cbe-f8d1-0d784d8d200f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 1\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 2\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.98390069280931\n",
            "R2: 0.9383258209368289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 3\n",
            "\n",
            "Model Performance\n",
            "RMSE: 38.61736458643734\n",
            "R2: 0.9362515663323397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 4\n",
            "\n",
            "Model Performance\n",
            "RMSE: 39.65982096508599\n",
            "R2: 0.9327633987065305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 5\n",
            "\n",
            "Model Performance\n",
            "RMSE: 40.60461338571866\n",
            "R2: 0.9295217660134533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 6\n",
            "\n",
            "Model Performance\n",
            "RMSE: 41.451836107488866\n",
            "R2: 0.9265500002855224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 7\n",
            "\n",
            "Model Performance\n",
            "RMSE: 42.449062332710746\n",
            "R2: 0.9229734481820501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 8\n",
            "\n",
            "Model Performance\n",
            "RMSE: 43.36652672170715\n",
            "R2: 0.9196078701993899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN LEAF 9\n",
            "\n",
            "Model Performance\n",
            "RMSE: 44.356180910242884\n",
            "R2: 0.9158967950506827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### min_samples_split"
      ],
      "metadata": {
        "id": "ddPVwn4BpjeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing min_samples_leaf -> 1\n",
        "for s in np.arange(2, 20, 1): \n",
        "    RF = RandomForestRegressor(\n",
        "        n_estimators=100, \n",
        "        max_depth=17, \n",
        "        min_samples_leaf=1,\n",
        "        min_samples_split=2,\n",
        "        random_state=42, \n",
        "        criterion='mse'\n",
        "    )\n",
        "    RF.fit(features, target)\n",
        "    print(f'\\nMIN SPLIT {s}\\n')\n",
        "    evaluate(RF, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6L4QsYvpdjS",
        "outputId": "a8996eec-4e3a-45c9-8711-2fdd54cd0359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 2\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 3\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 4\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 5\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 6\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 7\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 8\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 9\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 10\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 11\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 12\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 13\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 14\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 15\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 16\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 17\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 18\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MIN SPLIT 19\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.47877361749996\n",
            "R2: 0.9399552561350482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max features"
      ],
      "metadata": {
        "id": "LuDN4AF3rjdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing max_features -> 10\n",
        "for f in np.arange(1, len(features), 1): \n",
        "    RF = RandomForestRegressor(\n",
        "        n_estimators=600, \n",
        "        max_depth=17, \n",
        "        min_samples_leaf=1,\n",
        "        min_samples_split=2,\n",
        "        max_features=f,\n",
        "        random_state=42, \n",
        "    )\n",
        "    RF.fit(features, target)\n",
        "    print(f'\\nMAX FEATURES {f}\\n')\n",
        "    evaluate(RF, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KOEqNvsBrpox",
        "outputId": "613ca30b-2990-4558-8627-b4fbf3c77d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MAX FEATURES 1\n",
            "\n",
            "Model Performance\n",
            "RMSE: 73.68381677442765\n",
            "R2: 0.7679142468591567\n",
            "\n",
            "MAX FEATURES 2\n",
            "\n",
            "Model Performance\n",
            "RMSE: 64.16421980758679\n",
            "R2: 0.8240091497250878\n",
            "\n",
            "MAX FEATURES 3\n",
            "\n",
            "Model Performance\n",
            "RMSE: 55.43321667803399\n",
            "R2: 0.8686456635710457\n",
            "\n",
            "MAX FEATURES 4\n",
            "\n",
            "Model Performance\n",
            "RMSE: 47.96358368345373\n",
            "R2: 0.9016606115630048\n",
            "\n",
            "MAX FEATURES 5\n",
            "\n",
            "Model Performance\n",
            "RMSE: 42.96657512924211\n",
            "R2: 0.9210838792786424\n",
            "\n",
            "MAX FEATURES 6\n",
            "\n",
            "Model Performance\n",
            "RMSE: 39.62733570242659\n",
            "R2: 0.93287350026848\n",
            "\n",
            "MAX FEATURES 7\n",
            "\n",
            "Model Performance\n",
            "RMSE: 38.036286846503145\n",
            "R2: 0.9381555856044075\n",
            "\n",
            "MAX FEATURES 8\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.01631017467806\n",
            "R2: 0.9414279391388262\n",
            "\n",
            "MAX FEATURES 9\n",
            "\n",
            "Model Performance\n",
            "RMSE: 36.651282210439625\n",
            "R2: 0.9425774335563684\n",
            "\n",
            "MAX FEATURES 10\n",
            "\n",
            "Model Performance\n",
            "RMSE: 36.510118231620325\n",
            "R2: 0.9430189126625063\n",
            "\n",
            "MAX FEATURES 11\n",
            "\n",
            "Model Performance\n",
            "RMSE: 36.73438943826602\n",
            "R2: 0.942316725577341\n",
            "\n",
            "MAX FEATURES 12\n",
            "\n",
            "Model Performance\n",
            "RMSE: 36.66643710962849\n",
            "R2: 0.9425299365484047\n",
            "\n",
            "MAX FEATURES 13\n",
            "\n",
            "Model Performance\n",
            "RMSE: 37.008299691314235\n",
            "R2: 0.9414532868706509\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-185-d0e9dc1b6f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nMAX FEATURES {f}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 )\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m             )\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         )\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_depth must be greater than zero. \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_features must be in (0, n_features]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: max_features must be in (0, n_features]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rest of code"
      ],
      "metadata": {
        "id": "Ll-NnmiLq2L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF_best = RandomForestRegressor(\n",
        "    n_estimators=600, \n",
        "    max_depth=17, \n",
        "    min_samples_leaf=1,\n",
        "    min_samples_split=2,\n",
        "    random_state=42, \n",
        ")\n",
        "\n",
        "RFR = RandomForestRegressor()\n",
        "\n",
        "params_search = dict(\n",
        "    n_estimators = [100, 300, 600],\n",
        "    max_depth = [6, 8, 10, 12, 15, 20],\n",
        "\n",
        ")\n",
        "rand_search = RandomizedSearchCV(\n",
        "    estimator=RFR,\n",
        "    param_distributions=params_search,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_iter=25,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "rand_search.fit(features, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymUhutSPqTGs",
        "outputId": "8fb26c23-9ce9-4181-ec5d-f4f7e344d15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:296: UserWarning:\n",
            "\n",
            "The total space of parameters 18 is smaller than n_iter=25. Running 18 iterations. For exhaustive searches, use GridSearchCV.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV] END ......................max_depth=6, n_estimators=100; total time=   1.7s\n",
            "[CV] END ......................max_depth=6, n_estimators=100; total time=   1.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=100; total time=   1.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=100; total time=   1.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=100; total time=   1.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=300; total time=   4.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=300; total time=   4.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=300; total time=   4.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=300; total time=   4.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=300; total time=   4.5s\n",
            "[CV] END ......................max_depth=6, n_estimators=600; total time=   9.0s\n",
            "[CV] END ......................max_depth=6, n_estimators=600; total time=   8.9s\n",
            "[CV] END ......................max_depth=6, n_estimators=600; total time=   9.7s\n",
            "[CV] END ......................max_depth=6, n_estimators=600; total time=   9.0s\n",
            "[CV] END ......................max_depth=6, n_estimators=600; total time=   9.0s\n",
            "[CV] END ......................max_depth=8, n_estimators=100; total time=   2.0s\n",
            "[CV] END ......................max_depth=8, n_estimators=100; total time=   2.0s\n",
            "[CV] END ......................max_depth=8, n_estimators=100; total time=   2.0s\n",
            "[CV] END ......................max_depth=8, n_estimators=100; total time=   2.0s\n",
            "[CV] END ......................max_depth=8, n_estimators=100; total time=   2.0s\n",
            "[CV] END ......................max_depth=8, n_estimators=300; total time=   6.0s\n",
            "[CV] END ......................max_depth=8, n_estimators=300; total time=   6.0s\n",
            "[CV] END ......................max_depth=8, n_estimators=300; total time=   5.9s\n",
            "[CV] END ......................max_depth=8, n_estimators=300; total time=   5.9s\n",
            "[CV] END ......................max_depth=8, n_estimators=300; total time=   5.9s\n",
            "[CV] END ......................max_depth=8, n_estimators=600; total time=  11.8s\n",
            "[CV] END ......................max_depth=8, n_estimators=600; total time=  11.8s\n",
            "[CV] END ......................max_depth=8, n_estimators=600; total time=  11.8s\n",
            "[CV] END ......................max_depth=8, n_estimators=600; total time=  11.8s\n",
            "[CV] END ......................max_depth=8, n_estimators=600; total time=  11.9s\n",
            "[CV] END .....................max_depth=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END .....................max_depth=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END .....................max_depth=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END .....................max_depth=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END .....................max_depth=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END .....................max_depth=10, n_estimators=300; total time=   7.5s\n",
            "[CV] END .....................max_depth=10, n_estimators=300; total time=   8.2s\n",
            "[CV] END .....................max_depth=10, n_estimators=300; total time=   7.5s\n",
            "[CV] END .....................max_depth=10, n_estimators=300; total time=   7.5s\n",
            "[CV] END .....................max_depth=10, n_estimators=300; total time=   7.4s\n",
            "[CV] END .....................max_depth=10, n_estimators=600; total time=  14.9s\n",
            "[CV] END .....................max_depth=10, n_estimators=600; total time=  14.9s\n",
            "[CV] END .....................max_depth=10, n_estimators=600; total time=  14.8s\n",
            "[CV] END .....................max_depth=10, n_estimators=600; total time=  14.8s\n",
            "[CV] END .....................max_depth=10, n_estimators=600; total time=  14.8s\n",
            "[CV] END .....................max_depth=12, n_estimators=100; total time=   3.0s\n",
            "[CV] END .....................max_depth=12, n_estimators=100; total time=   3.0s\n",
            "[CV] END .....................max_depth=12, n_estimators=100; total time=   3.0s\n",
            "[CV] END .....................max_depth=12, n_estimators=100; total time=   3.0s\n",
            "[CV] END .....................max_depth=12, n_estimators=100; total time=   3.0s\n",
            "[CV] END .....................max_depth=12, n_estimators=300; total time=   8.9s\n",
            "[CV] END .....................max_depth=12, n_estimators=300; total time=   9.0s\n",
            "[CV] END .....................max_depth=12, n_estimators=300; total time=  10.3s\n",
            "[CV] END .....................max_depth=12, n_estimators=300; total time=   8.9s\n",
            "[CV] END .....................max_depth=12, n_estimators=300; total time=   8.9s\n",
            "[CV] END .....................max_depth=12, n_estimators=600; total time=  17.8s\n",
            "[CV] END .....................max_depth=12, n_estimators=600; total time=  17.7s\n",
            "[CV] END .....................max_depth=12, n_estimators=600; total time=  17.7s\n",
            "[CV] END .....................max_depth=12, n_estimators=600; total time=  17.7s\n",
            "[CV] END .....................max_depth=12, n_estimators=600; total time=  17.8s\n",
            "[CV] END .....................max_depth=15, n_estimators=100; total time=   3.6s\n",
            "[CV] END .....................max_depth=15, n_estimators=100; total time=   3.6s\n",
            "[CV] END .....................max_depth=15, n_estimators=100; total time=   3.6s\n",
            "[CV] END .....................max_depth=15, n_estimators=100; total time=   3.6s\n",
            "[CV] END .....................max_depth=15, n_estimators=100; total time=   3.6s\n",
            "[CV] END .....................max_depth=15, n_estimators=300; total time=  10.7s\n",
            "[CV] END .....................max_depth=15, n_estimators=300; total time=  11.5s\n",
            "[CV] END .....................max_depth=15, n_estimators=300; total time=  10.8s\n",
            "[CV] END .....................max_depth=15, n_estimators=300; total time=  10.8s\n",
            "[CV] END .....................max_depth=15, n_estimators=300; total time=  10.8s\n",
            "[CV] END .....................max_depth=15, n_estimators=600; total time=  21.5s\n",
            "[CV] END .....................max_depth=15, n_estimators=600; total time=  21.7s\n",
            "[CV] END .....................max_depth=15, n_estimators=600; total time=  21.6s\n",
            "[CV] END .....................max_depth=15, n_estimators=600; total time=  21.5s\n",
            "[CV] END .....................max_depth=15, n_estimators=600; total time=  22.2s\n",
            "[CV] END .....................max_depth=20, n_estimators=100; total time=   4.1s\n",
            "[CV] END .....................max_depth=20, n_estimators=100; total time=   4.1s\n",
            "[CV] END .....................max_depth=20, n_estimators=100; total time=   4.1s\n",
            "[CV] END .....................max_depth=20, n_estimators=100; total time=   4.0s\n",
            "[CV] END .....................max_depth=20, n_estimators=100; total time=   4.4s\n",
            "[CV] END .....................max_depth=20, n_estimators=300; total time=  20.3s\n",
            "[CV] END .....................max_depth=20, n_estimators=300; total time=  15.5s\n",
            "[CV] END .....................max_depth=20, n_estimators=300; total time=  12.9s\n",
            "[CV] END .....................max_depth=20, n_estimators=300; total time=  12.1s\n",
            "[CV] END .....................max_depth=20, n_estimators=300; total time=  12.2s\n",
            "[CV] END .....................max_depth=20, n_estimators=600; total time=  24.5s\n",
            "[CV] END .....................max_depth=20, n_estimators=600; total time=  25.0s\n",
            "[CV] END .....................max_depth=20, n_estimators=600; total time=  24.3s\n",
            "[CV] END .....................max_depth=20, n_estimators=600; total time=  24.3s\n",
            "[CV] END .....................max_depth=20, n_estimators=600; total time=  24.2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(estimator=RandomForestRegressor(), n_iter=25,\n",
              "                   param_distributions={'max_depth': [6, 8, 10, 12, 15, 20],\n",
              "                                        'n_estimators': [100, 300, 600]},\n",
              "                   scoring='neg_mean_squared_error', verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_RFR = rand_search.best_estimator_\n",
        "evaluate(best_RFR, X_test, y_test)\n",
        "best_RFR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKChYsZNqoQN",
        "outputId": "e429c8dc-2c37-4d4e-b0aa-97240673c4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 38.941286749076944\n",
            "R2: 0.9361592184671619\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(max_depth=20, n_estimators=600)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = best_RFR.predict(data_test)\n",
        "pd.DataFrame({'pred': pred}).to_csv('hackcheek.csv', index=False)"
      ],
      "metadata": {
        "id": "Duu1ehfFwXT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deploy(RF_best)"
      ],
      "metadata": {
        "id": "KTIGQjlzaYGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_best.feature_importances_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er23U0ccMRwz",
        "outputId": "a33cf37a-f297-47d5-bd09-930ec21dc73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01506727, 0.05730237, 0.0355855 , 0.07404715, 0.57812095,\n",
              "       0.01948687, 0.09747497, 0.01772358, 0.0147447 , 0.01998987,\n",
              "       0.05582443, 0.01149127, 0.00314107])"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_X = E_data_train['temp']\n",
        "_y = E_data_train.iloc[:, -1]\n",
        "_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmqSIRvD1JBZ",
        "outputId": "f03ca51f-c8e8-4591-dfc2-ef60c1ad8b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0.24\n",
              "1        0.22\n",
              "2        0.22\n",
              "3        0.24\n",
              "4        0.24\n",
              "         ... \n",
              "11994    0.60\n",
              "11995    0.58\n",
              "11996    0.56\n",
              "11997    0.56\n",
              "11998    0.54\n",
              "Name: temp, Length: 11999, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(_X, _y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "1SMdaHct1Asr",
        "outputId": "a8ec2094-8566-4f53-df59-2ff0f8c9bb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f14368ae250>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting"
      ],
      "metadata": {
        "id": "4YsNORXDLGUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop(['cnt'], axis=1)\n",
        "y = data_train['cnt']\n",
        "\n",
        "pred_to_column = model.predict(X)\n",
        "pred_to_column_test = model.predict(data_test)\n",
        "\n",
        "X['pred'] = pred_to_column\n",
        "data_test['pred'] = pred_to_column_test\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "features = X_train\n",
        "target = y_train"
      ],
      "metadata": {
        "id": "PtR2hg0s8hV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "631f1a8b-3d91-4f9a-838a-7613f831dfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GB = GradientBoostingRegressor()\n",
        "\n",
        "param_grid = dict(\n",
        "    n_estimators = [100],\n",
        "    subsample = [0.9, 0.5, 0.2, 0.1],\n",
        "    max_depth = [4,6,8,10],\n",
        "    learning_rate = [0.1,0.2,0.3,0.4],\n",
        ")\n",
        "\n",
        "grid_GB = GridSearchCV(\n",
        "    estimator=GB, \n",
        "    param_grid=param_grid, \n",
        "    cv=3,\n",
        ")"
      ],
      "metadata": {
        "id": "bUB7TumY8jZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_GB.fit(features, target)"
      ],
      "metadata": {
        "id": "2F1Ascf4_yhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061e1023-2880-406d-e646-9b8812b595a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
              "             param_grid={'learning_rate': [0.1, 0.2, 0.3, 0.4],\n",
              "                         'max_depth': [4, 6, 8, 10], 'n_estimators': [100],\n",
              "                         'subsample': [0.9, 0.5, 0.2, 0.1]})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_GB = grid_GB.best_estimator_\n",
        "\n",
        "evaluate(best_GB, X_test, y_test)\n",
        "deploy(best_GB)"
      ],
      "metadata": {
        "id": "w_LF4YS888xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3f1b14-97b8-49ab-b93e-e66c02d934b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "Average Error: 21.6660 degrees.\n",
            "Accuracy = 66.14%.\n",
            "RMSE: 36.29157242781587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BEST ESTIMATOR:\\n\", grid_GB.best_estimator_, end='\\n\\n')\n",
        "print(\"BEST PARAMS:\\n\", grid_GB.best_params_, end='\\n\\n')\n",
        "print(\"BEST SCORE:\\n\", grid_GB.best_score_, end='\\n\\n')"
      ],
      "metadata": {
        "id": "ONh-HS3jgoD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9d0b5b-1eac-4477-e34f-0d775b22d105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEST ESTIMATOR:\n",
            " GradientBoostingRegressor(max_depth=8, subsample=0.9)\n",
            "\n",
            "BEST PARAMS:\n",
            " {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100, 'subsample': 0.9}\n",
            "\n",
            "BEST SCORE:\n",
            " 0.9398196096897333\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate"
      ],
      "metadata": {
        "id": "PkklKNWOOCql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in np.arange(0.01, 1, 0.01):\n",
        "    GBR = GradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=lr\n",
        "    )\n",
        "    GBR.fit(features, target)\n",
        "    print(f'\\n\\nLEARNING RATE {lr}')\n",
        "    evaluate(GBR, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44daLZTsN06-",
        "outputId": "5dc07653-ee37-4ac7-c1d9-bf8b4673c25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "LEARNING RATE 0.01\n",
            "Model Performance\n",
            "RMSE: 110.5964898815338\n",
            "R2: 0.4651055203731881\n",
            "\n",
            "\n",
            "LEARNING RATE 0.02\n",
            "Model Performance\n",
            "RMSE: 92.89738489262541\n",
            "R2: 0.6226082466464107\n",
            "\n",
            "\n",
            "LEARNING RATE 0.03\n",
            "Model Performance\n",
            "RMSE: 84.76343541064115\n",
            "R2: 0.6858026419543246\n",
            "\n",
            "\n",
            "LEARNING RATE 0.04\n",
            "Model Performance\n",
            "RMSE: 80.12248068504114\n",
            "R2: 0.7192665240931171\n",
            "\n",
            "\n",
            "LEARNING RATE 0.05\n",
            "Model Performance\n",
            "RMSE: 73.50545056870276\n",
            "R2: 0.7637213304089332\n",
            "\n",
            "\n",
            "LEARNING RATE 0.060000000000000005\n",
            "Model Performance\n",
            "RMSE: 69.73655743023755\n",
            "R2: 0.7873298988252788\n",
            "\n",
            "\n",
            "LEARNING RATE 0.06999999999999999\n",
            "Model Performance\n",
            "RMSE: 66.32430994889073\n",
            "R2: 0.8076328507309622\n",
            "\n",
            "\n",
            "LEARNING RATE 0.08\n",
            "Model Performance\n",
            "RMSE: 64.9658491672048\n",
            "R2: 0.8154323138432207\n",
            "\n",
            "\n",
            "LEARNING RATE 0.09\n",
            "Model Performance\n",
            "RMSE: 61.93560382771957\n",
            "R2: 0.832248589406845\n",
            "\n",
            "\n",
            "LEARNING RATE 0.09999999999999999\n",
            "Model Performance\n",
            "RMSE: 62.60032161931637\n",
            "R2: 0.8286285159405258\n",
            "\n",
            "\n",
            "LEARNING RATE 0.11\n",
            "Model Performance\n",
            "RMSE: 60.00234370023726\n",
            "R2: 0.842557543444487\n",
            "\n",
            "\n",
            "LEARNING RATE 0.12\n",
            "Model Performance\n",
            "RMSE: 57.385536506772745\n",
            "R2: 0.8559907723069746\n",
            "\n",
            "\n",
            "LEARNING RATE 0.13\n",
            "Model Performance\n",
            "RMSE: 55.54391327490647\n",
            "R2: 0.8650855780174926\n",
            "\n",
            "\n",
            "LEARNING RATE 0.14\n",
            "Model Performance\n",
            "RMSE: 54.10999160663775\n",
            "R2: 0.8719615636125326\n",
            "\n",
            "\n",
            "LEARNING RATE 0.15000000000000002\n",
            "Model Performance\n",
            "RMSE: 54.541650483953724\n",
            "R2: 0.8699105790101188\n",
            "\n",
            "\n",
            "LEARNING RATE 0.16\n",
            "Model Performance\n",
            "RMSE: 52.96390825248666\n",
            "R2: 0.8773279909644267\n",
            "\n",
            "\n",
            "LEARNING RATE 0.17\n",
            "Model Performance\n",
            "RMSE: 53.62964828950698\n",
            "R2: 0.8742247102858813\n",
            "\n",
            "\n",
            "LEARNING RATE 0.18000000000000002\n",
            "Model Performance\n",
            "RMSE: 51.289316657683294\n",
            "R2: 0.8849625478229034\n",
            "\n",
            "\n",
            "LEARNING RATE 0.19\n",
            "Model Performance\n",
            "RMSE: 51.37833354713399\n",
            "R2: 0.8845628871037525\n",
            "\n",
            "\n",
            "LEARNING RATE 0.2\n",
            "Model Performance\n",
            "RMSE: 50.32345165911178\n",
            "R2: 0.8892544532169615\n",
            "\n",
            "\n",
            "LEARNING RATE 0.21000000000000002\n",
            "Model Performance\n",
            "RMSE: 49.179819020518735\n",
            "R2: 0.8942307849812274\n",
            "\n",
            "\n",
            "LEARNING RATE 0.22\n",
            "Model Performance\n",
            "RMSE: 49.379021303965544\n",
            "R2: 0.8933722157392032\n",
            "\n",
            "\n",
            "LEARNING RATE 0.23\n",
            "Model Performance\n",
            "RMSE: 49.15497941799668\n",
            "R2: 0.8943376012252329\n",
            "\n",
            "\n",
            "LEARNING RATE 0.24000000000000002\n",
            "Model Performance\n",
            "RMSE: 49.2333537234478\n",
            "R2: 0.8940003894458367\n",
            "\n",
            "\n",
            "LEARNING RATE 0.25\n",
            "Model Performance\n",
            "RMSE: 48.95697103654672\n",
            "R2: 0.8951871550746606\n",
            "\n",
            "\n",
            "LEARNING RATE 0.26\n",
            "Model Performance\n",
            "RMSE: 48.164469336949864\n",
            "R2: 0.8985530514832613\n",
            "\n",
            "\n",
            "LEARNING RATE 0.27\n",
            "Model Performance\n",
            "RMSE: 48.36105657366349\n",
            "R2: 0.8977232333450845\n",
            "\n",
            "\n",
            "LEARNING RATE 0.28\n",
            "Model Performance\n",
            "RMSE: 48.66040591405225\n",
            "R2: 0.8964531519592631\n",
            "\n",
            "\n",
            "LEARNING RATE 0.29000000000000004\n",
            "Model Performance\n",
            "RMSE: 48.37090864209655\n",
            "R2: 0.8976815576492627\n",
            "\n",
            "\n",
            "LEARNING RATE 0.3\n",
            "Model Performance\n",
            "RMSE: 48.18256920052124\n",
            "R2: 0.8984767910795209\n",
            "\n",
            "\n",
            "LEARNING RATE 0.31\n",
            "Model Performance\n",
            "RMSE: 47.47298904119681\n",
            "R2: 0.9014450180226795\n",
            "\n",
            "\n",
            "LEARNING RATE 0.32\n",
            "Model Performance\n",
            "RMSE: 47.42961049229805\n",
            "R2: 0.9016250454010786\n",
            "\n",
            "\n",
            "LEARNING RATE 0.33\n",
            "Model Performance\n",
            "RMSE: 47.32899640543256\n",
            "R2: 0.9020419751518373\n",
            "\n",
            "\n",
            "LEARNING RATE 0.34\n",
            "Model Performance\n",
            "RMSE: 47.83698131352256\n",
            "R2: 0.8999279120199029\n",
            "\n",
            "\n",
            "LEARNING RATE 0.35000000000000003\n",
            "Model Performance\n",
            "RMSE: 47.14639131398907\n",
            "R2: 0.9027964017544897\n",
            "\n",
            "\n",
            "LEARNING RATE 0.36000000000000004\n",
            "Model Performance\n",
            "RMSE: 46.53231794318831\n",
            "R2: 0.9053120307489206\n",
            "\n",
            "\n",
            "LEARNING RATE 0.37\n",
            "Model Performance\n",
            "RMSE: 46.83588483622129\n",
            "R2: 0.9040725527174988\n",
            "\n",
            "\n",
            "LEARNING RATE 0.38\n",
            "Model Performance\n",
            "RMSE: 46.59035572277945\n",
            "R2: 0.9050756828998894\n",
            "\n",
            "\n",
            "LEARNING RATE 0.39\n",
            "Model Performance\n",
            "RMSE: 46.64269897545857\n",
            "R2: 0.9048622722692119\n",
            "\n",
            "\n",
            "LEARNING RATE 0.4\n",
            "Model Performance\n",
            "RMSE: 45.920337552584975\n",
            "R2: 0.9077862736052222\n",
            "\n",
            "\n",
            "LEARNING RATE 0.41000000000000003\n",
            "Model Performance\n",
            "RMSE: 46.1616308594591\n",
            "R2: 0.9068146337991558\n",
            "\n",
            "\n",
            "LEARNING RATE 0.42000000000000004\n",
            "Model Performance\n",
            "RMSE: 46.00731514196535\n",
            "R2: 0.9074366192303848\n",
            "\n",
            "\n",
            "LEARNING RATE 0.43\n",
            "Model Performance\n",
            "RMSE: 46.97336442883755\n",
            "R2: 0.9035085654159906\n",
            "\n",
            "\n",
            "LEARNING RATE 0.44\n",
            "Model Performance\n",
            "RMSE: 45.78572656767913\n",
            "R2: 0.9083261122681051\n",
            "\n",
            "\n",
            "LEARNING RATE 0.45\n",
            "Model Performance\n",
            "RMSE: 46.26718468705296\n",
            "R2: 0.9063879886592616\n",
            "\n",
            "\n",
            "LEARNING RATE 0.46\n",
            "Model Performance\n",
            "RMSE: 44.75639683370975\n",
            "R2: 0.9124017035299837\n",
            "\n",
            "\n",
            "LEARNING RATE 0.47000000000000003\n",
            "Model Performance\n",
            "RMSE: 45.19024891403771\n",
            "R2: 0.9106951807677153\n",
            "\n",
            "\n",
            "LEARNING RATE 0.48000000000000004\n",
            "Model Performance\n",
            "RMSE: 46.07118080701559\n",
            "R2: 0.9071794546902645\n",
            "\n",
            "\n",
            "LEARNING RATE 0.49\n",
            "Model Performance\n",
            "RMSE: 45.412165511570585\n",
            "R2: 0.9098159254876546\n",
            "\n",
            "\n",
            "LEARNING RATE 0.5\n",
            "Model Performance\n",
            "RMSE: 45.1202666866928\n",
            "R2: 0.910971563885268\n",
            "\n",
            "\n",
            "LEARNING RATE 0.51\n",
            "Model Performance\n",
            "RMSE: 46.0294223173739\n",
            "R2: 0.9073476417914446\n",
            "\n",
            "\n",
            "LEARNING RATE 0.52\n",
            "Model Performance\n",
            "RMSE: 44.712704568995726\n",
            "R2: 0.9125726511506344\n",
            "\n",
            "\n",
            "LEARNING RATE 0.53\n",
            "Model Performance\n",
            "RMSE: 44.43186759646548\n",
            "R2: 0.9136674507160883\n",
            "\n",
            "\n",
            "LEARNING RATE 0.54\n",
            "Model Performance\n",
            "RMSE: 44.91778971858993\n",
            "R2: 0.9117688003770743\n",
            "\n",
            "\n",
            "LEARNING RATE 0.55\n",
            "Model Performance\n",
            "RMSE: 44.919811024298106\n",
            "R2: 0.9117608593701252\n",
            "\n",
            "\n",
            "LEARNING RATE 0.56\n",
            "Model Performance\n",
            "RMSE: 45.127281919956374\n",
            "R2: 0.9109438777103063\n",
            "\n",
            "\n",
            "LEARNING RATE 0.5700000000000001\n",
            "Model Performance\n",
            "RMSE: 45.45667757255616\n",
            "R2: 0.9096390457315002\n",
            "\n",
            "\n",
            "LEARNING RATE 0.5800000000000001\n",
            "Model Performance\n",
            "RMSE: 46.003527041238485\n",
            "R2: 0.9074518613706231\n",
            "\n",
            "\n",
            "LEARNING RATE 0.59\n",
            "Model Performance\n",
            "RMSE: 44.07833544372676\n",
            "R2: 0.9150358337583149\n",
            "\n",
            "\n",
            "LEARNING RATE 0.6\n",
            "Model Performance\n",
            "RMSE: 44.797688732776344\n",
            "R2: 0.912239993972189\n",
            "\n",
            "\n",
            "LEARNING RATE 0.61\n",
            "Model Performance\n",
            "RMSE: 44.73273951792516\n",
            "R2: 0.912494284390372\n",
            "\n",
            "\n",
            "LEARNING RATE 0.62\n",
            "Model Performance\n",
            "RMSE: 44.997508997434\n",
            "R2: 0.911455340195008\n",
            "\n",
            "\n",
            "LEARNING RATE 0.63\n",
            "Model Performance\n",
            "RMSE: 45.8573386271385\n",
            "R2: 0.9080391193375404\n",
            "\n",
            "\n",
            "LEARNING RATE 0.64\n",
            "Model Performance\n",
            "RMSE: 44.6570190238534\n",
            "R2: 0.9127902809286552\n",
            "\n",
            "\n",
            "LEARNING RATE 0.65\n",
            "Model Performance\n",
            "RMSE: 44.05311121393329\n",
            "R2: 0.9151330489735325\n",
            "\n",
            "\n",
            "LEARNING RATE 0.66\n",
            "Model Performance\n",
            "RMSE: 45.228364071506704\n",
            "R2: 0.9105444711432049\n",
            "\n",
            "\n",
            "LEARNING RATE 0.67\n",
            "Model Performance\n",
            "RMSE: 44.39059923181677\n",
            "R2: 0.9138277477618766\n",
            "\n",
            "\n",
            "LEARNING RATE 0.68\n",
            "Model Performance\n",
            "RMSE: 45.25453198093682\n",
            "R2: 0.9104409280942533\n",
            "\n",
            "\n",
            "LEARNING RATE 0.6900000000000001\n",
            "Model Performance\n",
            "RMSE: 44.84863150055507\n",
            "R2: 0.9120402836863595\n",
            "\n",
            "\n",
            "LEARNING RATE 0.7000000000000001\n",
            "Model Performance\n",
            "RMSE: 44.88201980636978\n",
            "R2: 0.9119092688039125\n",
            "\n",
            "\n",
            "LEARNING RATE 0.7100000000000001\n",
            "Model Performance\n",
            "RMSE: 45.03936824116717\n",
            "R2: 0.9112905250069414\n",
            "\n",
            "\n",
            "LEARNING RATE 0.72\n",
            "Model Performance\n",
            "RMSE: 46.46416855825015\n",
            "R2: 0.9055891801281184\n",
            "\n",
            "\n",
            "LEARNING RATE 0.73\n",
            "Model Performance\n",
            "RMSE: 45.3760162264489\n",
            "R2: 0.9099594461600294\n",
            "\n",
            "\n",
            "LEARNING RATE 0.74\n",
            "Model Performance\n",
            "RMSE: 47.77811410708989\n",
            "R2: 0.9001740537892963\n",
            "\n",
            "\n",
            "LEARNING RATE 0.75\n",
            "Model Performance\n",
            "RMSE: 47.32600437405438\n",
            "R2: 0.9020543601268842\n",
            "\n",
            "\n",
            "LEARNING RATE 0.76\n",
            "Model Performance\n",
            "RMSE: 45.898858170490016\n",
            "R2: 0.9078725199497087\n",
            "\n",
            "\n",
            "LEARNING RATE 0.77\n",
            "Model Performance\n",
            "RMSE: 45.62930438254162\n",
            "R2: 0.9089514309270709\n",
            "\n",
            "\n",
            "LEARNING RATE 0.78\n",
            "Model Performance\n",
            "RMSE: 44.90943052757806\n",
            "R2: 0.911801636935922\n",
            "\n",
            "\n",
            "LEARNING RATE 0.79\n",
            "Model Performance\n",
            "RMSE: 46.0819843998693\n",
            "R2: 0.9071359171494809\n",
            "\n",
            "\n",
            "LEARNING RATE 0.8\n",
            "Model Performance\n",
            "RMSE: 45.35716113378966\n",
            "R2: 0.9100342597024929\n",
            "\n",
            "\n",
            "LEARNING RATE 0.81\n",
            "Model Performance\n",
            "RMSE: 46.4561214450442\n",
            "R2: 0.9056218792504687\n",
            "\n",
            "\n",
            "LEARNING RATE 0.8200000000000001\n",
            "Model Performance\n",
            "RMSE: 45.55892655419841\n",
            "R2: 0.9092320777181138\n",
            "\n",
            "\n",
            "LEARNING RATE 0.8300000000000001\n",
            "Model Performance\n",
            "RMSE: 45.11209697583335\n",
            "R2: 0.9110038008726378\n",
            "\n",
            "\n",
            "LEARNING RATE 0.8400000000000001\n",
            "Model Performance\n",
            "RMSE: 44.979068696919946\n",
            "R2: 0.9115278977924199\n",
            "\n",
            "\n",
            "LEARNING RATE 0.85\n",
            "Model Performance\n",
            "RMSE: 44.696569762273455\n",
            "R2: 0.9126357369726267\n",
            "\n",
            "\n",
            "LEARNING RATE 0.86\n",
            "Model Performance\n",
            "RMSE: 45.72807131977825\n",
            "R2: 0.9085568458732312\n",
            "\n",
            "\n",
            "LEARNING RATE 0.87\n",
            "Model Performance\n",
            "RMSE: 44.70133548557394\n",
            "R2: 0.9126171057397247\n",
            "\n",
            "\n",
            "LEARNING RATE 0.88\n",
            "Model Performance\n",
            "RMSE: 45.737847760757354\n",
            "R2: 0.9085177414833594\n",
            "\n",
            "\n",
            "LEARNING RATE 0.89\n",
            "Model Performance\n",
            "RMSE: 45.07724820917008\n",
            "R2: 0.9111412455971203\n",
            "\n",
            "\n",
            "LEARNING RATE 0.9\n",
            "Model Performance\n",
            "RMSE: 45.42835044757628\n",
            "R2: 0.9097516306642821\n",
            "\n",
            "\n",
            "LEARNING RATE 0.91\n",
            "Model Performance\n",
            "RMSE: 46.63505620042064\n",
            "R2: 0.9048934478527335\n",
            "\n",
            "\n",
            "LEARNING RATE 0.92\n",
            "Model Performance\n",
            "RMSE: 45.479623910813224\n",
            "R2: 0.9095477950520499\n",
            "\n",
            "\n",
            "LEARNING RATE 0.93\n",
            "Model Performance\n",
            "RMSE: 47.040931352148824\n",
            "R2: 0.9032307774232525\n",
            "\n",
            "\n",
            "LEARNING RATE 0.9400000000000001\n",
            "Model Performance\n",
            "RMSE: 45.70663775486299\n",
            "R2: 0.9086425478672588\n",
            "\n",
            "\n",
            "LEARNING RATE 0.9500000000000001\n",
            "Model Performance\n",
            "RMSE: 46.485456913068774\n",
            "R2: 0.9055026484406539\n",
            "\n",
            "\n",
            "LEARNING RATE 0.9600000000000001\n",
            "Model Performance\n",
            "RMSE: 48.08628041402982\n",
            "R2: 0.8988821567106391\n",
            "\n",
            "\n",
            "LEARNING RATE 0.97\n",
            "Model Performance\n",
            "RMSE: 46.2923526936648\n",
            "R2: 0.9062861165206525\n",
            "\n",
            "\n",
            "LEARNING RATE 0.98\n",
            "Model Performance\n",
            "RMSE: 48.402436901472655\n",
            "R2: 0.8975481314304274\n",
            "\n",
            "\n",
            "LEARNING RATE 0.99\n",
            "Model Performance\n",
            "RMSE: 47.979830378451396\n",
            "R2: 0.8993293563539133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimators"
      ],
      "metadata": {
        "id": "uM3cds3yPoE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 300 o 500 \n",
        "for e in np.arange(100, 2001, 100):\n",
        "    GBR = GradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.53,\n",
        "    )\n",
        "    GBR.fit(features, target)\n",
        "    print(f'\\n\\nESTIMATORS {e}')\n",
        "    evaluate(GBR, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDRqVdsZPRDN",
        "outputId": "f7705890-0d6c-4436-b3b9-088d7e80e984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ESTIMATORS 100\n",
            "Model Performance\n",
            "RMSE: 44.43186759646548\n",
            "R2: 0.9136674507160883\n",
            "\n",
            "\n",
            "ESTIMATORS 200\n",
            "Model Performance\n",
            "RMSE: 44.431867596465494\n",
            "R2: 0.9136674507160882\n",
            "\n",
            "\n",
            "ESTIMATORS 300\n",
            "Model Performance\n",
            "RMSE: 44.41982731431776\n",
            "R2: 0.913714233688266\n",
            "\n",
            "\n",
            "ESTIMATORS 400\n",
            "Model Performance\n",
            "RMSE: 44.44671549652151\n",
            "R2: 0.9136097411795203\n",
            "\n",
            "\n",
            "ESTIMATORS 500\n",
            "Model Performance\n",
            "RMSE: 44.41069887379966\n",
            "R2: 0.9137496941391494\n",
            "\n",
            "\n",
            "ESTIMATORS 600\n",
            "Model Performance\n",
            "RMSE: 44.42555384886266\n",
            "R2: 0.9136919846025814\n",
            "\n",
            "\n",
            "ESTIMATORS 700\n",
            "Model Performance\n",
            "RMSE: 44.43759257941665\n",
            "R2: 0.9136452016304037\n",
            "\n",
            "\n",
            "ESTIMATORS 800\n",
            "Model Performance\n",
            "RMSE: 44.43467923764834\n",
            "R2: 0.913656524151698\n",
            "\n",
            "\n",
            "ESTIMATORS 900\n",
            "Model Performance\n",
            "RMSE: 44.51831287074086\n",
            "R2: 0.9133311920244239\n",
            "\n",
            "\n",
            "ESTIMATORS 1000\n",
            "Model Performance\n",
            "RMSE: 44.44671549652155\n",
            "R2: 0.9136097411795202\n",
            "\n",
            "\n",
            "ESTIMATORS 1100\n",
            "Model Performance\n",
            "RMSE: 44.43759257941665\n",
            "R2: 0.9136452016304037\n",
            "\n",
            "\n",
            "ESTIMATORS 1200\n",
            "Model Performance\n",
            "RMSE: 44.41982731431776\n",
            "R2: 0.913714233688266\n",
            "\n",
            "\n",
            "ESTIMATORS 1300\n",
            "Model Performance\n",
            "RMSE: 44.434679237648325\n",
            "R2: 0.913656524151698\n",
            "\n",
            "\n",
            "ESTIMATORS 1400\n",
            "Model Performance\n",
            "RMSE: 44.446715496521506\n",
            "R2: 0.9136097411795203\n",
            "\n",
            "\n",
            "ESTIMATORS 1500\n",
            "Model Performance\n",
            "RMSE: 44.50920462873576\n",
            "R2: 0.9133666524753072\n",
            "\n",
            "\n",
            "ESTIMATORS 1600\n",
            "Model Performance\n",
            "RMSE: 44.51831287074087\n",
            "R2: 0.9133311920244238\n",
            "\n",
            "\n",
            "ESTIMATORS 1700\n",
            "Model Performance\n",
            "RMSE: 44.530326523990404\n",
            "R2: 0.9132844090522462\n",
            "\n",
            "\n",
            "ESTIMATORS 1800\n",
            "Model Performance\n",
            "RMSE: 44.43186759646548\n",
            "R2: 0.9136674507160883\n",
            "\n",
            "\n",
            "ESTIMATORS 1900\n",
            "Model Performance\n",
            "RMSE: 44.5183128707409\n",
            "R2: 0.9133311920244237\n",
            "\n",
            "\n",
            "ESTIMATORS 2000\n",
            "Model Performance\n",
            "RMSE: 44.52122073976301\n",
            "R2: 0.9133198695031296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## depth"
      ],
      "metadata": {
        "id": "--w4kfvNQCyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11\n",
        "for d in np.arange(1, 20, 1):\n",
        "    GBR = GradientBoostingRegressor(\n",
        "        n_estimators = 100,\n",
        "        learning_rate = 0.53,\n",
        "        max_depth = d,\n",
        "    )\n",
        "    GBR.fit(features, target)\n",
        "    print(f'\\n\\nESTIMATORS {d}')\n",
        "    evaluate(GBR, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CQYMIU5P17b",
        "outputId": "97330d97-20be-46cf-db75-65998ac1425b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ESTIMATORS 1\n",
            "Model Performance\n",
            "RMSE: 85.85706391700081\n",
            "R2: 0.6776427114632508\n",
            "\n",
            "\n",
            "ESTIMATORS 2\n",
            "Model Performance\n",
            "RMSE: 64.64146667330012\n",
            "R2: 0.8172708507556417\n",
            "\n",
            "\n",
            "ESTIMATORS 3\n",
            "Model Performance\n",
            "RMSE: 44.44671549652152\n",
            "R2: 0.9136097411795203\n",
            "\n",
            "\n",
            "ESTIMATORS 4\n",
            "Model Performance\n",
            "RMSE: 40.22038384052809\n",
            "R2: 0.9292579158335943\n",
            "\n",
            "\n",
            "ESTIMATORS 5\n",
            "Model Performance\n",
            "RMSE: 37.091519753449276\n",
            "R2: 0.9398362795168378\n",
            "\n",
            "\n",
            "ESTIMATORS 6\n",
            "Model Performance\n",
            "RMSE: 38.16030461513264\n",
            "R2: 0.9363191144114622\n",
            "\n",
            "\n",
            "ESTIMATORS 7\n",
            "Model Performance\n",
            "RMSE: 41.228111238576965\n",
            "R2: 0.9256686009540684\n",
            "\n",
            "\n",
            "ESTIMATORS 8\n",
            "Model Performance\n",
            "RMSE: 39.23849472960263\n",
            "R2: 0.9326697687757984\n",
            "\n",
            "\n",
            "ESTIMATORS 9\n",
            "Model Performance\n",
            "RMSE: 39.68507792125342\n",
            "R2: 0.9311284426555663\n",
            "\n",
            "\n",
            "ESTIMATORS 10\n",
            "Model Performance\n",
            "RMSE: 38.734390318367815\n",
            "R2: 0.934388664495979\n",
            "\n",
            "\n",
            "ESTIMATORS 11\n",
            "Model Performance\n",
            "RMSE: 37.38558359848023\n",
            "R2: 0.9388785346400342\n",
            "\n",
            "\n",
            "ESTIMATORS 12\n",
            "Model Performance\n",
            "RMSE: 40.21185490723904\n",
            "R2: 0.9292879150771025\n",
            "\n",
            "\n",
            "ESTIMATORS 13\n",
            "Model Performance\n",
            "RMSE: 41.66100241149763\n",
            "R2: 0.9240994610943335\n",
            "\n",
            "\n",
            "ESTIMATORS 14\n",
            "Model Performance\n",
            "RMSE: 43.28291801981139\n",
            "R2: 0.9180746146411334\n",
            "\n",
            "\n",
            "ESTIMATORS 15\n",
            "Model Performance\n",
            "RMSE: 45.04364475400957\n",
            "R2: 0.9112736781800685\n",
            "\n",
            "\n",
            "ESTIMATORS 16\n",
            "Model Performance\n",
            "RMSE: 47.65291423824879\n",
            "R2: 0.9006965449003718\n",
            "\n",
            "\n",
            "ESTIMATORS 17\n",
            "Model Performance\n",
            "RMSE: 48.64241632075878\n",
            "R2: 0.8965296996706474\n",
            "\n",
            "\n",
            "ESTIMATORS 18\n",
            "Model Performance\n",
            "RMSE: 48.66635933096188\n",
            "R2: 0.8964278132776342\n",
            "\n",
            "\n",
            "ESTIMATORS 19\n",
            "Model Performance\n",
            "RMSE: 48.49763453125476\n",
            "R2: 0.8971447316463104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## subsample"
      ],
      "metadata": {
        "id": "qnqTyCr5QxZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1\n",
        "for s in np.arange(0.1, 3, 0.1):\n",
        "    GBR = GradientBoostingRegressor(\n",
        "        n_estimators = 100,\n",
        "        learning_rate = 0.53,\n",
        "        max_depth = 11,\n",
        "        subsample = s,\n",
        "    )\n",
        "    GBR.fit(features, target)\n",
        "    print(f'\\n\\nSUBSAMPLE {s}')\n",
        "    evaluate(GBR, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y_aBU8MsQf5F",
        "outputId": "682947b5-d351-4509-e282-579a06eec94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "SUBSAMPLE 0.1\n",
            "Model Performance\n",
            "RMSE: 1194.9934732945933\n",
            "R2: -61.44770686784764\n",
            "\n",
            "\n",
            "SUBSAMPLE 0.2\n",
            "Model Performance\n",
            "RMSE: 196.2480546827782\n",
            "R2: -0.6842096433971574\n",
            "\n",
            "\n",
            "SUBSAMPLE 0.30000000000000004\n",
            "Model Performance\n",
            "RMSE: 89.20818167870056\n",
            "R2: 0.6519875320710989\n",
            "\n",
            "\n",
            "SUBSAMPLE 0.4\n",
            "Model Performance\n",
            "RMSE: 61.14417346302155\n",
            "R2: 0.836508346543982\n",
            "\n",
            "\n",
            "SUBSAMPLE 0.5\n",
            "Model Performance\n",
            "RMSE: 51.531162883813096\n",
            "R2: 0.8838751101647058\n",
            "\n",
            "\n",
            "SUBSAMPLE 0.6\n",
            "Model Performance\n",
            "RMSE: 46.9670228725313\n",
            "R2: 0.9035346169696014\n",
            "\n",
            "\n",
            "SUBSAMPLE 0.7000000000000001\n",
            "Model Performance\n",
            "RMSE: 44.266178207365805\n",
            "R2: 0.9143101298163272\n",
            "\n",
            "\n",
            "SUBSAMPLE 0.8\n",
            "Model Performance\n",
            "RMSE: 44.39800467268667\n",
            "R2: 0.9137989940726444\n",
            "\n",
            "\n",
            "SUBSAMPLE 0.9\n",
            "Model Performance\n",
            "RMSE: 42.446771667785484\n",
            "R2: 0.921209336265521\n",
            "\n",
            "\n",
            "SUBSAMPLE 1.0\n",
            "Model Performance\n",
            "RMSE: 38.23515039404617\n",
            "R2: 0.9360690682090657\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-cd34ab2a9ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msubsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mGBR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n\\nSUBSAMPLE {s}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGBR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsample\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subsample must be in (0,1] but was %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: subsample must be in (0,1] but was 1.1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "crRZONV9XCMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GBR = GradientBoostingRegressor(\n",
        "    n_estimators = 100,\n",
        "    learning_rate = 0.3,\n",
        "    max_depth = 11,\n",
        "    subsample = 1,\n",
        ")\n",
        "GBR.fit(features, target)\n",
        "evaluate(GBR, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xauAYjrYWGxc",
        "outputId": "8ee64dff-09c6-4bac-d257-7cf7662d50fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 38.666960729240536\n",
            "R2: 0.9360499999213429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict = GBR.predict(data_test.drop(['yr', 'mnth'], axis=1))\n",
        "pd.DataFrame({'pred': predict}).to_csv('hackcheek.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "I6vHXNtu7ygz",
        "outputId": "f5e275de-920b-4b57-e7d7-7ea6b33745de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:493: FutureWarning:\n",
            "\n",
            "The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
            "Feature names unseen at fit time:\n",
            "- atemp\n",
            "- pred\n",
            "Feature names must be in the same order as they were in fit.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-cf3da5ef6d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGBR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mnth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hackcheek.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1876\u001b[0m         \"\"\"\n\u001b[1;32m   1877\u001b[0m         X = self._validate_data(\n\u001b[0;32m-> 1878\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1879\u001b[0m         )\n\u001b[1;32m   1880\u001b[0m         \u001b[0;31m# In regression we can directly return the raw value from the trees.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             raise ValueError(\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: X has 12 features, but GradientBoostingRegressor is expecting 10 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "XGB = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "XGB.fit(features, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn9URXVTcNdZ",
        "outputId": "a0d837e8-fc8b-4da2-b763-c072709ce547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(objective='reg:squarederror')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(XGB, X_test, y_test)\n",
        "deploy(XGB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOq42cVscp5l",
        "outputId": "79c3f397-056d-4f00-c690-230f380f9d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 58.70651607529522\n",
            "R2: 0.844985763771593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = dict( \n",
        "    max_depth = [3, 5, 6, 10, 15, 20],\n",
        "    learning_rate = [0.01, 0.1, 0.2, 0.3],\n",
        "    subsample = np.arange(0.5, 1.0, 0.1),\n",
        "    colsample_bytree = np.arange(0.4, 1.0, 0.1),\n",
        "    colsample_bylevel = np.arange(0.4, 1.0, 0.1),\n",
        "    n_estimators = [100, 500, 1000],\n",
        ")\n",
        "\n",
        "XGB = xgb.XGBRegressor(seed = 20)\n",
        "rand_search = RandomizedSearchCV(\n",
        "    estimator=XGB,\n",
        "    param_distributions=params,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_iter=25,\n",
        "    verbose=1\n",
        ")\n",
        "rand_search.fit(features, target)\n",
        "print(\"Best parameters:\", rand_search.best_params_)\n",
        "print(\"Lowest RMSE: \", (-rand_search.best_score_)**(1/2.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTFdpkwvY84W",
        "outputId": "29862aa8-026e-4802-a2b2-5cf2dd368fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
            "[18:41:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:41:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:41:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:41:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:41:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:42:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:42:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:42:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:42:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:42:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:42:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:42:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:42:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:43:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:43:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:43:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:43:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:43:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:43:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:44:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:44:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:44:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:45:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:45:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:45:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:46:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:47:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:47:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:47:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:47:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:47:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:47:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:47:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:48:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:48:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:48:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:48:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:48:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:49:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:50:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:51:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:52:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:52:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:52:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:52:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[18:52:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Best parameters: {'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 0.8999999999999999, 'colsample_bylevel': 0.7999999999999999}\n",
            "Lowest RMSE:  31.70408979922556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_rand = rand_search.best_estimator_\n",
        "evaluate(best_rand, X_test, y_test)\n",
        "deploy(best_rand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1GdPZ7QgR2R",
        "outputId": "334ca943-404d-4872-b94c-e281e3db39a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 32.095880491626126\n",
            "R2: 0.9536662386673325\n",
            "[19:01:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(best_rand, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R57qe0Ps8p6z",
        "outputId": "aa38c1cc-fc67-4a6b-ecdd-29c413d0f741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 11.820909503641223\n",
            "R2: 0.9938993899170137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "XGBR = xgb.XGBRegressor(\n",
        "    colsample_bylevel=0.7999999999999999,\n",
        "    colsample_bytree=0.8999999999999999, learning_rate=0.01,\n",
        "    max_depth=10, n_estimators=1000, seed=20, subsample=0.6\n",
        ")\n",
        "\n",
        "XGBR.fit(features, target)\n",
        "pred = XGBR.predict(data_test)\n",
        "evaluate(XGBR, X_test, y_test)\n",
        "\n",
        "pd.DataFrame({'pred': pred}).to_csv('hackcheek.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTug54GF9bVz",
        "outputId": "79789a03-5ea6-4087-ee5d-300775a27247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:43:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Model Performance\n",
            "RMSE: 32.97248667688851\n",
            "R2: 0.9542299905643185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = best_rand.predict(data_test)\n",
        "pd.DataFrame({'pred': pred}).to_csv('hackcheek.csv')"
      ],
      "metadata": {
        "id": "pkmCVxTd80UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomic regression"
      ],
      "metadata": {
        "id": "5LvbmhTfuQL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop('cnt', axis=1)\n",
        "y = data_train['cnt']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "features = X_train\n",
        "target = y_train"
      ],
      "metadata": {
        "id": "cUk_pGFbzNZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the pipeline and train model\n",
        "for d in np.arange(1, 10):\n",
        "    PF = Pipeline(\n",
        "        [('poly', PolynomialFeatures(\n",
        "            degree=d\n",
        "        )),\n",
        "        ('linear', LinearRegression(fit_intercept=False))]\n",
        "    )\n",
        "                    \n",
        "    PF.fit(features, target)\n",
        "    print('\\nDEGREE', d)\n",
        "    evaluate(PF, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80-3PADFuVim",
        "outputId": "a96aab0e-f099-44ab-a10f-3b7f437f572c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEGREE 1\n",
            "Model Performance\n",
            "RMSE: 121.6011331047428\n",
            "R2: 0.3818610101142593\n",
            "\n",
            "DEGREE 2\n",
            "Model Performance\n",
            "RMSE: 103.26164261256065\n",
            "R2: 0.5542524621751386\n",
            "\n",
            "DEGREE 3\n",
            "Model Performance\n",
            "RMSE: 92.08348166610831\n",
            "R2: 0.6455341851297312\n",
            "\n",
            "DEGREE 4\n",
            "Model Performance\n",
            "RMSE: 2543.784644761751\n",
            "R2: -269.50268320627305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PF = Pipeline(\n",
        "    [('poly', PolynomialFeatures(\n",
        "        degree=3\n",
        "    )),\n",
        "    ('linear', LinearRegression(fit_intercept=False))]\n",
        ")\n",
        "\n",
        "PF.fit(features, target)\n",
        "evaluate(PF, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB6xzOcd2WTJ",
        "outputId": "3fa790bf-fcdb-40a2-d4ce-b949da048444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 91.52420330327331\n",
            "R2: 0.6258814175401735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poly_pred = PF.predict(data_train.drop('cnt', axis=1))\n",
        "\n",
        "DATA = data_train\n",
        "DATA['poly'] = poly_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPcWAehK1iRX",
        "outputId": "1c163682-185d-4cce-9ac1-56959c3cbf99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJH8x37l32Hu",
        "outputId": "1fb051db-2444-4e87-dd3c-f517e268a918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 90.17831582965549\n",
            "R2: 0.6451628158350551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian regressor"
      ],
      "metadata": {
        "id": "jKfTPWjX6f6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop('cnt', axis=1)\n",
        "y = data_train['cnt']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "features = X_train\n",
        "target = y_train"
      ],
      "metadata": {
        "id": "LWX1OhiT9v5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,ExpSineSquared, DotProduct,ConstantKernel)\n",
        "                                              \n",
        "# Determination of the kernel used and the value lenght_scale at which the optimization process starts\n",
        "kernel = 1.0 * RBF(\n",
        "    length_scale=1.0, length_scale_bounds=(1e-2, 1e3)\n",
        ")\n",
        "\n",
        "# Application of the regressor (alpha makes a statement about a possible noise of the training data)\n",
        "gp = GaussianProcessRegressor(kernel=kernel, alpha=0.1)\n",
        "gp.fit(features, target)\n",
        "\n",
        "# Prediction of the dependent variable for the test dataset\n",
        "pred, sigma = gp.predict(X_test, return_std=True)\n",
        "evaluate(gp, X_test, y_test)"
      ],
      "metadata": {
        "id": "WTWw4oi3_Gz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.gaussian_process as gp\n",
        "kernel = gp.kernels.ConstantKernel(1.0, (1e-1, 1e3)) * gp.kernels.RBF(10.0, (1e-3, 1e3))\n",
        "GPR = gp.GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.1, normalize_y=True)\n",
        "\n",
        "GPR.fit(features, target)\n",
        "params = GPR.kernel_.get_params()\n",
        "\n",
        "# evaluate(GPR, X_test, y_test)"
      ],
      "metadata": {
        "id": "o-HU-A9xAy5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning"
      ],
      "metadata": {
        "id": "oxgmZ_62LoJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch"
      ],
      "metadata": {
        "id": "1msTFs2z5suX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(13, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        ) \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TorchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y, scale_data=True):\n",
        "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "            # Apply scaling if necessary\n",
        "            if scale_data:\n",
        "                X = StandardScaler().fit_transform(X)\n",
        "            self.X = torch.from_numpy(X)\n",
        "            self.y = torch.from_numpy(y)    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)  \n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]"
      ],
      "metadata": {
        "id": "itJ8r1y8lAi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop('cnt', axis=1).to_numpy()\n",
        "y = data_train.cnt.to_numpy()\n",
        "\n",
        "data = TorchDataset(X, y, scale_data=False)\n",
        "data_torch = torch.utils.data.DataLoader(data, batch_size=1100, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "WnOuScQHLnHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "87b5b06d-eb3c-4074-8505-2b06b5a2a0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3ea1a72999b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "  \n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  current_loss = 0.0\n",
        "  \n",
        "  for i, data in enumerate(data_torch, 0):\n",
        "\n",
        "\n",
        "    # Get and prepare inputs\n",
        "    features, target = data\n",
        "    features, target = features.float(), target.float()\n",
        "    target = target.reshape((target.shape[0], 1))\n",
        "    \n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Perform forward pass\n",
        "    outputs = mlp(features)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = loss_function(outputs, target)\n",
        "    \n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print statistics\n",
        "    current_loss += loss.item()\n",
        "    if i % 20 == 0:\n",
        "        print('Loss after mini-batch %5d: %.3f' %\n",
        "              (i + 1, current_loss / 500))\n",
        "        current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we0YtVFiley0",
        "outputId": "1b846872-4c76-44ea-dc30-e22048471385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch     1: 108.997\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 98.761\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 101.877\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 104.201\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 93.326\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TEt3ApNzij8",
        "outputId": "366b7fc2-b9c6-445e-e06b-83128a124295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor flow"
      ],
      "metadata": {
        "id": "rixPYM2F0jxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop('cnt', axis=1)\n",
        "y = data_train['cnt']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "features = X_train\n",
        "target = y_train\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=13 ,activation='relu',input_shape=(13, )))\n",
        "model.add(tf.keras.layers.Dense(units=13 ,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=7 ,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=1 ,activation='linear'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdqyZsxy0jcK",
        "outputId": "69c82fe7-e726-4732-afec-8304b77b9bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_31 (Dense)            (None, 13)                182       \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 13)                182       \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 7)                 98        \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 1)                 8         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 470\n",
            "Trainable params: 470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "metrics = tf.keras.metrics.RootMeanSquaredError()\n",
        "model.compile(optimizer = \"Adam\", loss = 'mean_squared_error', metrics = metrics)\n",
        "\n",
        "# dejar primero\n",
        "epoch_historial = model.fit(features, target,epochs= 1000, batch_size= 900, shuffle=True, use_multiprocessing=True)\n",
        "epoch_historial = model.fit(features, target,epochs= 100, batch_size= 100, shuffle=True, use_multiprocessing=True)\n",
        "# epoch_historial = model.fit(features, target,epochs= 1000, batch_size= 10, shuffle=True, use_multiprocessing=True)\n",
        "# epoch_historial = model.fit(features, target,epochs= 1000, batch_size= 500, shuffle=True, use_multiprocessing=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eR6192K0n6H",
        "outputId": "245c46d9-6fa6-4356-a2e2-17c4d86d9371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "11/11 [==============================] - 1s 2ms/step - loss: 3168.5029 - root_mean_squared_error: 56.2895\n",
            "Epoch 2/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3119.7722 - root_mean_squared_error: 55.8549\n",
            "Epoch 3/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3120.2512 - root_mean_squared_error: 55.8592\n",
            "Epoch 4/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3119.1497 - root_mean_squared_error: 55.8493\n",
            "Epoch 5/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3111.4421 - root_mean_squared_error: 55.7803\n",
            "Epoch 6/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3107.8840 - root_mean_squared_error: 55.7484\n",
            "Epoch 7/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3106.4036 - root_mean_squared_error: 55.7351\n",
            "Epoch 8/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3133.6956 - root_mean_squared_error: 55.9794\n",
            "Epoch 9/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3105.8086 - root_mean_squared_error: 55.7298\n",
            "Epoch 10/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3101.8684 - root_mean_squared_error: 55.6944\n",
            "Epoch 11/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3098.6877 - root_mean_squared_error: 55.6659\n",
            "Epoch 12/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3095.2739 - root_mean_squared_error: 55.6352\n",
            "Epoch 13/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3100.1792 - root_mean_squared_error: 55.6793\n",
            "Epoch 14/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3109.7148 - root_mean_squared_error: 55.7648\n",
            "Epoch 15/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3088.0496 - root_mean_squared_error: 55.5702\n",
            "Epoch 16/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3083.6199 - root_mean_squared_error: 55.5303\n",
            "Epoch 17/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3090.8416 - root_mean_squared_error: 55.5953\n",
            "Epoch 18/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3083.4612 - root_mean_squared_error: 55.5289\n",
            "Epoch 19/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3079.4316 - root_mean_squared_error: 55.4926\n",
            "Epoch 20/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3078.9019 - root_mean_squared_error: 55.4879\n",
            "Epoch 21/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3097.8274 - root_mean_squared_error: 55.6581\n",
            "Epoch 22/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3089.3623 - root_mean_squared_error: 55.5820\n",
            "Epoch 23/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3075.9412 - root_mean_squared_error: 55.4612\n",
            "Epoch 24/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3076.6284 - root_mean_squared_error: 55.4674\n",
            "Epoch 25/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3095.1648 - root_mean_squared_error: 55.6342\n",
            "Epoch 26/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3074.2275 - root_mean_squared_error: 55.4457\n",
            "Epoch 27/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3071.1196 - root_mean_squared_error: 55.4177\n",
            "Epoch 28/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3070.4573 - root_mean_squared_error: 55.4117\n",
            "Epoch 29/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3079.1418 - root_mean_squared_error: 55.4900\n",
            "Epoch 30/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3076.3394 - root_mean_squared_error: 55.4648\n",
            "Epoch 31/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3070.0952 - root_mean_squared_error: 55.4084\n",
            "Epoch 32/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3062.5693 - root_mean_squared_error: 55.3405\n",
            "Epoch 33/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3062.0303 - root_mean_squared_error: 55.3356\n",
            "Epoch 34/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3056.4666 - root_mean_squared_error: 55.2853\n",
            "Epoch 35/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3058.3262 - root_mean_squared_error: 55.3021\n",
            "Epoch 36/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3054.4392 - root_mean_squared_error: 55.2670\n",
            "Epoch 37/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3056.9302 - root_mean_squared_error: 55.2895\n",
            "Epoch 38/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3086.2268 - root_mean_squared_error: 55.5538\n",
            "Epoch 39/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3070.2131 - root_mean_squared_error: 55.4095\n",
            "Epoch 40/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3062.1370 - root_mean_squared_error: 55.3366\n",
            "Epoch 41/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3048.1152 - root_mean_squared_error: 55.2097\n",
            "Epoch 42/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3049.6453 - root_mean_squared_error: 55.2236\n",
            "Epoch 43/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3042.4407 - root_mean_squared_error: 55.1583\n",
            "Epoch 44/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3045.8706 - root_mean_squared_error: 55.1894\n",
            "Epoch 45/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3035.8352 - root_mean_squared_error: 55.0984\n",
            "Epoch 46/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3038.1204 - root_mean_squared_error: 55.1191\n",
            "Epoch 47/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3033.6680 - root_mean_squared_error: 55.0787\n",
            "Epoch 48/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3036.5793 - root_mean_squared_error: 55.1052\n",
            "Epoch 49/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3033.1025 - root_mean_squared_error: 55.0736\n",
            "Epoch 50/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3038.0791 - root_mean_squared_error: 55.1188\n",
            "Epoch 51/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3033.8103 - root_mean_squared_error: 55.0800\n",
            "Epoch 52/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3057.2959 - root_mean_squared_error: 55.2928\n",
            "Epoch 53/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3038.4399 - root_mean_squared_error: 55.1220\n",
            "Epoch 54/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3055.3101 - root_mean_squared_error: 55.2749\n",
            "Epoch 55/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3036.3391 - root_mean_squared_error: 55.1030\n",
            "Epoch 56/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3023.6191 - root_mean_squared_error: 54.9874\n",
            "Epoch 57/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3023.9651 - root_mean_squared_error: 54.9906\n",
            "Epoch 58/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3033.3420 - root_mean_squared_error: 55.0758\n",
            "Epoch 59/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3017.1738 - root_mean_squared_error: 54.9288\n",
            "Epoch 60/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3019.7803 - root_mean_squared_error: 54.9525\n",
            "Epoch 61/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3019.9585 - root_mean_squared_error: 54.9541\n",
            "Epoch 62/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3017.6367 - root_mean_squared_error: 54.9330\n",
            "Epoch 63/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3011.7285 - root_mean_squared_error: 54.8792\n",
            "Epoch 64/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3012.8103 - root_mean_squared_error: 54.8891\n",
            "Epoch 65/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3022.4424 - root_mean_squared_error: 54.9767\n",
            "Epoch 66/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3015.1147 - root_mean_squared_error: 54.9101\n",
            "Epoch 67/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3013.7385 - root_mean_squared_error: 54.8975\n",
            "Epoch 68/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3016.2026 - root_mean_squared_error: 54.9200\n",
            "Epoch 69/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3028.3660 - root_mean_squared_error: 55.0306\n",
            "Epoch 70/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3008.4670 - root_mean_squared_error: 54.8495\n",
            "Epoch 71/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3011.9927 - root_mean_squared_error: 54.8816\n",
            "Epoch 72/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3021.1577 - root_mean_squared_error: 54.9651\n",
            "Epoch 73/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3015.8423 - root_mean_squared_error: 54.9167\n",
            "Epoch 74/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3010.7849 - root_mean_squared_error: 54.8706\n",
            "Epoch 75/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2999.9087 - root_mean_squared_error: 54.7714\n",
            "Epoch 76/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3004.3010 - root_mean_squared_error: 54.8115\n",
            "Epoch 77/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2991.3438 - root_mean_squared_error: 54.6932\n",
            "Epoch 78/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2995.8323 - root_mean_squared_error: 54.7342\n",
            "Epoch 79/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2988.0312 - root_mean_squared_error: 54.6629\n",
            "Epoch 80/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2995.3938 - root_mean_squared_error: 54.7302\n",
            "Epoch 81/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2982.2136 - root_mean_squared_error: 54.6096\n",
            "Epoch 82/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2982.5354 - root_mean_squared_error: 54.6126\n",
            "Epoch 83/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2982.4475 - root_mean_squared_error: 54.6118\n",
            "Epoch 84/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2988.1855 - root_mean_squared_error: 54.6643\n",
            "Epoch 85/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2983.8057 - root_mean_squared_error: 54.6242\n",
            "Epoch 86/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2980.5039 - root_mean_squared_error: 54.5940\n",
            "Epoch 87/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2976.1970 - root_mean_squared_error: 54.5545\n",
            "Epoch 88/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2979.2278 - root_mean_squared_error: 54.5823\n",
            "Epoch 89/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2987.5696 - root_mean_squared_error: 54.6587\n",
            "Epoch 90/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2978.3130 - root_mean_squared_error: 54.5739\n",
            "Epoch 91/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2972.2217 - root_mean_squared_error: 54.5181\n",
            "Epoch 92/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2979.4412 - root_mean_squared_error: 54.5843\n",
            "Epoch 93/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2993.8447 - root_mean_squared_error: 54.7160\n",
            "Epoch 94/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2996.6240 - root_mean_squared_error: 54.7414\n",
            "Epoch 95/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2978.7627 - root_mean_squared_error: 54.5780\n",
            "Epoch 96/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2965.3735 - root_mean_squared_error: 54.4552\n",
            "Epoch 97/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2958.8181 - root_mean_squared_error: 54.3950\n",
            "Epoch 98/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2965.7151 - root_mean_squared_error: 54.4584\n",
            "Epoch 99/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2958.2593 - root_mean_squared_error: 54.3899\n",
            "Epoch 100/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2963.8911 - root_mean_squared_error: 54.4416\n",
            "Epoch 101/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2973.2781 - root_mean_squared_error: 54.5278\n",
            "Epoch 102/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2961.2261 - root_mean_squared_error: 54.4171\n",
            "Epoch 103/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2955.7825 - root_mean_squared_error: 54.3671\n",
            "Epoch 104/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2960.0425 - root_mean_squared_error: 54.4063\n",
            "Epoch 105/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2956.5366 - root_mean_squared_error: 54.3740\n",
            "Epoch 106/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2960.1387 - root_mean_squared_error: 54.4072\n",
            "Epoch 107/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2945.8999 - root_mean_squared_error: 54.2761\n",
            "Epoch 108/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2943.8928 - root_mean_squared_error: 54.2577\n",
            "Epoch 109/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2947.4077 - root_mean_squared_error: 54.2900\n",
            "Epoch 110/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2946.8647 - root_mean_squared_error: 54.2850\n",
            "Epoch 111/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2939.5845 - root_mean_squared_error: 54.2179\n",
            "Epoch 112/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2939.1938 - root_mean_squared_error: 54.2143\n",
            "Epoch 113/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2936.9717 - root_mean_squared_error: 54.1938\n",
            "Epoch 114/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2958.2756 - root_mean_squared_error: 54.3900\n",
            "Epoch 115/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2947.3801 - root_mean_squared_error: 54.2898\n",
            "Epoch 116/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2953.3726 - root_mean_squared_error: 54.3449\n",
            "Epoch 117/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2932.6301 - root_mean_squared_error: 54.1538\n",
            "Epoch 118/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2936.3367 - root_mean_squared_error: 54.1880\n",
            "Epoch 119/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2937.7573 - root_mean_squared_error: 54.2011\n",
            "Epoch 120/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2931.6208 - root_mean_squared_error: 54.1444\n",
            "Epoch 121/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2933.5276 - root_mean_squared_error: 54.1620\n",
            "Epoch 122/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2927.6775 - root_mean_squared_error: 54.1080\n",
            "Epoch 123/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2932.3838 - root_mean_squared_error: 54.1515\n",
            "Epoch 124/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2938.6912 - root_mean_squared_error: 54.2097\n",
            "Epoch 125/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2935.6218 - root_mean_squared_error: 54.1814\n",
            "Epoch 126/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2937.2063 - root_mean_squared_error: 54.1960\n",
            "Epoch 127/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2921.6533 - root_mean_squared_error: 54.0523\n",
            "Epoch 128/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2920.1223 - root_mean_squared_error: 54.0382\n",
            "Epoch 129/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2930.0276 - root_mean_squared_error: 54.1297\n",
            "Epoch 130/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2921.2056 - root_mean_squared_error: 54.0482\n",
            "Epoch 131/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2914.9517 - root_mean_squared_error: 53.9903\n",
            "Epoch 132/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2914.4670 - root_mean_squared_error: 53.9858\n",
            "Epoch 133/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2917.0029 - root_mean_squared_error: 54.0093\n",
            "Epoch 134/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2920.6384 - root_mean_squared_error: 54.0429\n",
            "Epoch 135/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2910.9487 - root_mean_squared_error: 53.9532\n",
            "Epoch 136/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2914.9934 - root_mean_squared_error: 53.9907\n",
            "Epoch 137/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2908.5959 - root_mean_squared_error: 53.9314\n",
            "Epoch 138/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2906.7502 - root_mean_squared_error: 53.9143\n",
            "Epoch 139/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2908.8867 - root_mean_squared_error: 53.9341\n",
            "Epoch 140/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2904.8176 - root_mean_squared_error: 53.8964\n",
            "Epoch 141/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2900.1514 - root_mean_squared_error: 53.8531\n",
            "Epoch 142/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2913.6409 - root_mean_squared_error: 53.9782\n",
            "Epoch 143/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2914.9758 - root_mean_squared_error: 53.9905\n",
            "Epoch 144/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2904.3096 - root_mean_squared_error: 53.8916\n",
            "Epoch 145/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2895.7974 - root_mean_squared_error: 53.8126\n",
            "Epoch 146/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2902.6191 - root_mean_squared_error: 53.8760\n",
            "Epoch 147/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2891.0515 - root_mean_squared_error: 53.7685\n",
            "Epoch 148/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2895.5552 - root_mean_squared_error: 53.8104\n",
            "Epoch 149/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2900.7812 - root_mean_squared_error: 53.8589\n",
            "Epoch 150/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2891.9751 - root_mean_squared_error: 53.7771\n",
            "Epoch 151/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2906.8784 - root_mean_squared_error: 53.9155\n",
            "Epoch 152/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2896.8706 - root_mean_squared_error: 53.8226\n",
            "Epoch 153/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2905.0098 - root_mean_squared_error: 53.8981\n",
            "Epoch 154/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2910.0327 - root_mean_squared_error: 53.9447\n",
            "Epoch 155/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2893.5398 - root_mean_squared_error: 53.7916\n",
            "Epoch 156/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2893.3623 - root_mean_squared_error: 53.7900\n",
            "Epoch 157/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2900.1748 - root_mean_squared_error: 53.8533\n",
            "Epoch 158/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2890.2202 - root_mean_squared_error: 53.7608\n",
            "Epoch 159/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2883.9553 - root_mean_squared_error: 53.7025\n",
            "Epoch 160/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2885.7449 - root_mean_squared_error: 53.7191\n",
            "Epoch 161/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2875.2905 - root_mean_squared_error: 53.6217\n",
            "Epoch 162/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2878.1904 - root_mean_squared_error: 53.6488\n",
            "Epoch 163/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2874.4954 - root_mean_squared_error: 53.6143\n",
            "Epoch 164/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2891.4907 - root_mean_squared_error: 53.7726\n",
            "Epoch 165/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2874.4143 - root_mean_squared_error: 53.6136\n",
            "Epoch 166/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2879.1941 - root_mean_squared_error: 53.6581\n",
            "Epoch 167/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2867.3198 - root_mean_squared_error: 53.5474\n",
            "Epoch 168/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2872.0054 - root_mean_squared_error: 53.5911\n",
            "Epoch 169/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2871.8643 - root_mean_squared_error: 53.5898\n",
            "Epoch 170/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2889.2295 - root_mean_squared_error: 53.7516\n",
            "Epoch 171/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2882.1836 - root_mean_squared_error: 53.6860\n",
            "Epoch 172/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2866.4895 - root_mean_squared_error: 53.5396\n",
            "Epoch 173/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2859.7795 - root_mean_squared_error: 53.4769\n",
            "Epoch 174/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2862.9717 - root_mean_squared_error: 53.5067\n",
            "Epoch 175/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2864.3762 - root_mean_squared_error: 53.5199\n",
            "Epoch 176/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2863.4861 - root_mean_squared_error: 53.5116\n",
            "Epoch 177/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2860.8030 - root_mean_squared_error: 53.4865\n",
            "Epoch 178/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2856.8518 - root_mean_squared_error: 53.4495\n",
            "Epoch 179/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2854.0469 - root_mean_squared_error: 53.4233\n",
            "Epoch 180/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2863.5693 - root_mean_squared_error: 53.5123\n",
            "Epoch 181/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2861.4253 - root_mean_squared_error: 53.4923\n",
            "Epoch 182/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2865.2480 - root_mean_squared_error: 53.5280\n",
            "Epoch 183/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2854.2173 - root_mean_squared_error: 53.4249\n",
            "Epoch 184/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2851.4392 - root_mean_squared_error: 53.3989\n",
            "Epoch 185/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2892.4902 - root_mean_squared_error: 53.7819\n",
            "Epoch 186/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2872.2546 - root_mean_squared_error: 53.5934\n",
            "Epoch 187/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2848.8184 - root_mean_squared_error: 53.3743\n",
            "Epoch 188/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2844.1572 - root_mean_squared_error: 53.3306\n",
            "Epoch 189/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2852.7212 - root_mean_squared_error: 53.4109\n",
            "Epoch 190/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2839.6028 - root_mean_squared_error: 53.2879\n",
            "Epoch 191/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2835.6777 - root_mean_squared_error: 53.2511\n",
            "Epoch 192/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2835.5835 - root_mean_squared_error: 53.2502\n",
            "Epoch 193/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2832.3176 - root_mean_squared_error: 53.2195\n",
            "Epoch 194/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2831.8425 - root_mean_squared_error: 53.2151\n",
            "Epoch 195/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2830.0156 - root_mean_squared_error: 53.1979\n",
            "Epoch 196/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2830.1160 - root_mean_squared_error: 53.1988\n",
            "Epoch 197/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2838.2974 - root_mean_squared_error: 53.2757\n",
            "Epoch 198/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2853.9705 - root_mean_squared_error: 53.4226\n",
            "Epoch 199/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2838.3845 - root_mean_squared_error: 53.2765\n",
            "Epoch 200/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2829.5027 - root_mean_squared_error: 53.1931\n",
            "Epoch 201/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2817.2485 - root_mean_squared_error: 53.0778\n",
            "Epoch 202/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2817.2759 - root_mean_squared_error: 53.0780\n",
            "Epoch 203/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2818.8789 - root_mean_squared_error: 53.0931\n",
            "Epoch 204/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2820.0125 - root_mean_squared_error: 53.1038\n",
            "Epoch 205/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2813.9141 - root_mean_squared_error: 53.0463\n",
            "Epoch 206/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2811.3223 - root_mean_squared_error: 53.0219\n",
            "Epoch 207/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2810.4812 - root_mean_squared_error: 53.0140\n",
            "Epoch 208/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2819.9211 - root_mean_squared_error: 53.1029\n",
            "Epoch 209/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2804.8157 - root_mean_squared_error: 52.9605\n",
            "Epoch 210/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2809.0752 - root_mean_squared_error: 53.0007\n",
            "Epoch 211/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2803.5789 - root_mean_squared_error: 52.9488\n",
            "Epoch 212/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2801.2974 - root_mean_squared_error: 52.9273\n",
            "Epoch 213/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2797.2983 - root_mean_squared_error: 52.8895\n",
            "Epoch 214/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2795.1521 - root_mean_squared_error: 52.8692\n",
            "Epoch 215/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2795.2905 - root_mean_squared_error: 52.8705\n",
            "Epoch 216/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2803.0127 - root_mean_squared_error: 52.9435\n",
            "Epoch 217/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2792.1294 - root_mean_squared_error: 52.8406\n",
            "Epoch 218/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2792.9163 - root_mean_squared_error: 52.8480\n",
            "Epoch 219/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2794.8333 - root_mean_squared_error: 52.8662\n",
            "Epoch 220/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2792.1768 - root_mean_squared_error: 52.8411\n",
            "Epoch 221/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2790.2917 - root_mean_squared_error: 52.8232\n",
            "Epoch 222/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2801.9360 - root_mean_squared_error: 52.9333\n",
            "Epoch 223/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2800.9470 - root_mean_squared_error: 52.9240\n",
            "Epoch 224/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2783.0952 - root_mean_squared_error: 52.7551\n",
            "Epoch 225/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2770.7803 - root_mean_squared_error: 52.6382\n",
            "Epoch 226/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2769.6946 - root_mean_squared_error: 52.6279\n",
            "Epoch 227/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2772.6248 - root_mean_squared_error: 52.6557\n",
            "Epoch 228/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2770.9739 - root_mean_squared_error: 52.6400\n",
            "Epoch 229/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2774.0718 - root_mean_squared_error: 52.6695\n",
            "Epoch 230/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2762.5183 - root_mean_squared_error: 52.5597\n",
            "Epoch 231/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2770.4697 - root_mean_squared_error: 52.6353\n",
            "Epoch 232/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2760.7156 - root_mean_squared_error: 52.5425\n",
            "Epoch 233/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2761.9333 - root_mean_squared_error: 52.5541\n",
            "Epoch 234/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2760.0671 - root_mean_squared_error: 52.5363\n",
            "Epoch 235/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2753.9480 - root_mean_squared_error: 52.4781\n",
            "Epoch 236/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2751.7549 - root_mean_squared_error: 52.4572\n",
            "Epoch 237/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2753.1606 - root_mean_squared_error: 52.4706\n",
            "Epoch 238/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2748.0007 - root_mean_squared_error: 52.4214\n",
            "Epoch 239/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2747.1833 - root_mean_squared_error: 52.4136\n",
            "Epoch 240/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2742.3884 - root_mean_squared_error: 52.3678\n",
            "Epoch 241/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2754.4026 - root_mean_squared_error: 52.4824\n",
            "Epoch 242/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2751.2048 - root_mean_squared_error: 52.4519\n",
            "Epoch 243/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2740.1196 - root_mean_squared_error: 52.3462\n",
            "Epoch 244/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2734.7144 - root_mean_squared_error: 52.2945\n",
            "Epoch 245/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2733.2385 - root_mean_squared_error: 52.2804\n",
            "Epoch 246/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2731.2605 - root_mean_squared_error: 52.2615\n",
            "Epoch 247/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2730.8943 - root_mean_squared_error: 52.2580\n",
            "Epoch 248/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2732.6787 - root_mean_squared_error: 52.2750\n",
            "Epoch 249/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2738.0371 - root_mean_squared_error: 52.3263\n",
            "Epoch 250/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2734.5447 - root_mean_squared_error: 52.2929\n",
            "Epoch 251/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2728.2461 - root_mean_squared_error: 52.2326\n",
            "Epoch 252/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2725.0706 - root_mean_squared_error: 52.2022\n",
            "Epoch 253/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2721.6536 - root_mean_squared_error: 52.1695\n",
            "Epoch 254/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2726.2290 - root_mean_squared_error: 52.2133\n",
            "Epoch 255/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2718.0017 - root_mean_squared_error: 52.1345\n",
            "Epoch 256/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2714.4897 - root_mean_squared_error: 52.1008\n",
            "Epoch 257/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2722.8130 - root_mean_squared_error: 52.1806\n",
            "Epoch 258/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2711.9126 - root_mean_squared_error: 52.0760\n",
            "Epoch 259/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2710.5349 - root_mean_squared_error: 52.0628\n",
            "Epoch 260/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2714.4827 - root_mean_squared_error: 52.1007\n",
            "Epoch 261/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2710.8877 - root_mean_squared_error: 52.0662\n",
            "Epoch 262/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2708.9070 - root_mean_squared_error: 52.0472\n",
            "Epoch 263/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2712.7627 - root_mean_squared_error: 52.0842\n",
            "Epoch 264/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2721.2148 - root_mean_squared_error: 52.1653\n",
            "Epoch 265/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2715.1514 - root_mean_squared_error: 52.1071\n",
            "Epoch 266/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2707.0383 - root_mean_squared_error: 52.0292\n",
            "Epoch 267/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2698.6736 - root_mean_squared_error: 51.9488\n",
            "Epoch 268/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2700.1865 - root_mean_squared_error: 51.9633\n",
            "Epoch 269/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2699.4844 - root_mean_squared_error: 51.9566\n",
            "Epoch 270/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2698.4983 - root_mean_squared_error: 51.9471\n",
            "Epoch 271/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2695.4814 - root_mean_squared_error: 51.9180\n",
            "Epoch 272/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2695.4065 - root_mean_squared_error: 51.9173\n",
            "Epoch 273/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2698.1428 - root_mean_squared_error: 51.9436\n",
            "Epoch 274/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2694.9631 - root_mean_squared_error: 51.9130\n",
            "Epoch 275/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2685.4688 - root_mean_squared_error: 51.8215\n",
            "Epoch 276/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2694.0527 - root_mean_squared_error: 51.9043\n",
            "Epoch 277/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2694.7559 - root_mean_squared_error: 51.9110\n",
            "Epoch 278/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2682.5930 - root_mean_squared_error: 51.7938\n",
            "Epoch 279/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2681.8977 - root_mean_squared_error: 51.7870\n",
            "Epoch 280/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2681.2124 - root_mean_squared_error: 51.7804\n",
            "Epoch 281/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2680.6675 - root_mean_squared_error: 51.7752\n",
            "Epoch 282/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2686.3347 - root_mean_squared_error: 51.8299\n",
            "Epoch 283/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2677.3845 - root_mean_squared_error: 51.7435\n",
            "Epoch 284/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2679.1238 - root_mean_squared_error: 51.7603\n",
            "Epoch 285/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2675.4268 - root_mean_squared_error: 51.7245\n",
            "Epoch 286/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2700.4656 - root_mean_squared_error: 51.9660\n",
            "Epoch 287/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2677.0234 - root_mean_squared_error: 51.7400\n",
            "Epoch 288/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2678.7500 - root_mean_squared_error: 51.7566\n",
            "Epoch 289/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2675.4480 - root_mean_squared_error: 51.7247\n",
            "Epoch 290/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2704.8923 - root_mean_squared_error: 52.0086\n",
            "Epoch 291/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2676.8562 - root_mean_squared_error: 51.7383\n",
            "Epoch 292/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2693.0942 - root_mean_squared_error: 51.8950\n",
            "Epoch 293/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2688.5032 - root_mean_squared_error: 51.8508\n",
            "Epoch 294/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2670.9182 - root_mean_squared_error: 51.6809\n",
            "Epoch 295/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2669.6477 - root_mean_squared_error: 51.6686\n",
            "Epoch 296/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2679.7329 - root_mean_squared_error: 51.7661\n",
            "Epoch 297/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2682.9961 - root_mean_squared_error: 51.7976\n",
            "Epoch 298/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2679.5190 - root_mean_squared_error: 51.7641\n",
            "Epoch 299/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2664.5332 - root_mean_squared_error: 51.6191\n",
            "Epoch 300/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2658.7307 - root_mean_squared_error: 51.5629\n",
            "Epoch 301/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2670.8877 - root_mean_squared_error: 51.6806\n",
            "Epoch 302/1000\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 2670.4727 - root_mean_squared_error: 51.6766\n",
            "Epoch 303/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2668.9761 - root_mean_squared_error: 51.6621\n",
            "Epoch 304/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2654.7241 - root_mean_squared_error: 51.5240\n",
            "Epoch 305/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2653.7500 - root_mean_squared_error: 51.5146\n",
            "Epoch 306/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2673.5913 - root_mean_squared_error: 51.7068\n",
            "Epoch 307/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2658.8350 - root_mean_squared_error: 51.5639\n",
            "Epoch 308/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2651.1719 - root_mean_squared_error: 51.4895\n",
            "Epoch 309/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2647.3540 - root_mean_squared_error: 51.4524\n",
            "Epoch 310/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2644.3640 - root_mean_squared_error: 51.4234\n",
            "Epoch 311/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2647.5088 - root_mean_squared_error: 51.4539\n",
            "Epoch 312/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2643.0989 - root_mean_squared_error: 51.4111\n",
            "Epoch 313/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2647.3713 - root_mean_squared_error: 51.4526\n",
            "Epoch 314/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2641.6624 - root_mean_squared_error: 51.3971\n",
            "Epoch 315/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2638.7385 - root_mean_squared_error: 51.3687\n",
            "Epoch 316/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2648.3997 - root_mean_squared_error: 51.4626\n",
            "Epoch 317/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2640.7920 - root_mean_squared_error: 51.3886\n",
            "Epoch 318/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2648.5024 - root_mean_squared_error: 51.4636\n",
            "Epoch 319/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2639.1536 - root_mean_squared_error: 51.3727\n",
            "Epoch 320/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2636.2183 - root_mean_squared_error: 51.3441\n",
            "Epoch 321/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2660.1748 - root_mean_squared_error: 51.5769\n",
            "Epoch 322/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2663.3193 - root_mean_squared_error: 51.6074\n",
            "Epoch 323/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2648.3340 - root_mean_squared_error: 51.4620\n",
            "Epoch 324/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2632.2207 - root_mean_squared_error: 51.3052\n",
            "Epoch 325/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2630.8318 - root_mean_squared_error: 51.2916\n",
            "Epoch 326/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2635.0242 - root_mean_squared_error: 51.3325\n",
            "Epoch 327/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2646.8154 - root_mean_squared_error: 51.4472\n",
            "Epoch 328/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2629.3970 - root_mean_squared_error: 51.2776\n",
            "Epoch 329/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2623.9805 - root_mean_squared_error: 51.2248\n",
            "Epoch 330/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2624.5745 - root_mean_squared_error: 51.2306\n",
            "Epoch 331/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2629.5703 - root_mean_squared_error: 51.2793\n",
            "Epoch 332/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2625.2632 - root_mean_squared_error: 51.2373\n",
            "Epoch 333/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2626.5110 - root_mean_squared_error: 51.2495\n",
            "Epoch 334/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2630.9531 - root_mean_squared_error: 51.2928\n",
            "Epoch 335/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2626.0845 - root_mean_squared_error: 51.2453\n",
            "Epoch 336/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2649.0498 - root_mean_squared_error: 51.4689\n",
            "Epoch 337/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2626.8591 - root_mean_squared_error: 51.2529\n",
            "Epoch 338/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2615.9421 - root_mean_squared_error: 51.1463\n",
            "Epoch 339/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2627.5303 - root_mean_squared_error: 51.2594\n",
            "Epoch 340/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2629.2312 - root_mean_squared_error: 51.2760\n",
            "Epoch 341/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2617.1001 - root_mean_squared_error: 51.1576\n",
            "Epoch 342/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2620.2441 - root_mean_squared_error: 51.1883\n",
            "Epoch 343/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2610.1016 - root_mean_squared_error: 51.0892\n",
            "Epoch 344/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2614.6504 - root_mean_squared_error: 51.1337\n",
            "Epoch 345/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2618.5706 - root_mean_squared_error: 51.1720\n",
            "Epoch 346/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2605.8621 - root_mean_squared_error: 51.0476\n",
            "Epoch 347/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2617.9822 - root_mean_squared_error: 51.1662\n",
            "Epoch 348/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2607.3335 - root_mean_squared_error: 51.0621\n",
            "Epoch 349/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2607.8186 - root_mean_squared_error: 51.0668\n",
            "Epoch 350/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2608.2622 - root_mean_squared_error: 51.0711\n",
            "Epoch 351/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2614.2739 - root_mean_squared_error: 51.1300\n",
            "Epoch 352/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2610.3611 - root_mean_squared_error: 51.0917\n",
            "Epoch 353/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2599.2061 - root_mean_squared_error: 50.9824\n",
            "Epoch 354/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2611.3750 - root_mean_squared_error: 51.1016\n",
            "Epoch 355/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2609.9402 - root_mean_squared_error: 51.0876\n",
            "Epoch 356/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2600.0322 - root_mean_squared_error: 50.9905\n",
            "Epoch 357/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2602.9365 - root_mean_squared_error: 51.0190\n",
            "Epoch 358/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2611.2566 - root_mean_squared_error: 51.1005\n",
            "Epoch 359/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2601.4993 - root_mean_squared_error: 51.0049\n",
            "Epoch 360/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2605.3198 - root_mean_squared_error: 51.0423\n",
            "Epoch 361/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2600.2922 - root_mean_squared_error: 50.9931\n",
            "Epoch 362/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2599.6729 - root_mean_squared_error: 50.9870\n",
            "Epoch 363/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2592.4038 - root_mean_squared_error: 50.9157\n",
            "Epoch 364/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2588.7612 - root_mean_squared_error: 50.8799\n",
            "Epoch 365/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2585.6460 - root_mean_squared_error: 50.8492\n",
            "Epoch 366/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2590.9270 - root_mean_squared_error: 50.9011\n",
            "Epoch 367/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2587.0181 - root_mean_squared_error: 50.8627\n",
            "Epoch 368/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2591.2849 - root_mean_squared_error: 50.9047\n",
            "Epoch 369/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2593.4766 - root_mean_squared_error: 50.9262\n",
            "Epoch 370/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2596.3987 - root_mean_squared_error: 50.9549\n",
            "Epoch 371/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2591.5972 - root_mean_squared_error: 50.9077\n",
            "Epoch 372/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2581.0315 - root_mean_squared_error: 50.8039\n",
            "Epoch 373/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2578.3145 - root_mean_squared_error: 50.7771\n",
            "Epoch 374/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2578.1138 - root_mean_squared_error: 50.7751\n",
            "Epoch 375/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2578.7639 - root_mean_squared_error: 50.7815\n",
            "Epoch 376/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2584.0793 - root_mean_squared_error: 50.8338\n",
            "Epoch 377/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2574.2000 - root_mean_squared_error: 50.7366\n",
            "Epoch 378/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2583.0610 - root_mean_squared_error: 50.8238\n",
            "Epoch 379/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2597.5374 - root_mean_squared_error: 50.9660\n",
            "Epoch 380/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2581.6873 - root_mean_squared_error: 50.8103\n",
            "Epoch 381/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2577.1890 - root_mean_squared_error: 50.7660\n",
            "Epoch 382/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2569.0342 - root_mean_squared_error: 50.6856\n",
            "Epoch 383/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2574.8340 - root_mean_squared_error: 50.7428\n",
            "Epoch 384/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2576.6243 - root_mean_squared_error: 50.7605\n",
            "Epoch 385/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2588.1577 - root_mean_squared_error: 50.8739\n",
            "Epoch 386/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2604.9800 - root_mean_squared_error: 51.0390\n",
            "Epoch 387/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2589.5889 - root_mean_squared_error: 50.8880\n",
            "Epoch 388/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2578.8469 - root_mean_squared_error: 50.7823\n",
            "Epoch 389/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2560.1409 - root_mean_squared_error: 50.5978\n",
            "Epoch 390/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2562.0918 - root_mean_squared_error: 50.6171\n",
            "Epoch 391/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2564.2092 - root_mean_squared_error: 50.6380\n",
            "Epoch 392/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2566.3582 - root_mean_squared_error: 50.6592\n",
            "Epoch 393/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2568.6433 - root_mean_squared_error: 50.6818\n",
            "Epoch 394/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2557.3369 - root_mean_squared_error: 50.5701\n",
            "Epoch 395/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2556.0298 - root_mean_squared_error: 50.5572\n",
            "Epoch 396/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2571.9114 - root_mean_squared_error: 50.7140\n",
            "Epoch 397/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2573.7505 - root_mean_squared_error: 50.7321\n",
            "Epoch 398/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2576.1663 - root_mean_squared_error: 50.7559\n",
            "Epoch 399/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2560.8950 - root_mean_squared_error: 50.6053\n",
            "Epoch 400/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2562.6541 - root_mean_squared_error: 50.6227\n",
            "Epoch 401/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2557.5576 - root_mean_squared_error: 50.5723\n",
            "Epoch 402/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2562.1863 - root_mean_squared_error: 50.6180\n",
            "Epoch 403/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2553.3953 - root_mean_squared_error: 50.5311\n",
            "Epoch 404/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2558.2390 - root_mean_squared_error: 50.5790\n",
            "Epoch 405/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2555.8376 - root_mean_squared_error: 50.5553\n",
            "Epoch 406/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2554.6462 - root_mean_squared_error: 50.5435\n",
            "Epoch 407/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2548.9783 - root_mean_squared_error: 50.4874\n",
            "Epoch 408/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2553.1064 - root_mean_squared_error: 50.5283\n",
            "Epoch 409/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2563.0198 - root_mean_squared_error: 50.6263\n",
            "Epoch 410/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2595.6484 - root_mean_squared_error: 50.9475\n",
            "Epoch 411/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2555.3652 - root_mean_squared_error: 50.5506\n",
            "Epoch 412/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2539.8320 - root_mean_squared_error: 50.3967\n",
            "Epoch 413/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2547.5740 - root_mean_squared_error: 50.4735\n",
            "Epoch 414/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2554.7478 - root_mean_squared_error: 50.5445\n",
            "Epoch 415/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2556.5278 - root_mean_squared_error: 50.5621\n",
            "Epoch 416/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2555.2754 - root_mean_squared_error: 50.5497\n",
            "Epoch 417/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2542.8308 - root_mean_squared_error: 50.4265\n",
            "Epoch 418/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2549.6453 - root_mean_squared_error: 50.4940\n",
            "Epoch 419/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2536.6179 - root_mean_squared_error: 50.3648\n",
            "Epoch 420/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2542.2600 - root_mean_squared_error: 50.4208\n",
            "Epoch 421/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2536.7705 - root_mean_squared_error: 50.3664\n",
            "Epoch 422/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2536.0461 - root_mean_squared_error: 50.3592\n",
            "Epoch 423/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2545.9700 - root_mean_squared_error: 50.4576\n",
            "Epoch 424/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2548.7458 - root_mean_squared_error: 50.4851\n",
            "Epoch 425/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2553.5674 - root_mean_squared_error: 50.5328\n",
            "Epoch 426/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2534.8645 - root_mean_squared_error: 50.3474\n",
            "Epoch 427/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2538.8706 - root_mean_squared_error: 50.3872\n",
            "Epoch 428/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2539.4470 - root_mean_squared_error: 50.3929\n",
            "Epoch 429/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2546.3433 - root_mean_squared_error: 50.4613\n",
            "Epoch 430/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2540.7393 - root_mean_squared_error: 50.4057\n",
            "Epoch 431/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2528.0229 - root_mean_squared_error: 50.2794\n",
            "Epoch 432/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2539.3696 - root_mean_squared_error: 50.3922\n",
            "Epoch 433/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2534.4800 - root_mean_squared_error: 50.3436\n",
            "Epoch 434/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2524.7427 - root_mean_squared_error: 50.2468\n",
            "Epoch 435/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2525.4849 - root_mean_squared_error: 50.2542\n",
            "Epoch 436/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2540.3191 - root_mean_squared_error: 50.4016\n",
            "Epoch 437/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2525.0710 - root_mean_squared_error: 50.2501\n",
            "Epoch 438/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2531.6638 - root_mean_squared_error: 50.3156\n",
            "Epoch 439/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2525.9248 - root_mean_squared_error: 50.2586\n",
            "Epoch 440/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2521.7310 - root_mean_squared_error: 50.2168\n",
            "Epoch 441/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2530.7021 - root_mean_squared_error: 50.3061\n",
            "Epoch 442/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2519.7573 - root_mean_squared_error: 50.1972\n",
            "Epoch 443/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2520.4897 - root_mean_squared_error: 50.2045\n",
            "Epoch 444/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2517.7520 - root_mean_squared_error: 50.1772\n",
            "Epoch 445/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2522.8770 - root_mean_squared_error: 50.2282\n",
            "Epoch 446/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2515.4460 - root_mean_squared_error: 50.1542\n",
            "Epoch 447/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2517.0007 - root_mean_squared_error: 50.1697\n",
            "Epoch 448/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2519.4009 - root_mean_squared_error: 50.1936\n",
            "Epoch 449/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2517.8572 - root_mean_squared_error: 50.1783\n",
            "Epoch 450/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2511.6792 - root_mean_squared_error: 50.1167\n",
            "Epoch 451/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2514.9321 - root_mean_squared_error: 50.1491\n",
            "Epoch 452/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2519.6311 - root_mean_squared_error: 50.1959\n",
            "Epoch 453/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2523.4600 - root_mean_squared_error: 50.2341\n",
            "Epoch 454/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2520.8403 - root_mean_squared_error: 50.2080\n",
            "Epoch 455/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2511.8430 - root_mean_squared_error: 50.1183\n",
            "Epoch 456/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2506.9900 - root_mean_squared_error: 50.0699\n",
            "Epoch 457/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2516.4390 - root_mean_squared_error: 50.1641\n",
            "Epoch 458/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2523.0107 - root_mean_squared_error: 50.2296\n",
            "Epoch 459/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2509.4534 - root_mean_squared_error: 50.0944\n",
            "Epoch 460/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2515.8474 - root_mean_squared_error: 50.1582\n",
            "Epoch 461/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2510.9419 - root_mean_squared_error: 50.1093\n",
            "Epoch 462/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2511.1809 - root_mean_squared_error: 50.1117\n",
            "Epoch 463/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2510.9814 - root_mean_squared_error: 50.1097\n",
            "Epoch 464/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2504.5137 - root_mean_squared_error: 50.0451\n",
            "Epoch 465/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2506.8562 - root_mean_squared_error: 50.0685\n",
            "Epoch 466/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2500.9714 - root_mean_squared_error: 50.0097\n",
            "Epoch 467/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2513.1143 - root_mean_squared_error: 50.1310\n",
            "Epoch 468/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2545.0972 - root_mean_squared_error: 50.4490\n",
            "Epoch 469/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2506.5422 - root_mean_squared_error: 50.0654\n",
            "Epoch 470/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2500.0786 - root_mean_squared_error: 50.0008\n",
            "Epoch 471/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2497.0522 - root_mean_squared_error: 49.9705\n",
            "Epoch 472/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2503.6389 - root_mean_squared_error: 50.0364\n",
            "Epoch 473/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2500.0291 - root_mean_squared_error: 50.0003\n",
            "Epoch 474/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2520.2070 - root_mean_squared_error: 50.2017\n",
            "Epoch 475/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2499.2207 - root_mean_squared_error: 49.9922\n",
            "Epoch 476/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2500.9302 - root_mean_squared_error: 50.0093\n",
            "Epoch 477/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2552.3716 - root_mean_squared_error: 50.5210\n",
            "Epoch 478/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2544.3896 - root_mean_squared_error: 50.4419\n",
            "Epoch 479/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2528.6091 - root_mean_squared_error: 50.2853\n",
            "Epoch 480/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2519.5066 - root_mean_squared_error: 50.1947\n",
            "Epoch 481/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2503.4910 - root_mean_squared_error: 50.0349\n",
            "Epoch 482/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2495.5942 - root_mean_squared_error: 49.9559\n",
            "Epoch 483/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2499.4563 - root_mean_squared_error: 49.9946\n",
            "Epoch 484/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2494.3208 - root_mean_squared_error: 49.9432\n",
            "Epoch 485/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2488.3950 - root_mean_squared_error: 49.8838\n",
            "Epoch 486/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2495.7393 - root_mean_squared_error: 49.9574\n",
            "Epoch 487/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2495.3792 - root_mean_squared_error: 49.9538\n",
            "Epoch 488/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2496.6721 - root_mean_squared_error: 49.9667\n",
            "Epoch 489/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2492.4209 - root_mean_squared_error: 49.9242\n",
            "Epoch 490/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2494.2095 - root_mean_squared_error: 49.9421\n",
            "Epoch 491/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2485.3325 - root_mean_squared_error: 49.8531\n",
            "Epoch 492/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2486.0520 - root_mean_squared_error: 49.8603\n",
            "Epoch 493/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2486.3440 - root_mean_squared_error: 49.8633\n",
            "Epoch 494/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2488.5286 - root_mean_squared_error: 49.8852\n",
            "Epoch 495/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2492.8843 - root_mean_squared_error: 49.9288\n",
            "Epoch 496/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2511.2373 - root_mean_squared_error: 50.1122\n",
            "Epoch 497/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2488.2668 - root_mean_squared_error: 49.8825\n",
            "Epoch 498/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2489.2290 - root_mean_squared_error: 49.8922\n",
            "Epoch 499/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2485.3086 - root_mean_squared_error: 49.8529\n",
            "Epoch 500/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2501.7065 - root_mean_squared_error: 50.0171\n",
            "Epoch 501/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2482.7896 - root_mean_squared_error: 49.8276\n",
            "Epoch 502/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2483.7791 - root_mean_squared_error: 49.8375\n",
            "Epoch 503/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2478.4866 - root_mean_squared_error: 49.7844\n",
            "Epoch 504/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2478.7642 - root_mean_squared_error: 49.7872\n",
            "Epoch 505/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2487.7988 - root_mean_squared_error: 49.8778\n",
            "Epoch 506/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2491.0540 - root_mean_squared_error: 49.9105\n",
            "Epoch 507/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2475.8162 - root_mean_squared_error: 49.7576\n",
            "Epoch 508/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2482.8267 - root_mean_squared_error: 49.8280\n",
            "Epoch 509/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2485.6177 - root_mean_squared_error: 49.8560\n",
            "Epoch 510/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2484.1216 - root_mean_squared_error: 49.8410\n",
            "Epoch 511/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2483.0554 - root_mean_squared_error: 49.8303\n",
            "Epoch 512/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2487.8345 - root_mean_squared_error: 49.8782\n",
            "Epoch 513/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2475.8525 - root_mean_squared_error: 49.7579\n",
            "Epoch 514/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2475.2251 - root_mean_squared_error: 49.7516\n",
            "Epoch 515/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2471.9272 - root_mean_squared_error: 49.7185\n",
            "Epoch 516/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2468.7764 - root_mean_squared_error: 49.6868\n",
            "Epoch 517/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2484.4963 - root_mean_squared_error: 49.8447\n",
            "Epoch 518/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2473.8845 - root_mean_squared_error: 49.7382\n",
            "Epoch 519/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2489.3135 - root_mean_squared_error: 49.8930\n",
            "Epoch 520/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2501.3999 - root_mean_squared_error: 50.0140\n",
            "Epoch 521/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2475.5293 - root_mean_squared_error: 49.7547\n",
            "Epoch 522/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2495.2319 - root_mean_squared_error: 49.9523\n",
            "Epoch 523/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2488.9792 - root_mean_squared_error: 49.8897\n",
            "Epoch 524/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2474.3679 - root_mean_squared_error: 49.7430\n",
            "Epoch 525/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2477.3765 - root_mean_squared_error: 49.7733\n",
            "Epoch 526/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2475.6738 - root_mean_squared_error: 49.7561\n",
            "Epoch 527/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2463.7354 - root_mean_squared_error: 49.6360\n",
            "Epoch 528/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2471.7358 - root_mean_squared_error: 49.7166\n",
            "Epoch 529/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2462.6709 - root_mean_squared_error: 49.6253\n",
            "Epoch 530/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2469.7070 - root_mean_squared_error: 49.6961\n",
            "Epoch 531/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2473.2427 - root_mean_squared_error: 49.7317\n",
            "Epoch 532/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2467.1694 - root_mean_squared_error: 49.6706\n",
            "Epoch 533/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2463.8452 - root_mean_squared_error: 49.6371\n",
            "Epoch 534/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2460.4470 - root_mean_squared_error: 49.6029\n",
            "Epoch 535/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2472.7332 - root_mean_squared_error: 49.7266\n",
            "Epoch 536/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2456.7380 - root_mean_squared_error: 49.5655\n",
            "Epoch 537/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2455.9539 - root_mean_squared_error: 49.5576\n",
            "Epoch 538/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2458.4631 - root_mean_squared_error: 49.5829\n",
            "Epoch 539/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2455.5732 - root_mean_squared_error: 49.5537\n",
            "Epoch 540/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2480.6980 - root_mean_squared_error: 49.8066\n",
            "Epoch 541/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2476.5752 - root_mean_squared_error: 49.7652\n",
            "Epoch 542/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2467.1877 - root_mean_squared_error: 49.6708\n",
            "Epoch 543/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2458.5408 - root_mean_squared_error: 49.5837\n",
            "Epoch 544/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2463.3335 - root_mean_squared_error: 49.6320\n",
            "Epoch 545/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2452.9155 - root_mean_squared_error: 49.5269\n",
            "Epoch 546/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2475.8914 - root_mean_squared_error: 49.7583\n",
            "Epoch 547/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2466.8823 - root_mean_squared_error: 49.6677\n",
            "Epoch 548/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2460.5623 - root_mean_squared_error: 49.6041\n",
            "Epoch 549/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2456.2153 - root_mean_squared_error: 49.5602\n",
            "Epoch 550/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2463.6675 - root_mean_squared_error: 49.6353\n",
            "Epoch 551/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2465.9214 - root_mean_squared_error: 49.6580\n",
            "Epoch 552/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2462.3979 - root_mean_squared_error: 49.6226\n",
            "Epoch 553/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2449.2744 - root_mean_squared_error: 49.4901\n",
            "Epoch 554/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2459.1594 - root_mean_squared_error: 49.5899\n",
            "Epoch 555/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2462.7871 - root_mean_squared_error: 49.6265\n",
            "Epoch 556/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2450.1714 - root_mean_squared_error: 49.4992\n",
            "Epoch 557/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2445.3027 - root_mean_squared_error: 49.4500\n",
            "Epoch 558/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2450.9360 - root_mean_squared_error: 49.5069\n",
            "Epoch 559/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2453.3923 - root_mean_squared_error: 49.5317\n",
            "Epoch 560/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2469.5715 - root_mean_squared_error: 49.6948\n",
            "Epoch 561/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2458.3547 - root_mean_squared_error: 49.5818\n",
            "Epoch 562/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2460.0439 - root_mean_squared_error: 49.5988\n",
            "Epoch 563/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2448.4480 - root_mean_squared_error: 49.4818\n",
            "Epoch 564/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2449.9561 - root_mean_squared_error: 49.4970\n",
            "Epoch 565/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2440.7329 - root_mean_squared_error: 49.4038\n",
            "Epoch 566/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2440.8594 - root_mean_squared_error: 49.4051\n",
            "Epoch 567/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2452.4211 - root_mean_squared_error: 49.5219\n",
            "Epoch 568/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2454.3391 - root_mean_squared_error: 49.5413\n",
            "Epoch 569/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2444.1030 - root_mean_squared_error: 49.4379\n",
            "Epoch 570/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2437.5112 - root_mean_squared_error: 49.3712\n",
            "Epoch 571/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2454.6699 - root_mean_squared_error: 49.5446\n",
            "Epoch 572/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2473.2358 - root_mean_squared_error: 49.7316\n",
            "Epoch 573/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2453.2690 - root_mean_squared_error: 49.5305\n",
            "Epoch 574/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2438.9175 - root_mean_squared_error: 49.3854\n",
            "Epoch 575/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2437.4502 - root_mean_squared_error: 49.3705\n",
            "Epoch 576/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2444.7754 - root_mean_squared_error: 49.4447\n",
            "Epoch 577/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2448.2739 - root_mean_squared_error: 49.4800\n",
            "Epoch 578/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2438.3049 - root_mean_squared_error: 49.3792\n",
            "Epoch 579/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2444.8103 - root_mean_squared_error: 49.4450\n",
            "Epoch 580/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2440.8464 - root_mean_squared_error: 49.4049\n",
            "Epoch 581/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2439.8945 - root_mean_squared_error: 49.3953\n",
            "Epoch 582/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2437.7004 - root_mean_squared_error: 49.3731\n",
            "Epoch 583/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2440.2905 - root_mean_squared_error: 49.3993\n",
            "Epoch 584/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2438.0283 - root_mean_squared_error: 49.3764\n",
            "Epoch 585/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2435.2275 - root_mean_squared_error: 49.3480\n",
            "Epoch 586/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2435.3738 - root_mean_squared_error: 49.3495\n",
            "Epoch 587/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2434.8137 - root_mean_squared_error: 49.3438\n",
            "Epoch 588/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2434.5039 - root_mean_squared_error: 49.3407\n",
            "Epoch 589/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2434.2727 - root_mean_squared_error: 49.3383\n",
            "Epoch 590/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2431.1914 - root_mean_squared_error: 49.3071\n",
            "Epoch 591/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2432.5918 - root_mean_squared_error: 49.3213\n",
            "Epoch 592/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2441.3276 - root_mean_squared_error: 49.4098\n",
            "Epoch 593/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2445.7681 - root_mean_squared_error: 49.4547\n",
            "Epoch 594/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2437.3638 - root_mean_squared_error: 49.3697\n",
            "Epoch 595/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2425.2715 - root_mean_squared_error: 49.2470\n",
            "Epoch 596/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2429.5449 - root_mean_squared_error: 49.2904\n",
            "Epoch 597/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2430.7852 - root_mean_squared_error: 49.3030\n",
            "Epoch 598/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2435.7476 - root_mean_squared_error: 49.3533\n",
            "Epoch 599/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2427.0662 - root_mean_squared_error: 49.2653\n",
            "Epoch 600/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2421.4919 - root_mean_squared_error: 49.2087\n",
            "Epoch 601/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2423.3711 - root_mean_squared_error: 49.2277\n",
            "Epoch 602/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2428.6169 - root_mean_squared_error: 49.2810\n",
            "Epoch 603/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2422.3455 - root_mean_squared_error: 49.2173\n",
            "Epoch 604/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2420.0513 - root_mean_squared_error: 49.1940\n",
            "Epoch 605/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2424.7341 - root_mean_squared_error: 49.2416\n",
            "Epoch 606/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2419.1143 - root_mean_squared_error: 49.1845\n",
            "Epoch 607/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2425.5720 - root_mean_squared_error: 49.2501\n",
            "Epoch 608/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2430.8640 - root_mean_squared_error: 49.3038\n",
            "Epoch 609/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2414.9841 - root_mean_squared_error: 49.1425\n",
            "Epoch 610/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2423.1475 - root_mean_squared_error: 49.2255\n",
            "Epoch 611/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2431.6951 - root_mean_squared_error: 49.3122\n",
            "Epoch 612/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2427.5032 - root_mean_squared_error: 49.2697\n",
            "Epoch 613/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2440.8228 - root_mean_squared_error: 49.4047\n",
            "Epoch 614/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2437.2478 - root_mean_squared_error: 49.3685\n",
            "Epoch 615/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2427.1064 - root_mean_squared_error: 49.2657\n",
            "Epoch 616/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2416.6733 - root_mean_squared_error: 49.1597\n",
            "Epoch 617/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2414.4700 - root_mean_squared_error: 49.1373\n",
            "Epoch 618/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2437.3955 - root_mean_squared_error: 49.3700\n",
            "Epoch 619/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2424.1594 - root_mean_squared_error: 49.2358\n",
            "Epoch 620/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2417.1729 - root_mean_squared_error: 49.1648\n",
            "Epoch 621/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2416.8730 - root_mean_squared_error: 49.1617\n",
            "Epoch 622/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2446.1194 - root_mean_squared_error: 49.4583\n",
            "Epoch 623/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2443.0540 - root_mean_squared_error: 49.4273\n",
            "Epoch 624/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2426.2966 - root_mean_squared_error: 49.2575\n",
            "Epoch 625/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2411.6736 - root_mean_squared_error: 49.1088\n",
            "Epoch 626/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2419.6499 - root_mean_squared_error: 49.1899\n",
            "Epoch 627/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2429.1631 - root_mean_squared_error: 49.2865\n",
            "Epoch 628/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2417.3318 - root_mean_squared_error: 49.1664\n",
            "Epoch 629/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2411.4695 - root_mean_squared_error: 49.1067\n",
            "Epoch 630/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2405.0950 - root_mean_squared_error: 49.0418\n",
            "Epoch 631/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2408.9160 - root_mean_squared_error: 49.0807\n",
            "Epoch 632/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2410.0955 - root_mean_squared_error: 49.0927\n",
            "Epoch 633/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2410.0989 - root_mean_squared_error: 49.0928\n",
            "Epoch 634/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2407.4463 - root_mean_squared_error: 49.0657\n",
            "Epoch 635/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2410.7012 - root_mean_squared_error: 49.0989\n",
            "Epoch 636/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2410.1418 - root_mean_squared_error: 49.0932\n",
            "Epoch 637/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2420.6462 - root_mean_squared_error: 49.2001\n",
            "Epoch 638/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2418.2715 - root_mean_squared_error: 49.1759\n",
            "Epoch 639/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2414.0703 - root_mean_squared_error: 49.1332\n",
            "Epoch 640/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2410.4414 - root_mean_squared_error: 49.0962\n",
            "Epoch 641/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2418.1643 - root_mean_squared_error: 49.1748\n",
            "Epoch 642/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2405.0557 - root_mean_squared_error: 49.0414\n",
            "Epoch 643/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2406.9604 - root_mean_squared_error: 49.0608\n",
            "Epoch 644/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2418.6787 - root_mean_squared_error: 49.1801\n",
            "Epoch 645/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2465.9922 - root_mean_squared_error: 49.6588\n",
            "Epoch 646/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2427.6169 - root_mean_squared_error: 49.2709\n",
            "Epoch 647/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2401.3442 - root_mean_squared_error: 49.0035\n",
            "Epoch 648/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2402.7412 - root_mean_squared_error: 49.0178\n",
            "Epoch 649/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2403.8953 - root_mean_squared_error: 49.0295\n",
            "Epoch 650/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2407.6140 - root_mean_squared_error: 49.0674\n",
            "Epoch 651/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2416.3708 - root_mean_squared_error: 49.1566\n",
            "Epoch 652/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2408.9517 - root_mean_squared_error: 49.0811\n",
            "Epoch 653/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2402.2100 - root_mean_squared_error: 49.0123\n",
            "Epoch 654/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2391.8188 - root_mean_squared_error: 48.9062\n",
            "Epoch 655/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2401.6611 - root_mean_squared_error: 49.0067\n",
            "Epoch 656/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2398.5618 - root_mean_squared_error: 48.9751\n",
            "Epoch 657/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2393.0386 - root_mean_squared_error: 48.9187\n",
            "Epoch 658/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2398.4993 - root_mean_squared_error: 48.9745\n",
            "Epoch 659/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2401.0500 - root_mean_squared_error: 49.0005\n",
            "Epoch 660/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2397.9543 - root_mean_squared_error: 48.9689\n",
            "Epoch 661/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2424.3547 - root_mean_squared_error: 49.2377\n",
            "Epoch 662/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2417.8201 - root_mean_squared_error: 49.1713\n",
            "Epoch 663/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2391.9314 - root_mean_squared_error: 48.9074\n",
            "Epoch 664/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2390.8633 - root_mean_squared_error: 48.8965\n",
            "Epoch 665/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2391.5701 - root_mean_squared_error: 48.9037\n",
            "Epoch 666/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2394.5637 - root_mean_squared_error: 48.9343\n",
            "Epoch 667/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2398.5869 - root_mean_squared_error: 48.9754\n",
            "Epoch 668/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2397.2249 - root_mean_squared_error: 48.9615\n",
            "Epoch 669/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2392.8105 - root_mean_squared_error: 48.9164\n",
            "Epoch 670/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2386.5728 - root_mean_squared_error: 48.8526\n",
            "Epoch 671/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2391.8625 - root_mean_squared_error: 48.9067\n",
            "Epoch 672/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2403.9700 - root_mean_squared_error: 49.0303\n",
            "Epoch 673/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2389.7314 - root_mean_squared_error: 48.8849\n",
            "Epoch 674/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2384.9038 - root_mean_squared_error: 48.8355\n",
            "Epoch 675/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2388.1685 - root_mean_squared_error: 48.8689\n",
            "Epoch 676/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2382.8499 - root_mean_squared_error: 48.8144\n",
            "Epoch 677/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2387.9468 - root_mean_squared_error: 48.8666\n",
            "Epoch 678/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2389.2175 - root_mean_squared_error: 48.8796\n",
            "Epoch 679/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2383.8657 - root_mean_squared_error: 48.8248\n",
            "Epoch 680/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2386.1057 - root_mean_squared_error: 48.8478\n",
            "Epoch 681/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2384.4736 - root_mean_squared_error: 48.8311\n",
            "Epoch 682/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2395.1228 - root_mean_squared_error: 48.9400\n",
            "Epoch 683/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2402.1917 - root_mean_squared_error: 49.0122\n",
            "Epoch 684/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2386.5391 - root_mean_squared_error: 48.8522\n",
            "Epoch 685/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2389.8123 - root_mean_squared_error: 48.8857\n",
            "Epoch 686/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2375.8208 - root_mean_squared_error: 48.7424\n",
            "Epoch 687/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2381.7832 - root_mean_squared_error: 48.8035\n",
            "Epoch 688/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2393.7051 - root_mean_squared_error: 48.9255\n",
            "Epoch 689/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2388.3247 - root_mean_squared_error: 48.8705\n",
            "Epoch 690/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2388.9104 - root_mean_squared_error: 48.8765\n",
            "Epoch 691/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2396.1003 - root_mean_squared_error: 48.9500\n",
            "Epoch 692/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2386.2500 - root_mean_squared_error: 48.8493\n",
            "Epoch 693/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2375.1746 - root_mean_squared_error: 48.7358\n",
            "Epoch 694/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2374.1765 - root_mean_squared_error: 48.7255\n",
            "Epoch 695/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2377.5952 - root_mean_squared_error: 48.7606\n",
            "Epoch 696/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2377.3298 - root_mean_squared_error: 48.7579\n",
            "Epoch 697/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2371.9731 - root_mean_squared_error: 48.7029\n",
            "Epoch 698/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2385.8132 - root_mean_squared_error: 48.8448\n",
            "Epoch 699/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2374.5540 - root_mean_squared_error: 48.7294\n",
            "Epoch 700/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2371.9204 - root_mean_squared_error: 48.7024\n",
            "Epoch 701/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2369.1008 - root_mean_squared_error: 48.6734\n",
            "Epoch 702/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2371.8657 - root_mean_squared_error: 48.7018\n",
            "Epoch 703/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2373.3071 - root_mean_squared_error: 48.7166\n",
            "Epoch 704/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2384.6182 - root_mean_squared_error: 48.8326\n",
            "Epoch 705/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2369.1965 - root_mean_squared_error: 48.6744\n",
            "Epoch 706/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2367.7539 - root_mean_squared_error: 48.6596\n",
            "Epoch 707/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2370.9485 - root_mean_squared_error: 48.6924\n",
            "Epoch 708/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2370.1855 - root_mean_squared_error: 48.6846\n",
            "Epoch 709/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2370.9016 - root_mean_squared_error: 48.6919\n",
            "Epoch 710/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2377.3425 - root_mean_squared_error: 48.7580\n",
            "Epoch 711/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2384.4585 - root_mean_squared_error: 48.8309\n",
            "Epoch 712/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2381.1094 - root_mean_squared_error: 48.7966\n",
            "Epoch 713/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2369.1895 - root_mean_squared_error: 48.6743\n",
            "Epoch 714/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2377.3623 - root_mean_squared_error: 48.7582\n",
            "Epoch 715/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2390.7771 - root_mean_squared_error: 48.8956\n",
            "Epoch 716/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2381.5400 - root_mean_squared_error: 48.8010\n",
            "Epoch 717/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2370.0559 - root_mean_squared_error: 48.6832\n",
            "Epoch 718/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2368.8674 - root_mean_squared_error: 48.6710\n",
            "Epoch 719/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2366.7412 - root_mean_squared_error: 48.6492\n",
            "Epoch 720/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2374.3264 - root_mean_squared_error: 48.7271\n",
            "Epoch 721/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2359.5500 - root_mean_squared_error: 48.5752\n",
            "Epoch 722/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2362.1807 - root_mean_squared_error: 48.6023\n",
            "Epoch 723/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2362.5208 - root_mean_squared_error: 48.6058\n",
            "Epoch 724/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2363.4846 - root_mean_squared_error: 48.6157\n",
            "Epoch 725/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2361.8762 - root_mean_squared_error: 48.5991\n",
            "Epoch 726/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2363.2358 - root_mean_squared_error: 48.6131\n",
            "Epoch 727/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2357.8516 - root_mean_squared_error: 48.5577\n",
            "Epoch 728/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2367.6924 - root_mean_squared_error: 48.6589\n",
            "Epoch 729/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2361.4819 - root_mean_squared_error: 48.5951\n",
            "Epoch 730/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2369.4756 - root_mean_squared_error: 48.6773\n",
            "Epoch 731/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2372.3640 - root_mean_squared_error: 48.7069\n",
            "Epoch 732/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2363.5088 - root_mean_squared_error: 48.6159\n",
            "Epoch 733/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2363.7043 - root_mean_squared_error: 48.6179\n",
            "Epoch 734/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2356.0437 - root_mean_squared_error: 48.5391\n",
            "Epoch 735/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2354.5869 - root_mean_squared_error: 48.5241\n",
            "Epoch 736/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2365.5544 - root_mean_squared_error: 48.6370\n",
            "Epoch 737/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2359.9197 - root_mean_squared_error: 48.5790\n",
            "Epoch 738/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2358.7256 - root_mean_squared_error: 48.5667\n",
            "Epoch 739/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2369.2488 - root_mean_squared_error: 48.6749\n",
            "Epoch 740/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2369.4619 - root_mean_squared_error: 48.6771\n",
            "Epoch 741/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2371.7505 - root_mean_squared_error: 48.7006\n",
            "Epoch 742/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2365.3130 - root_mean_squared_error: 48.6345\n",
            "Epoch 743/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2350.2114 - root_mean_squared_error: 48.4790\n",
            "Epoch 744/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2362.3889 - root_mean_squared_error: 48.6044\n",
            "Epoch 745/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2356.8989 - root_mean_squared_error: 48.5479\n",
            "Epoch 746/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2352.9944 - root_mean_squared_error: 48.5077\n",
            "Epoch 747/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2351.3489 - root_mean_squared_error: 48.4907\n",
            "Epoch 748/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2349.8096 - root_mean_squared_error: 48.4748\n",
            "Epoch 749/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2350.4927 - root_mean_squared_error: 48.4819\n",
            "Epoch 750/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2364.2944 - root_mean_squared_error: 48.6240\n",
            "Epoch 751/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2355.8518 - root_mean_squared_error: 48.5371\n",
            "Epoch 752/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2350.7129 - root_mean_squared_error: 48.4841\n",
            "Epoch 753/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2355.6367 - root_mean_squared_error: 48.5349\n",
            "Epoch 754/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2356.7798 - root_mean_squared_error: 48.5467\n",
            "Epoch 755/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2348.7261 - root_mean_squared_error: 48.4637\n",
            "Epoch 756/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2373.4775 - root_mean_squared_error: 48.7183\n",
            "Epoch 757/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2361.2463 - root_mean_squared_error: 48.5927\n",
            "Epoch 758/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2351.2737 - root_mean_squared_error: 48.4899\n",
            "Epoch 759/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2352.9771 - root_mean_squared_error: 48.5075\n",
            "Epoch 760/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2344.1641 - root_mean_squared_error: 48.4166\n",
            "Epoch 761/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2346.6880 - root_mean_squared_error: 48.4426\n",
            "Epoch 762/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2339.1941 - root_mean_squared_error: 48.3652\n",
            "Epoch 763/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2340.4370 - root_mean_squared_error: 48.3781\n",
            "Epoch 764/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2352.9956 - root_mean_squared_error: 48.5077\n",
            "Epoch 765/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2377.3928 - root_mean_squared_error: 48.7585\n",
            "Epoch 766/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2370.1150 - root_mean_squared_error: 48.6838\n",
            "Epoch 767/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2344.2842 - root_mean_squared_error: 48.4178\n",
            "Epoch 768/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2342.9507 - root_mean_squared_error: 48.4040\n",
            "Epoch 769/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2347.7927 - root_mean_squared_error: 48.4540\n",
            "Epoch 770/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2340.3162 - root_mean_squared_error: 48.3768\n",
            "Epoch 771/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2353.4573 - root_mean_squared_error: 48.5124\n",
            "Epoch 772/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2354.9460 - root_mean_squared_error: 48.5278\n",
            "Epoch 773/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2343.8347 - root_mean_squared_error: 48.4132\n",
            "Epoch 774/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2355.7761 - root_mean_squared_error: 48.5363\n",
            "Epoch 775/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2342.9507 - root_mean_squared_error: 48.4040\n",
            "Epoch 776/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2343.2961 - root_mean_squared_error: 48.4076\n",
            "Epoch 777/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2352.1897 - root_mean_squared_error: 48.4994\n",
            "Epoch 778/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2353.1289 - root_mean_squared_error: 48.5091\n",
            "Epoch 779/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2348.9971 - root_mean_squared_error: 48.4665\n",
            "Epoch 780/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2352.2004 - root_mean_squared_error: 48.4995\n",
            "Epoch 781/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2341.7429 - root_mean_squared_error: 48.3916\n",
            "Epoch 782/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2335.5010 - root_mean_squared_error: 48.3270\n",
            "Epoch 783/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2333.2173 - root_mean_squared_error: 48.3034\n",
            "Epoch 784/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2338.1858 - root_mean_squared_error: 48.3548\n",
            "Epoch 785/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2335.8184 - root_mean_squared_error: 48.3303\n",
            "Epoch 786/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2336.8127 - root_mean_squared_error: 48.3406\n",
            "Epoch 787/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2327.8662 - root_mean_squared_error: 48.2480\n",
            "Epoch 788/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2330.2629 - root_mean_squared_error: 48.2728\n",
            "Epoch 789/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2333.4070 - root_mean_squared_error: 48.3054\n",
            "Epoch 790/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2343.3081 - root_mean_squared_error: 48.4077\n",
            "Epoch 791/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2343.4038 - root_mean_squared_error: 48.4087\n",
            "Epoch 792/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2327.2732 - root_mean_squared_error: 48.2418\n",
            "Epoch 793/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2326.1372 - root_mean_squared_error: 48.2300\n",
            "Epoch 794/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2323.1990 - root_mean_squared_error: 48.1996\n",
            "Epoch 795/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2339.3445 - root_mean_squared_error: 48.3668\n",
            "Epoch 796/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2326.1851 - root_mean_squared_error: 48.2305\n",
            "Epoch 797/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2330.6663 - root_mean_squared_error: 48.2770\n",
            "Epoch 798/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2342.7783 - root_mean_squared_error: 48.4023\n",
            "Epoch 799/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2344.7747 - root_mean_squared_error: 48.4229\n",
            "Epoch 800/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2338.2551 - root_mean_squared_error: 48.3555\n",
            "Epoch 801/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2329.5525 - root_mean_squared_error: 48.2654\n",
            "Epoch 802/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2325.8381 - root_mean_squared_error: 48.2269\n",
            "Epoch 803/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2319.8118 - root_mean_squared_error: 48.1644\n",
            "Epoch 804/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2324.1609 - root_mean_squared_error: 48.2096\n",
            "Epoch 805/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2316.9451 - root_mean_squared_error: 48.1347\n",
            "Epoch 806/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2319.2927 - root_mean_squared_error: 48.1590\n",
            "Epoch 807/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2332.5251 - root_mean_squared_error: 48.2962\n",
            "Epoch 808/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2345.9011 - root_mean_squared_error: 48.4345\n",
            "Epoch 809/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2356.1697 - root_mean_squared_error: 48.5404\n",
            "Epoch 810/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2328.1919 - root_mean_squared_error: 48.2513\n",
            "Epoch 811/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2320.3481 - root_mean_squared_error: 48.1700\n",
            "Epoch 812/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2315.7700 - root_mean_squared_error: 48.1224\n",
            "Epoch 813/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2322.9382 - root_mean_squared_error: 48.1969\n",
            "Epoch 814/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2321.6606 - root_mean_squared_error: 48.1836\n",
            "Epoch 815/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2316.2012 - root_mean_squared_error: 48.1269\n",
            "Epoch 816/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2321.3435 - root_mean_squared_error: 48.1803\n",
            "Epoch 817/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2319.7424 - root_mean_squared_error: 48.1637\n",
            "Epoch 818/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2318.9231 - root_mean_squared_error: 48.1552\n",
            "Epoch 819/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2313.9534 - root_mean_squared_error: 48.1036\n",
            "Epoch 820/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2320.8032 - root_mean_squared_error: 48.1747\n",
            "Epoch 821/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2320.7654 - root_mean_squared_error: 48.1743\n",
            "Epoch 822/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2321.1130 - root_mean_squared_error: 48.1779\n",
            "Epoch 823/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2320.7561 - root_mean_squared_error: 48.1742\n",
            "Epoch 824/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2319.2458 - root_mean_squared_error: 48.1586\n",
            "Epoch 825/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2317.9363 - root_mean_squared_error: 48.1450\n",
            "Epoch 826/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2311.0837 - root_mean_squared_error: 48.0737\n",
            "Epoch 827/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2317.0598 - root_mean_squared_error: 48.1358\n",
            "Epoch 828/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2316.6348 - root_mean_squared_error: 48.1314\n",
            "Epoch 829/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2329.1997 - root_mean_squared_error: 48.2618\n",
            "Epoch 830/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2332.7285 - root_mean_squared_error: 48.2983\n",
            "Epoch 831/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2307.4219 - root_mean_squared_error: 48.0356\n",
            "Epoch 832/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2311.4495 - root_mean_squared_error: 48.0775\n",
            "Epoch 833/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2317.5774 - root_mean_squared_error: 48.1412\n",
            "Epoch 834/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2315.1699 - root_mean_squared_error: 48.1162\n",
            "Epoch 835/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2318.6841 - root_mean_squared_error: 48.1527\n",
            "Epoch 836/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2312.4646 - root_mean_squared_error: 48.0881\n",
            "Epoch 837/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2322.3445 - root_mean_squared_error: 48.1907\n",
            "Epoch 838/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2316.9585 - root_mean_squared_error: 48.1348\n",
            "Epoch 839/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2320.7151 - root_mean_squared_error: 48.1738\n",
            "Epoch 840/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2312.4656 - root_mean_squared_error: 48.0881\n",
            "Epoch 841/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2302.7339 - root_mean_squared_error: 47.9868\n",
            "Epoch 842/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2304.8118 - root_mean_squared_error: 48.0085\n",
            "Epoch 843/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2310.5615 - root_mean_squared_error: 48.0683\n",
            "Epoch 844/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2314.0625 - root_mean_squared_error: 48.1047\n",
            "Epoch 845/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2308.5779 - root_mean_squared_error: 48.0477\n",
            "Epoch 846/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2301.3191 - root_mean_squared_error: 47.9721\n",
            "Epoch 847/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2310.1726 - root_mean_squared_error: 48.0643\n",
            "Epoch 848/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2303.7141 - root_mean_squared_error: 47.9970\n",
            "Epoch 849/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2313.6719 - root_mean_squared_error: 48.1006\n",
            "Epoch 850/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2305.0996 - root_mean_squared_error: 48.0115\n",
            "Epoch 851/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2303.1628 - root_mean_squared_error: 47.9913\n",
            "Epoch 852/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2300.4868 - root_mean_squared_error: 47.9634\n",
            "Epoch 853/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2300.7283 - root_mean_squared_error: 47.9659\n",
            "Epoch 854/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2303.0071 - root_mean_squared_error: 47.9897\n",
            "Epoch 855/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2313.8452 - root_mean_squared_error: 48.1024\n",
            "Epoch 856/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2304.7825 - root_mean_squared_error: 48.0081\n",
            "Epoch 857/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2311.9521 - root_mean_squared_error: 48.0828\n",
            "Epoch 858/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2313.8132 - root_mean_squared_error: 48.1021\n",
            "Epoch 859/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2302.1921 - root_mean_squared_error: 47.9812\n",
            "Epoch 860/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2313.5918 - root_mean_squared_error: 48.0998\n",
            "Epoch 861/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2295.6284 - root_mean_squared_error: 47.9127\n",
            "Epoch 862/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2294.3618 - root_mean_squared_error: 47.8995\n",
            "Epoch 863/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2322.3909 - root_mean_squared_error: 48.1912\n",
            "Epoch 864/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2297.1555 - root_mean_squared_error: 47.9286\n",
            "Epoch 865/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2301.2278 - root_mean_squared_error: 47.9711\n",
            "Epoch 866/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2319.1904 - root_mean_squared_error: 48.1580\n",
            "Epoch 867/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2306.3879 - root_mean_squared_error: 48.0249\n",
            "Epoch 868/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2297.7104 - root_mean_squared_error: 47.9344\n",
            "Epoch 869/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2294.6797 - root_mean_squared_error: 47.9028\n",
            "Epoch 870/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2298.9773 - root_mean_squared_error: 47.9477\n",
            "Epoch 871/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2299.9333 - root_mean_squared_error: 47.9576\n",
            "Epoch 872/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2301.9500 - root_mean_squared_error: 47.9786\n",
            "Epoch 873/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2314.3247 - root_mean_squared_error: 48.1074\n",
            "Epoch 874/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2295.6736 - root_mean_squared_error: 47.9132\n",
            "Epoch 875/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2295.3281 - root_mean_squared_error: 47.9096\n",
            "Epoch 876/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2290.2590 - root_mean_squared_error: 47.8567\n",
            "Epoch 877/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2290.9331 - root_mean_squared_error: 47.8637\n",
            "Epoch 878/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2293.0764 - root_mean_squared_error: 47.8861\n",
            "Epoch 879/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2314.4524 - root_mean_squared_error: 48.1088\n",
            "Epoch 880/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2297.6516 - root_mean_squared_error: 47.9338\n",
            "Epoch 881/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2284.6282 - root_mean_squared_error: 47.7978\n",
            "Epoch 882/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2290.5483 - root_mean_squared_error: 47.8597\n",
            "Epoch 883/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2283.9751 - root_mean_squared_error: 47.7910\n",
            "Epoch 884/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2290.6045 - root_mean_squared_error: 47.8603\n",
            "Epoch 885/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2314.0632 - root_mean_squared_error: 48.1047\n",
            "Epoch 886/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2307.6235 - root_mean_squared_error: 48.0377\n",
            "Epoch 887/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2296.3115 - root_mean_squared_error: 47.9198\n",
            "Epoch 888/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2289.1069 - root_mean_squared_error: 47.8446\n",
            "Epoch 889/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2281.2886 - root_mean_squared_error: 47.7628\n",
            "Epoch 890/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2285.0938 - root_mean_squared_error: 47.8027\n",
            "Epoch 891/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2285.1106 - root_mean_squared_error: 47.8028\n",
            "Epoch 892/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2282.4011 - root_mean_squared_error: 47.7745\n",
            "Epoch 893/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2283.9307 - root_mean_squared_error: 47.7905\n",
            "Epoch 894/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2283.6963 - root_mean_squared_error: 47.7880\n",
            "Epoch 895/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2279.8232 - root_mean_squared_error: 47.7475\n",
            "Epoch 896/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2287.9658 - root_mean_squared_error: 47.8327\n",
            "Epoch 897/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2287.3594 - root_mean_squared_error: 47.8263\n",
            "Epoch 898/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2285.6545 - root_mean_squared_error: 47.8085\n",
            "Epoch 899/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2279.9695 - root_mean_squared_error: 47.7490\n",
            "Epoch 900/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2275.4255 - root_mean_squared_error: 47.7014\n",
            "Epoch 901/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2275.8167 - root_mean_squared_error: 47.7055\n",
            "Epoch 902/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2280.6873 - root_mean_squared_error: 47.7565\n",
            "Epoch 903/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2290.6670 - root_mean_squared_error: 47.8609\n",
            "Epoch 904/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2296.7966 - root_mean_squared_error: 47.9249\n",
            "Epoch 905/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2299.1028 - root_mean_squared_error: 47.9490\n",
            "Epoch 906/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2282.8464 - root_mean_squared_error: 47.7791\n",
            "Epoch 907/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2282.0625 - root_mean_squared_error: 47.7709\n",
            "Epoch 908/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2271.9607 - root_mean_squared_error: 47.6651\n",
            "Epoch 909/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2272.4133 - root_mean_squared_error: 47.6698\n",
            "Epoch 910/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2275.2588 - root_mean_squared_error: 47.6997\n",
            "Epoch 911/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2276.5911 - root_mean_squared_error: 47.7136\n",
            "Epoch 912/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2283.5925 - root_mean_squared_error: 47.7869\n",
            "Epoch 913/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2281.7622 - root_mean_squared_error: 47.7678\n",
            "Epoch 914/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2273.0686 - root_mean_squared_error: 47.6767\n",
            "Epoch 915/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2286.8843 - root_mean_squared_error: 47.8214\n",
            "Epoch 916/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2281.6477 - root_mean_squared_error: 47.7666\n",
            "Epoch 917/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2277.0042 - root_mean_squared_error: 47.7180\n",
            "Epoch 918/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2293.1648 - root_mean_squared_error: 47.8870\n",
            "Epoch 919/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2273.6108 - root_mean_squared_error: 47.6824\n",
            "Epoch 920/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2275.8789 - root_mean_squared_error: 47.7062\n",
            "Epoch 921/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2275.7957 - root_mean_squared_error: 47.7053\n",
            "Epoch 922/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2276.2041 - root_mean_squared_error: 47.7096\n",
            "Epoch 923/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2274.0198 - root_mean_squared_error: 47.6867\n",
            "Epoch 924/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2267.7393 - root_mean_squared_error: 47.6208\n",
            "Epoch 925/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2271.2444 - root_mean_squared_error: 47.6576\n",
            "Epoch 926/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2278.5381 - root_mean_squared_error: 47.7340\n",
            "Epoch 927/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2271.7842 - root_mean_squared_error: 47.6632\n",
            "Epoch 928/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2273.4548 - root_mean_squared_error: 47.6808\n",
            "Epoch 929/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2279.4292 - root_mean_squared_error: 47.7434\n",
            "Epoch 930/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2274.9714 - root_mean_squared_error: 47.6967\n",
            "Epoch 931/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2269.3464 - root_mean_squared_error: 47.6377\n",
            "Epoch 932/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2270.3140 - root_mean_squared_error: 47.6478\n",
            "Epoch 933/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2268.4756 - root_mean_squared_error: 47.6285\n",
            "Epoch 934/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2277.6885 - root_mean_squared_error: 47.7251\n",
            "Epoch 935/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2273.7695 - root_mean_squared_error: 47.6841\n",
            "Epoch 936/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2266.0486 - root_mean_squared_error: 47.6030\n",
            "Epoch 937/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2262.0408 - root_mean_squared_error: 47.5609\n",
            "Epoch 938/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2263.8074 - root_mean_squared_error: 47.5795\n",
            "Epoch 939/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2290.5872 - root_mean_squared_error: 47.8601\n",
            "Epoch 940/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2278.5422 - root_mean_squared_error: 47.7341\n",
            "Epoch 941/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2270.7461 - root_mean_squared_error: 47.6523\n",
            "Epoch 942/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2263.2302 - root_mean_squared_error: 47.5734\n",
            "Epoch 943/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2268.8474 - root_mean_squared_error: 47.6324\n",
            "Epoch 944/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2270.9922 - root_mean_squared_error: 47.6549\n",
            "Epoch 945/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2269.1843 - root_mean_squared_error: 47.6360\n",
            "Epoch 946/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2273.5444 - root_mean_squared_error: 47.6817\n",
            "Epoch 947/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2258.4382 - root_mean_squared_error: 47.5230\n",
            "Epoch 948/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2264.1812 - root_mean_squared_error: 47.5834\n",
            "Epoch 949/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2261.5642 - root_mean_squared_error: 47.5559\n",
            "Epoch 950/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2263.9233 - root_mean_squared_error: 47.5807\n",
            "Epoch 951/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2277.3274 - root_mean_squared_error: 47.7214\n",
            "Epoch 952/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2261.4060 - root_mean_squared_error: 47.5542\n",
            "Epoch 953/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2257.3679 - root_mean_squared_error: 47.5118\n",
            "Epoch 954/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2257.7976 - root_mean_squared_error: 47.5163\n",
            "Epoch 955/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2261.1294 - root_mean_squared_error: 47.5513\n",
            "Epoch 956/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2261.3840 - root_mean_squared_error: 47.5540\n",
            "Epoch 957/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2261.6462 - root_mean_squared_error: 47.5568\n",
            "Epoch 958/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2269.0330 - root_mean_squared_error: 47.6344\n",
            "Epoch 959/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2255.2173 - root_mean_squared_error: 47.4891\n",
            "Epoch 960/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2249.8613 - root_mean_squared_error: 47.4327\n",
            "Epoch 961/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2268.6838 - root_mean_squared_error: 47.6307\n",
            "Epoch 962/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2264.4636 - root_mean_squared_error: 47.5864\n",
            "Epoch 963/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2256.0325 - root_mean_squared_error: 47.4977\n",
            "Epoch 964/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2255.3030 - root_mean_squared_error: 47.4900\n",
            "Epoch 965/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2255.1833 - root_mean_squared_error: 47.4888\n",
            "Epoch 966/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2250.3079 - root_mean_squared_error: 47.4374\n",
            "Epoch 967/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2256.6431 - root_mean_squared_error: 47.5041\n",
            "Epoch 968/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2259.4976 - root_mean_squared_error: 47.5342\n",
            "Epoch 969/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2258.8611 - root_mean_squared_error: 47.5275\n",
            "Epoch 970/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2263.4045 - root_mean_squared_error: 47.5753\n",
            "Epoch 971/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2269.5300 - root_mean_squared_error: 47.6396\n",
            "Epoch 972/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2266.0732 - root_mean_squared_error: 47.6033\n",
            "Epoch 973/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2256.5654 - root_mean_squared_error: 47.5033\n",
            "Epoch 974/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2251.4719 - root_mean_squared_error: 47.4497\n",
            "Epoch 975/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2253.0154 - root_mean_squared_error: 47.4659\n",
            "Epoch 976/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2247.4182 - root_mean_squared_error: 47.4069\n",
            "Epoch 977/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2254.2820 - root_mean_squared_error: 47.4793\n",
            "Epoch 978/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2244.0676 - root_mean_squared_error: 47.3716\n",
            "Epoch 979/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2248.6655 - root_mean_squared_error: 47.4201\n",
            "Epoch 980/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2250.7441 - root_mean_squared_error: 47.4420\n",
            "Epoch 981/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2249.6807 - root_mean_squared_error: 47.4308\n",
            "Epoch 982/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2256.6404 - root_mean_squared_error: 47.5041\n",
            "Epoch 983/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2250.2188 - root_mean_squared_error: 47.4365\n",
            "Epoch 984/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2270.6406 - root_mean_squared_error: 47.6512\n",
            "Epoch 985/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2255.5168 - root_mean_squared_error: 47.4923\n",
            "Epoch 986/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2279.1943 - root_mean_squared_error: 47.7409\n",
            "Epoch 987/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2258.3560 - root_mean_squared_error: 47.5222\n",
            "Epoch 988/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2246.3547 - root_mean_squared_error: 47.3957\n",
            "Epoch 989/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2243.2310 - root_mean_squared_error: 47.3628\n",
            "Epoch 990/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2256.6716 - root_mean_squared_error: 47.5044\n",
            "Epoch 991/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2255.9497 - root_mean_squared_error: 47.4968\n",
            "Epoch 992/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2240.9907 - root_mean_squared_error: 47.3391\n",
            "Epoch 993/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2271.5872 - root_mean_squared_error: 47.6612\n",
            "Epoch 994/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2272.1506 - root_mean_squared_error: 47.6671\n",
            "Epoch 995/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2251.1890 - root_mean_squared_error: 47.4467\n",
            "Epoch 996/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2238.2407 - root_mean_squared_error: 47.3100\n",
            "Epoch 997/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2237.6392 - root_mean_squared_error: 47.3037\n",
            "Epoch 998/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2250.1030 - root_mean_squared_error: 47.4352\n",
            "Epoch 999/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2263.3743 - root_mean_squared_error: 47.5749\n",
            "Epoch 1000/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2240.5601 - root_mean_squared_error: 47.3346\n",
            "Epoch 1/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2397.1257 - root_mean_squared_error: 48.9604\n",
            "Epoch 2/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2365.8088 - root_mean_squared_error: 48.6396\n",
            "Epoch 3/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2386.8435 - root_mean_squared_error: 48.8553\n",
            "Epoch 4/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2319.1030 - root_mean_squared_error: 48.1571\n",
            "Epoch 5/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2336.1709 - root_mean_squared_error: 48.3340\n",
            "Epoch 6/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2317.7805 - root_mean_squared_error: 48.1433\n",
            "Epoch 7/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2357.6108 - root_mean_squared_error: 48.5552\n",
            "Epoch 8/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2343.0071 - root_mean_squared_error: 48.4046\n",
            "Epoch 9/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2313.9470 - root_mean_squared_error: 48.1035\n",
            "Epoch 10/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2291.6230 - root_mean_squared_error: 47.8709\n",
            "Epoch 11/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2282.2983 - root_mean_squared_error: 47.7734\n",
            "Epoch 12/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2318.8936 - root_mean_squared_error: 48.1549\n",
            "Epoch 13/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2317.2598 - root_mean_squared_error: 48.1379\n",
            "Epoch 14/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2293.5735 - root_mean_squared_error: 47.8913\n",
            "Epoch 15/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2275.8208 - root_mean_squared_error: 47.7056\n",
            "Epoch 16/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2313.6360 - root_mean_squared_error: 48.1003\n",
            "Epoch 17/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2283.5112 - root_mean_squared_error: 47.7861\n",
            "Epoch 18/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2273.5220 - root_mean_squared_error: 47.6815\n",
            "Epoch 19/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2280.1013 - root_mean_squared_error: 47.7504\n",
            "Epoch 20/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2305.3127 - root_mean_squared_error: 48.0137\n",
            "Epoch 21/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2287.3176 - root_mean_squared_error: 47.8259\n",
            "Epoch 22/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2266.7007 - root_mean_squared_error: 47.6099\n",
            "Epoch 23/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2261.1943 - root_mean_squared_error: 47.5520\n",
            "Epoch 24/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2258.0115 - root_mean_squared_error: 47.5185\n",
            "Epoch 25/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2281.1604 - root_mean_squared_error: 47.7615\n",
            "Epoch 26/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2263.2678 - root_mean_squared_error: 47.5738\n",
            "Epoch 27/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2255.2031 - root_mean_squared_error: 47.4890\n",
            "Epoch 28/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2274.0447 - root_mean_squared_error: 47.6869\n",
            "Epoch 29/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2264.4658 - root_mean_squared_error: 47.5864\n",
            "Epoch 30/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2257.3630 - root_mean_squared_error: 47.5117\n",
            "Epoch 31/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2301.8296 - root_mean_squared_error: 47.9774\n",
            "Epoch 32/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2287.9795 - root_mean_squared_error: 47.8328\n",
            "Epoch 33/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2283.5432 - root_mean_squared_error: 47.7864\n",
            "Epoch 34/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2273.0396 - root_mean_squared_error: 47.6764\n",
            "Epoch 35/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2280.3577 - root_mean_squared_error: 47.7531\n",
            "Epoch 36/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2244.4045 - root_mean_squared_error: 47.3751\n",
            "Epoch 37/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2269.6440 - root_mean_squared_error: 47.6408\n",
            "Epoch 38/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2243.3752 - root_mean_squared_error: 47.3643\n",
            "Epoch 39/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2255.9026 - root_mean_squared_error: 47.4963\n",
            "Epoch 40/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2288.0789 - root_mean_squared_error: 47.8339\n",
            "Epoch 41/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2257.0610 - root_mean_squared_error: 47.5085\n",
            "Epoch 42/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2289.6604 - root_mean_squared_error: 47.8504\n",
            "Epoch 43/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2267.1270 - root_mean_squared_error: 47.6144\n",
            "Epoch 44/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2266.0989 - root_mean_squared_error: 47.6036\n",
            "Epoch 45/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2258.2710 - root_mean_squared_error: 47.5213\n",
            "Epoch 46/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2235.2271 - root_mean_squared_error: 47.2782\n",
            "Epoch 47/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2256.1624 - root_mean_squared_error: 47.4991\n",
            "Epoch 48/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2294.0825 - root_mean_squared_error: 47.8966\n",
            "Epoch 49/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2245.2771 - root_mean_squared_error: 47.3844\n",
            "Epoch 50/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2247.7561 - root_mean_squared_error: 47.4105\n",
            "Epoch 51/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2247.0776 - root_mean_squared_error: 47.4034\n",
            "Epoch 52/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2265.8049 - root_mean_squared_error: 47.6005\n",
            "Epoch 53/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2231.3062 - root_mean_squared_error: 47.2367\n",
            "Epoch 54/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2222.8721 - root_mean_squared_error: 47.1473\n",
            "Epoch 55/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2269.0483 - root_mean_squared_error: 47.6345\n",
            "Epoch 56/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2231.7810 - root_mean_squared_error: 47.2417\n",
            "Epoch 57/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2249.0015 - root_mean_squared_error: 47.4236\n",
            "Epoch 58/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2250.8076 - root_mean_squared_error: 47.4427\n",
            "Epoch 59/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2261.3381 - root_mean_squared_error: 47.5535\n",
            "Epoch 60/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2208.9685 - root_mean_squared_error: 46.9997\n",
            "Epoch 61/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2249.7622 - root_mean_squared_error: 47.4317\n",
            "Epoch 62/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2244.2371 - root_mean_squared_error: 47.3734\n",
            "Epoch 63/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2232.3271 - root_mean_squared_error: 47.2475\n",
            "Epoch 64/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2228.6702 - root_mean_squared_error: 47.2088\n",
            "Epoch 65/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2215.3364 - root_mean_squared_error: 47.0674\n",
            "Epoch 66/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2212.7180 - root_mean_squared_error: 47.0395\n",
            "Epoch 67/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2203.2246 - root_mean_squared_error: 46.9385\n",
            "Epoch 68/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2245.4751 - root_mean_squared_error: 47.3864\n",
            "Epoch 69/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2226.3396 - root_mean_squared_error: 47.1841\n",
            "Epoch 70/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2225.6711 - root_mean_squared_error: 47.1770\n",
            "Epoch 71/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2239.2908 - root_mean_squared_error: 47.3211\n",
            "Epoch 72/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2214.9019 - root_mean_squared_error: 47.0627\n",
            "Epoch 73/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2221.3140 - root_mean_squared_error: 47.1308\n",
            "Epoch 74/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2212.5789 - root_mean_squared_error: 47.0381\n",
            "Epoch 75/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2229.1606 - root_mean_squared_error: 47.2140\n",
            "Epoch 76/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2216.5659 - root_mean_squared_error: 47.0804\n",
            "Epoch 77/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2208.0518 - root_mean_squared_error: 46.9899\n",
            "Epoch 78/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2218.4832 - root_mean_squared_error: 47.1008\n",
            "Epoch 79/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2214.8323 - root_mean_squared_error: 47.0620\n",
            "Epoch 80/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2234.5352 - root_mean_squared_error: 47.2709\n",
            "Epoch 81/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2224.1821 - root_mean_squared_error: 47.1612\n",
            "Epoch 82/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2211.5247 - root_mean_squared_error: 47.0269\n",
            "Epoch 83/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2197.2891 - root_mean_squared_error: 46.8753\n",
            "Epoch 84/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2200.5095 - root_mean_squared_error: 46.9096\n",
            "Epoch 85/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2233.6755 - root_mean_squared_error: 47.2618\n",
            "Epoch 86/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2214.9702 - root_mean_squared_error: 47.0635\n",
            "Epoch 87/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2218.4358 - root_mean_squared_error: 47.1003\n",
            "Epoch 88/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2184.0400 - root_mean_squared_error: 46.7337\n",
            "Epoch 89/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2244.2554 - root_mean_squared_error: 47.3736\n",
            "Epoch 90/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2196.1948 - root_mean_squared_error: 46.8636\n",
            "Epoch 91/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2201.3652 - root_mean_squared_error: 46.9187\n",
            "Epoch 92/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2196.8530 - root_mean_squared_error: 46.8706\n",
            "Epoch 93/100\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 2199.2258 - root_mean_squared_error: 46.8959\n",
            "Epoch 94/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2208.2488 - root_mean_squared_error: 46.9920\n",
            "Epoch 95/100\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 2260.0081 - root_mean_squared_error: 47.5395\n",
            "Epoch 96/100\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 2180.3577 - root_mean_squared_error: 46.6943\n",
            "Epoch 97/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2171.9041 - root_mean_squared_error: 46.6037\n",
            "Epoch 98/100\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 2180.3110 - root_mean_squared_error: 46.6938\n",
            "Epoch 99/100\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 2202.9358 - root_mean_squared_error: 46.9354\n",
            "Epoch 100/100\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2187.3184 - root_mean_squared_error: 46.7688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_historial.history.keys()\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig1 = px.line(epoch_historial.history['loss'])\n",
        "fig2 = px.line(epoch_historial.history['root_mean_squared_error'])\n",
        "\n",
        "go.Figure(fig1.data + fig2.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "yoA9RTMxE-C-",
        "outputId": "e72c7a7c-5ae3-4746-9371-edee02369b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"bf3a46e4-b03f-41af-9d58-9ebeb1e244f7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bf3a46e4-b03f-41af-9d58-9ebeb1e244f7\")) {                    Plotly.newPlot(                        \"bf3a46e4-b03f-41af-9d58-9ebeb1e244f7\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"xaxis\":\"x\",\"y\":[2397.125732421875,2365.808837890625,2386.843505859375,2319.10302734375,2336.1708984375,2317.780517578125,2357.61083984375,2343.007080078125,2313.947021484375,2291.623046875,2282.29833984375,2318.8935546875,2317.259765625,2293.573486328125,2275.82080078125,2313.635986328125,2283.51123046875,2273.52197265625,2280.101318359375,2305.312744140625,2287.317626953125,2266.70068359375,2261.1943359375,2258.011474609375,2281.160400390625,2263.267822265625,2255.203125,2274.044677734375,2264.4658203125,2257.363037109375,2301.82958984375,2287.9794921875,2283.543212890625,2273.03955078125,2280.357666015625,2244.404541015625,2269.64404296875,2243.375244140625,2255.902587890625,2288.078857421875,2257.06103515625,2289.660400390625,2267.126953125,2266.098876953125,2258.27099609375,2235.22705078125,2256.162353515625,2294.08251953125,2245.277099609375,2247.756103515625,2247.07763671875,2265.804931640625,2231.30615234375,2222.8720703125,2269.04833984375,2231.781005859375,2249.00146484375,2250.8076171875,2261.338134765625,2208.968505859375,2249.76220703125,2244.237060546875,2232.3271484375,2228.670166015625,2215.33642578125,2212.718017578125,2203.224609375,2245.47509765625,2226.339599609375,2225.671142578125,2239.290771484375,2214.90185546875,2221.31396484375,2212.578857421875,2229.16064453125,2216.56591796875,2208.0517578125,2218.483154296875,2214.832275390625,2234.53515625,2224.18212890625,2211.524658203125,2197.2890625,2200.509521484375,2233.675537109375,2214.97021484375,2218.435791015625,2184.0400390625,2244.25537109375,2196.19482421875,2201.365234375,2196.85302734375,2199.225830078125,2208.248779296875,2260.008056640625,2180.357666015625,2171.904052734375,2180.31103515625,2202.935791015625,2187.318359375],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"xaxis\":\"x\",\"y\":[48.96044921875,48.63957977294922,48.85533142089844,48.157066345214844,48.33395004272461,48.143333435058594,48.55523681640625,48.40461730957031,48.1035041809082,47.87089920043945,47.773406982421875,48.15489196777344,48.13792419433594,47.891265869140625,47.705562591552734,48.10026931762695,47.78609848022461,47.68146514892578,47.75040817260742,48.013671875,47.82590866088867,47.60987854003906,47.55201721191406,47.51853942871094,47.76149368286133,47.573814392089844,47.48897933959961,47.68694305419922,47.586402893066406,47.511714935302734,47.97739028930664,47.832828521728516,47.786434173583984,47.6764030456543,47.753089904785156,47.37514877319336,47.64078140258789,47.364280700683594,47.496341705322266,47.833866119384766,47.50853729248047,47.85039520263672,47.614356994628906,47.60356140136719,47.52126693725586,47.27819061279297,47.49907684326172,47.89657974243164,47.38435363769531,47.41050720214844,47.403350830078125,47.60047149658203,47.23670196533203,47.147342681884766,47.63452911376953,47.24173355102539,47.42363739013672,47.44268035888672,47.55352783203125,46.999664306640625,47.43165969848633,47.37337875366211,47.24750900268555,47.20879364013672,47.067359924316406,47.03953552246094,46.93851852416992,47.386444091796875,47.184104919433594,47.17702102661133,47.321144104003906,47.062744140625,47.13081741333008,47.03805923461914,47.2139892578125,47.080421447753906,46.98991012573242,47.10077667236328,47.06200408935547,47.270870208740234,47.161231994628906,47.026851654052734,46.87525177001953,46.90958786010742,47.261775970458984,47.06346893310547,47.10027313232422,46.73371505737305,47.373573303222656,46.86357498168945,46.91870880126953,46.870601654052734,46.895904541015625,46.992008209228516,47.53954315185547,46.69430160522461,46.60368728637695,46.69380187988281,46.93544387817383,46.768775939941406],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bf3a46e4-b03f-41af-9d58-9ebeb1e244f7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve2w8YT110Ac",
        "outputId": "565f918a-45e3-4ec3-8072-ff7468a2c84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 45.98804397710569\n",
            "R2: 0.902216342791594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(data_test)\n",
        "pred = np.array(pred.tolist()).reshape(1, -1).tolist()\n",
        "pd.DataFrame({'pred': pred[0]}).to_csv('hackcheek.csv', index=False)"
      ],
      "metadata": {
        "id": "daNGKtF-18ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop('cnt', axis=1)\n",
        "y = data_train['cnt']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "features = X_train\n",
        "target = y_train\n"
      ],
      "metadata": {
        "id": "Ypzy8XPmr6GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## keras"
      ],
      "metadata": {
        "id": "KqOKTZxnPxz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train.drop('cnt', axis=1)\n",
        "y = data_train['cnt']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "features = X_train\n",
        "target = y_train"
      ],
      "metadata": {
        "id": "1hGvkZnkP_a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model = tf.keras.Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(tf.keras.layers.Dense(600, kernel_initializer='normal', input_dim = features.shape[1], activation='tanh'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(tf.keras.layers.Dense(600, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(tf.keras.layers.Dense(300, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(tf.keras.layers.Dense(300, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(tf.keras.layers.Dense(100, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(tf.keras.layers.Dense(50, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(tf.keras.layers.Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhuLm_3A_auR",
        "outputId": "a7d95a6e-9cdd-472a-c733-3c42e5464eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 600)               8400      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 300)               180300    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 300)               90300     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 100)               30100     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 674,801\n",
            "Trainable params: 674,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto'\n",
        ")\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "EANS013BAQTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model.fit(features, target, epochs=128, batch_size=256, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4j7dvfACLCQ",
        "outputId": "6930a8f4-6e2d-4fae-c7a6-2b27a3d93672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/128\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.7293 - mean_absolute_error: 2.7293\n",
            "Epoch 1: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 2.7293 - mean_absolute_error: 2.7293 - val_loss: 20.5325 - val_mean_absolute_error: 20.5325\n",
            "Epoch 2/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1153 - mean_absolute_error: 2.1153\n",
            "Epoch 2: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 2.1153 - mean_absolute_error: 2.1153 - val_loss: 20.5679 - val_mean_absolute_error: 20.5679\n",
            "Epoch 3/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9267 - mean_absolute_error: 1.9267\n",
            "Epoch 3: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9652 - mean_absolute_error: 1.9652 - val_loss: 20.6541 - val_mean_absolute_error: 20.6541\n",
            "Epoch 4/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7681 - mean_absolute_error: 1.7681\n",
            "Epoch 4: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.7644 - mean_absolute_error: 1.7644 - val_loss: 20.5678 - val_mean_absolute_error: 20.5678\n",
            "Epoch 5/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7761 - mean_absolute_error: 1.7761\n",
            "Epoch 5: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.7675 - mean_absolute_error: 1.7675 - val_loss: 20.6024 - val_mean_absolute_error: 20.6024\n",
            "Epoch 6/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9127 - mean_absolute_error: 1.9127\n",
            "Epoch 6: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9033 - mean_absolute_error: 1.9033 - val_loss: 20.4791 - val_mean_absolute_error: 20.4791\n",
            "Epoch 7/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1002 - mean_absolute_error: 2.1002\n",
            "Epoch 7: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0927 - mean_absolute_error: 2.0927 - val_loss: 20.5611 - val_mean_absolute_error: 20.5611\n",
            "Epoch 8/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8745 - mean_absolute_error: 1.8745\n",
            "Epoch 8: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8637 - mean_absolute_error: 1.8637 - val_loss: 20.5072 - val_mean_absolute_error: 20.5072\n",
            "Epoch 9/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7133 - mean_absolute_error: 1.7133\n",
            "Epoch 9: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.7140 - mean_absolute_error: 1.7140 - val_loss: 20.6004 - val_mean_absolute_error: 20.6004\n",
            "Epoch 10/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8591 - mean_absolute_error: 1.8591\n",
            "Epoch 10: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.8630 - mean_absolute_error: 1.8630 - val_loss: 20.5389 - val_mean_absolute_error: 20.5389\n",
            "Epoch 11/128\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.6612 - mean_absolute_error: 1.6612\n",
            "Epoch 11: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.6612 - mean_absolute_error: 1.6612 - val_loss: 20.6776 - val_mean_absolute_error: 20.6776\n",
            "Epoch 12/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9652 - mean_absolute_error: 1.9652\n",
            "Epoch 12: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9497 - mean_absolute_error: 1.9497 - val_loss: 20.5723 - val_mean_absolute_error: 20.5723\n",
            "Epoch 13/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8260 - mean_absolute_error: 1.8260\n",
            "Epoch 13: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.8217 - mean_absolute_error: 1.8217 - val_loss: 20.4645 - val_mean_absolute_error: 20.4645\n",
            "Epoch 14/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8609 - mean_absolute_error: 1.8609\n",
            "Epoch 14: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.8493 - mean_absolute_error: 1.8493 - val_loss: 20.4949 - val_mean_absolute_error: 20.4949\n",
            "Epoch 15/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8790 - mean_absolute_error: 1.8790\n",
            "Epoch 15: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8895 - mean_absolute_error: 1.8895 - val_loss: 20.5566 - val_mean_absolute_error: 20.5566\n",
            "Epoch 16/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.2185 - mean_absolute_error: 2.2185\n",
            "Epoch 16: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 2.1988 - mean_absolute_error: 2.1988 - val_loss: 20.6800 - val_mean_absolute_error: 20.6800\n",
            "Epoch 17/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1519 - mean_absolute_error: 2.1519\n",
            "Epoch 17: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.1335 - mean_absolute_error: 2.1335 - val_loss: 20.3850 - val_mean_absolute_error: 20.3850\n",
            "Epoch 18/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9889 - mean_absolute_error: 1.9889\n",
            "Epoch 18: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0062 - mean_absolute_error: 2.0062 - val_loss: 20.5913 - val_mean_absolute_error: 20.5913\n",
            "Epoch 19/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9563 - mean_absolute_error: 1.9563\n",
            "Epoch 19: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9496 - mean_absolute_error: 1.9496 - val_loss: 20.5211 - val_mean_absolute_error: 20.5211\n",
            "Epoch 20/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9312 - mean_absolute_error: 1.9312\n",
            "Epoch 20: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9259 - mean_absolute_error: 1.9259 - val_loss: 20.5265 - val_mean_absolute_error: 20.5265\n",
            "Epoch 21/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8565 - mean_absolute_error: 1.8565\n",
            "Epoch 21: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.8579 - mean_absolute_error: 1.8579 - val_loss: 20.4268 - val_mean_absolute_error: 20.4268\n",
            "Epoch 22/128\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.8021 - mean_absolute_error: 1.8021\n",
            "Epoch 22: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 1.8021 - mean_absolute_error: 1.8021 - val_loss: 20.5778 - val_mean_absolute_error: 20.5778\n",
            "Epoch 23/128\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.0334 - mean_absolute_error: 2.0334\n",
            "Epoch 23: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 2s 60ms/step - loss: 2.0334 - mean_absolute_error: 2.0334 - val_loss: 20.6668 - val_mean_absolute_error: 20.6668\n",
            "Epoch 24/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0393 - mean_absolute_error: 2.0393\n",
            "Epoch 24: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 45ms/step - loss: 2.0363 - mean_absolute_error: 2.0363 - val_loss: 20.6176 - val_mean_absolute_error: 20.6176\n",
            "Epoch 25/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7399 - mean_absolute_error: 1.7399\n",
            "Epoch 25: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.7431 - mean_absolute_error: 1.7431 - val_loss: 20.6309 - val_mean_absolute_error: 20.6309\n",
            "Epoch 26/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7316 - mean_absolute_error: 1.7316\n",
            "Epoch 26: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.7295 - mean_absolute_error: 1.7295 - val_loss: 20.5913 - val_mean_absolute_error: 20.5913\n",
            "Epoch 27/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8513 - mean_absolute_error: 1.8513\n",
            "Epoch 27: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8503 - mean_absolute_error: 1.8503 - val_loss: 20.5926 - val_mean_absolute_error: 20.5926\n",
            "Epoch 28/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8574 - mean_absolute_error: 1.8574\n",
            "Epoch 28: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.8605 - mean_absolute_error: 1.8605 - val_loss: 20.7095 - val_mean_absolute_error: 20.7095\n",
            "Epoch 29/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9800 - mean_absolute_error: 1.9800\n",
            "Epoch 29: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 2.0032 - mean_absolute_error: 2.0032 - val_loss: 20.5914 - val_mean_absolute_error: 20.5914\n",
            "Epoch 30/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0100 - mean_absolute_error: 2.0100\n",
            "Epoch 30: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9970 - mean_absolute_error: 1.9970 - val_loss: 20.5975 - val_mean_absolute_error: 20.5975\n",
            "Epoch 31/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1438 - mean_absolute_error: 2.1438\n",
            "Epoch 31: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.1422 - mean_absolute_error: 2.1422 - val_loss: 20.6122 - val_mean_absolute_error: 20.6122\n",
            "Epoch 32/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8590 - mean_absolute_error: 1.8590\n",
            "Epoch 32: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.8481 - mean_absolute_error: 1.8481 - val_loss: 20.5489 - val_mean_absolute_error: 20.5489\n",
            "Epoch 33/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7345 - mean_absolute_error: 1.7345\n",
            "Epoch 33: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.7660 - mean_absolute_error: 1.7660 - val_loss: 20.5280 - val_mean_absolute_error: 20.5280\n",
            "Epoch 34/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8057 - mean_absolute_error: 1.8057\n",
            "Epoch 34: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.8169 - mean_absolute_error: 1.8169 - val_loss: 20.5149 - val_mean_absolute_error: 20.5149\n",
            "Epoch 35/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8711 - mean_absolute_error: 1.8711\n",
            "Epoch 35: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.8659 - mean_absolute_error: 1.8659 - val_loss: 20.6109 - val_mean_absolute_error: 20.6109\n",
            "Epoch 36/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7491 - mean_absolute_error: 1.7491\n",
            "Epoch 36: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.7428 - mean_absolute_error: 1.7428 - val_loss: 20.5216 - val_mean_absolute_error: 20.5216\n",
            "Epoch 37/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9380 - mean_absolute_error: 1.9380\n",
            "Epoch 37: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9293 - mean_absolute_error: 1.9293 - val_loss: 20.5146 - val_mean_absolute_error: 20.5146\n",
            "Epoch 38/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7625 - mean_absolute_error: 1.7625\n",
            "Epoch 38: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.7661 - mean_absolute_error: 1.7661 - val_loss: 20.4716 - val_mean_absolute_error: 20.4716\n",
            "Epoch 39/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8858 - mean_absolute_error: 1.8858\n",
            "Epoch 39: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8816 - mean_absolute_error: 1.8816 - val_loss: 20.6074 - val_mean_absolute_error: 20.6074\n",
            "Epoch 40/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0398 - mean_absolute_error: 2.0398\n",
            "Epoch 40: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.0404 - mean_absolute_error: 2.0404 - val_loss: 20.6847 - val_mean_absolute_error: 20.6847\n",
            "Epoch 41/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9077 - mean_absolute_error: 1.9077\n",
            "Epoch 41: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9383 - mean_absolute_error: 1.9383 - val_loss: 20.5861 - val_mean_absolute_error: 20.5861\n",
            "Epoch 42/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8927 - mean_absolute_error: 1.8927\n",
            "Epoch 42: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8961 - mean_absolute_error: 1.8961 - val_loss: 20.6135 - val_mean_absolute_error: 20.6135\n",
            "Epoch 43/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8598 - mean_absolute_error: 1.8598\n",
            "Epoch 43: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8495 - mean_absolute_error: 1.8495 - val_loss: 20.5128 - val_mean_absolute_error: 20.5128\n",
            "Epoch 44/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8992 - mean_absolute_error: 1.8992\n",
            "Epoch 44: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8948 - mean_absolute_error: 1.8948 - val_loss: 20.6452 - val_mean_absolute_error: 20.6452\n",
            "Epoch 45/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0501 - mean_absolute_error: 2.0501\n",
            "Epoch 45: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0455 - mean_absolute_error: 2.0455 - val_loss: 20.6396 - val_mean_absolute_error: 20.6396\n",
            "Epoch 46/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9924 - mean_absolute_error: 1.9924\n",
            "Epoch 46: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9855 - mean_absolute_error: 1.9855 - val_loss: 20.6141 - val_mean_absolute_error: 20.6141\n",
            "Epoch 47/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8810 - mean_absolute_error: 1.8810\n",
            "Epoch 47: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8690 - mean_absolute_error: 1.8690 - val_loss: 20.6342 - val_mean_absolute_error: 20.6342\n",
            "Epoch 48/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.2309 - mean_absolute_error: 2.2309\n",
            "Epoch 48: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.2201 - mean_absolute_error: 2.2201 - val_loss: 20.5638 - val_mean_absolute_error: 20.5638\n",
            "Epoch 49/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9478 - mean_absolute_error: 1.9478\n",
            "Epoch 49: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9417 - mean_absolute_error: 1.9417 - val_loss: 20.6321 - val_mean_absolute_error: 20.6321\n",
            "Epoch 50/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9613 - mean_absolute_error: 1.9613\n",
            "Epoch 50: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9933 - mean_absolute_error: 1.9933 - val_loss: 20.6911 - val_mean_absolute_error: 20.6911\n",
            "Epoch 51/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0403 - mean_absolute_error: 2.0403\n",
            "Epoch 51: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.0351 - mean_absolute_error: 2.0351 - val_loss: 20.6517 - val_mean_absolute_error: 20.6517\n",
            "Epoch 52/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9060 - mean_absolute_error: 1.9060\n",
            "Epoch 52: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9065 - mean_absolute_error: 1.9065 - val_loss: 20.6188 - val_mean_absolute_error: 20.6188\n",
            "Epoch 53/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9296 - mean_absolute_error: 1.9296\n",
            "Epoch 53: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9290 - mean_absolute_error: 1.9290 - val_loss: 20.5670 - val_mean_absolute_error: 20.5670\n",
            "Epoch 54/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.2615 - mean_absolute_error: 2.2615\n",
            "Epoch 54: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.2539 - mean_absolute_error: 2.2539 - val_loss: 20.6704 - val_mean_absolute_error: 20.6704\n",
            "Epoch 55/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
            "Epoch 55: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0736 - mean_absolute_error: 2.0736 - val_loss: 20.5207 - val_mean_absolute_error: 20.5207\n",
            "Epoch 56/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0588 - mean_absolute_error: 2.0588\n",
            "Epoch 56: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.0552 - mean_absolute_error: 2.0552 - val_loss: 20.5759 - val_mean_absolute_error: 20.5759\n",
            "Epoch 57/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0563 - mean_absolute_error: 2.0563\n",
            "Epoch 57: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0587 - mean_absolute_error: 2.0587 - val_loss: 20.5609 - val_mean_absolute_error: 20.5609\n",
            "Epoch 58/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9556 - mean_absolute_error: 1.9556\n",
            "Epoch 58: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9642 - mean_absolute_error: 1.9642 - val_loss: 20.6012 - val_mean_absolute_error: 20.6012\n",
            "Epoch 59/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9224 - mean_absolute_error: 1.9224\n",
            "Epoch 59: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9098 - mean_absolute_error: 1.9098 - val_loss: 20.6556 - val_mean_absolute_error: 20.6556\n",
            "Epoch 60/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8947 - mean_absolute_error: 1.8947\n",
            "Epoch 60: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8888 - mean_absolute_error: 1.8888 - val_loss: 20.5710 - val_mean_absolute_error: 20.5710\n",
            "Epoch 61/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8161 - mean_absolute_error: 1.8161\n",
            "Epoch 61: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.8133 - mean_absolute_error: 1.8133 - val_loss: 20.5102 - val_mean_absolute_error: 20.5102\n",
            "Epoch 62/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9089 - mean_absolute_error: 1.9089\n",
            "Epoch 62: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9107 - mean_absolute_error: 1.9107 - val_loss: 20.5051 - val_mean_absolute_error: 20.5051\n",
            "Epoch 63/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0063 - mean_absolute_error: 2.0063\n",
            "Epoch 63: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9938 - mean_absolute_error: 1.9938 - val_loss: 20.6741 - val_mean_absolute_error: 20.6741\n",
            "Epoch 64/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9641 - mean_absolute_error: 1.9641\n",
            "Epoch 64: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9495 - mean_absolute_error: 1.9495 - val_loss: 20.5513 - val_mean_absolute_error: 20.5513\n",
            "Epoch 65/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8975 - mean_absolute_error: 1.8975\n",
            "Epoch 65: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8916 - mean_absolute_error: 1.8916 - val_loss: 20.6358 - val_mean_absolute_error: 20.6358\n",
            "Epoch 66/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8336 - mean_absolute_error: 1.8336\n",
            "Epoch 66: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8351 - mean_absolute_error: 1.8351 - val_loss: 20.5753 - val_mean_absolute_error: 20.5753\n",
            "Epoch 67/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9728 - mean_absolute_error: 1.9728\n",
            "Epoch 67: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 1.9800 - mean_absolute_error: 1.9800 - val_loss: 20.5789 - val_mean_absolute_error: 20.5789\n",
            "Epoch 68/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9315 - mean_absolute_error: 1.9315\n",
            "Epoch 68: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9212 - mean_absolute_error: 1.9212 - val_loss: 20.4930 - val_mean_absolute_error: 20.4930\n",
            "Epoch 69/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8854 - mean_absolute_error: 1.8854\n",
            "Epoch 69: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.8821 - mean_absolute_error: 1.8821 - val_loss: 20.5656 - val_mean_absolute_error: 20.5656\n",
            "Epoch 70/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9306 - mean_absolute_error: 1.9306\n",
            "Epoch 70: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9255 - mean_absolute_error: 1.9255 - val_loss: 20.6229 - val_mean_absolute_error: 20.6229\n",
            "Epoch 71/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9995 - mean_absolute_error: 1.9995\n",
            "Epoch 71: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0056 - mean_absolute_error: 2.0056 - val_loss: 20.8081 - val_mean_absolute_error: 20.8081\n",
            "Epoch 72/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9418 - mean_absolute_error: 1.9418\n",
            "Epoch 72: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9321 - mean_absolute_error: 1.9321 - val_loss: 20.5870 - val_mean_absolute_error: 20.5870\n",
            "Epoch 73/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9514 - mean_absolute_error: 1.9514\n",
            "Epoch 73: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9373 - mean_absolute_error: 1.9373 - val_loss: 20.6057 - val_mean_absolute_error: 20.6057\n",
            "Epoch 74/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0631 - mean_absolute_error: 2.0631\n",
            "Epoch 74: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0608 - mean_absolute_error: 2.0608 - val_loss: 20.6119 - val_mean_absolute_error: 20.6119\n",
            "Epoch 75/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0406 - mean_absolute_error: 2.0406\n",
            "Epoch 75: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0358 - mean_absolute_error: 2.0358 - val_loss: 20.6333 - val_mean_absolute_error: 20.6333\n",
            "Epoch 76/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0358 - mean_absolute_error: 2.0358\n",
            "Epoch 76: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0362 - mean_absolute_error: 2.0362 - val_loss: 20.5645 - val_mean_absolute_error: 20.5645\n",
            "Epoch 77/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9857 - mean_absolute_error: 1.9857\n",
            "Epoch 77: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9730 - mean_absolute_error: 1.9730 - val_loss: 20.5367 - val_mean_absolute_error: 20.5367\n",
            "Epoch 78/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9482 - mean_absolute_error: 1.9482\n",
            "Epoch 78: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9562 - mean_absolute_error: 1.9562 - val_loss: 20.5578 - val_mean_absolute_error: 20.5578\n",
            "Epoch 79/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9107 - mean_absolute_error: 1.9107\n",
            "Epoch 79: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9190 - mean_absolute_error: 1.9190 - val_loss: 20.6835 - val_mean_absolute_error: 20.6835\n",
            "Epoch 80/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0960 - mean_absolute_error: 2.0960\n",
            "Epoch 80: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.1192 - mean_absolute_error: 2.1192 - val_loss: 20.7048 - val_mean_absolute_error: 20.7048\n",
            "Epoch 81/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0317 - mean_absolute_error: 2.0317\n",
            "Epoch 81: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0188 - mean_absolute_error: 2.0188 - val_loss: 20.6048 - val_mean_absolute_error: 20.6048\n",
            "Epoch 82/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8954 - mean_absolute_error: 1.8954\n",
            "Epoch 82: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.8996 - mean_absolute_error: 1.8996 - val_loss: 20.5493 - val_mean_absolute_error: 20.5493\n",
            "Epoch 83/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.2064 - mean_absolute_error: 2.2064\n",
            "Epoch 83: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.2006 - mean_absolute_error: 2.2006 - val_loss: 20.6626 - val_mean_absolute_error: 20.6626\n",
            "Epoch 84/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1760 - mean_absolute_error: 2.1760\n",
            "Epoch 84: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.1678 - mean_absolute_error: 2.1678 - val_loss: 20.5592 - val_mean_absolute_error: 20.5592\n",
            "Epoch 85/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1718 - mean_absolute_error: 2.1718\n",
            "Epoch 85: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.1755 - mean_absolute_error: 2.1755 - val_loss: 20.4952 - val_mean_absolute_error: 20.4952\n",
            "Epoch 86/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0159 - mean_absolute_error: 2.0159\n",
            "Epoch 86: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.0168 - mean_absolute_error: 2.0168 - val_loss: 20.7747 - val_mean_absolute_error: 20.7747\n",
            "Epoch 87/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.3059 - mean_absolute_error: 2.3059\n",
            "Epoch 87: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.2900 - mean_absolute_error: 2.2900 - val_loss: 20.6916 - val_mean_absolute_error: 20.6916\n",
            "Epoch 88/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9674 - mean_absolute_error: 1.9674\n",
            "Epoch 88: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9688 - mean_absolute_error: 1.9688 - val_loss: 20.5057 - val_mean_absolute_error: 20.5057\n",
            "Epoch 89/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9582 - mean_absolute_error: 1.9582\n",
            "Epoch 89: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9606 - mean_absolute_error: 1.9606 - val_loss: 20.5940 - val_mean_absolute_error: 20.5940\n",
            "Epoch 90/128\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.0205 - mean_absolute_error: 2.0205\n",
            "Epoch 90: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.0205 - mean_absolute_error: 2.0205 - val_loss: 20.5832 - val_mean_absolute_error: 20.5832\n",
            "Epoch 91/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0854 - mean_absolute_error: 2.0854\n",
            "Epoch 91: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0824 - mean_absolute_error: 2.0824 - val_loss: 20.6844 - val_mean_absolute_error: 20.6844\n",
            "Epoch 92/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
            "Epoch 92: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0719 - mean_absolute_error: 2.0719 - val_loss: 20.4921 - val_mean_absolute_error: 20.4921\n",
            "Epoch 93/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9282 - mean_absolute_error: 1.9282\n",
            "Epoch 93: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9283 - mean_absolute_error: 1.9283 - val_loss: 20.5968 - val_mean_absolute_error: 20.5968\n",
            "Epoch 94/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1399 - mean_absolute_error: 2.1399\n",
            "Epoch 94: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 2.1330 - mean_absolute_error: 2.1330 - val_loss: 20.6267 - val_mean_absolute_error: 20.6267\n",
            "Epoch 95/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8870 - mean_absolute_error: 1.8870\n",
            "Epoch 95: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.8742 - mean_absolute_error: 1.8742 - val_loss: 20.5032 - val_mean_absolute_error: 20.5032\n",
            "Epoch 96/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9123 - mean_absolute_error: 1.9123\n",
            "Epoch 96: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9303 - mean_absolute_error: 1.9303 - val_loss: 20.4352 - val_mean_absolute_error: 20.4352\n",
            "Epoch 97/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9276 - mean_absolute_error: 1.9276\n",
            "Epoch 97: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9231 - mean_absolute_error: 1.9231 - val_loss: 20.5921 - val_mean_absolute_error: 20.5921\n",
            "Epoch 98/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9680 - mean_absolute_error: 1.9680\n",
            "Epoch 98: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9553 - mean_absolute_error: 1.9553 - val_loss: 20.5139 - val_mean_absolute_error: 20.5139\n",
            "Epoch 99/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9230 - mean_absolute_error: 1.9230\n",
            "Epoch 99: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9122 - mean_absolute_error: 1.9122 - val_loss: 20.6421 - val_mean_absolute_error: 20.6421\n",
            "Epoch 100/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.7567 - mean_absolute_error: 1.7567\n",
            "Epoch 100: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.7546 - mean_absolute_error: 1.7546 - val_loss: 20.5720 - val_mean_absolute_error: 20.5720\n",
            "Epoch 101/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9654 - mean_absolute_error: 1.9654\n",
            "Epoch 101: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9624 - mean_absolute_error: 1.9624 - val_loss: 20.7070 - val_mean_absolute_error: 20.7070\n",
            "Epoch 102/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0781 - mean_absolute_error: 2.0781\n",
            "Epoch 102: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0807 - mean_absolute_error: 2.0807 - val_loss: 20.5918 - val_mean_absolute_error: 20.5918\n",
            "Epoch 103/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0888 - mean_absolute_error: 2.0888\n",
            "Epoch 103: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0813 - mean_absolute_error: 2.0813 - val_loss: 20.6145 - val_mean_absolute_error: 20.6145\n",
            "Epoch 104/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9304 - mean_absolute_error: 1.9304\n",
            "Epoch 104: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9283 - mean_absolute_error: 1.9283 - val_loss: 20.6519 - val_mean_absolute_error: 20.6519\n",
            "Epoch 105/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0071 - mean_absolute_error: 2.0071\n",
            "Epoch 105: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0096 - mean_absolute_error: 2.0096 - val_loss: 20.6149 - val_mean_absolute_error: 20.6149\n",
            "Epoch 106/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9038 - mean_absolute_error: 1.9038\n",
            "Epoch 106: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9092 - mean_absolute_error: 1.9092 - val_loss: 20.5384 - val_mean_absolute_error: 20.5384\n",
            "Epoch 107/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0576 - mean_absolute_error: 2.0576\n",
            "Epoch 107: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0523 - mean_absolute_error: 2.0523 - val_loss: 20.5241 - val_mean_absolute_error: 20.5241\n",
            "Epoch 108/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9277 - mean_absolute_error: 1.9277\n",
            "Epoch 108: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9364 - mean_absolute_error: 1.9364 - val_loss: 20.6735 - val_mean_absolute_error: 20.6735\n",
            "Epoch 109/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.2569 - mean_absolute_error: 2.2569\n",
            "Epoch 109: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.2802 - mean_absolute_error: 2.2802 - val_loss: 20.6509 - val_mean_absolute_error: 20.6509\n",
            "Epoch 110/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.2983 - mean_absolute_error: 2.2983\n",
            "Epoch 110: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.2905 - mean_absolute_error: 2.2905 - val_loss: 20.6822 - val_mean_absolute_error: 20.6822\n",
            "Epoch 111/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9733 - mean_absolute_error: 1.9733\n",
            "Epoch 111: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9717 - mean_absolute_error: 1.9717 - val_loss: 20.4313 - val_mean_absolute_error: 20.4313\n",
            "Epoch 112/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1447 - mean_absolute_error: 2.1447\n",
            "Epoch 112: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.1477 - mean_absolute_error: 2.1477 - val_loss: 20.6175 - val_mean_absolute_error: 20.6175\n",
            "Epoch 113/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9400 - mean_absolute_error: 1.9400\n",
            "Epoch 113: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9494 - mean_absolute_error: 1.9494 - val_loss: 20.5885 - val_mean_absolute_error: 20.5885\n",
            "Epoch 114/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0037 - mean_absolute_error: 2.0037\n",
            "Epoch 114: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0166 - mean_absolute_error: 2.0166 - val_loss: 20.5921 - val_mean_absolute_error: 20.5921\n",
            "Epoch 115/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0664 - mean_absolute_error: 2.0664\n",
            "Epoch 115: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0584 - mean_absolute_error: 2.0584 - val_loss: 20.5266 - val_mean_absolute_error: 20.5266\n",
            "Epoch 116/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.1185 - mean_absolute_error: 2.1185\n",
            "Epoch 116: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.1250 - mean_absolute_error: 2.1250 - val_loss: 20.5520 - val_mean_absolute_error: 20.5520\n",
            "Epoch 117/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0867 - mean_absolute_error: 2.0867\n",
            "Epoch 117: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.0723 - mean_absolute_error: 2.0723 - val_loss: 20.4822 - val_mean_absolute_error: 20.4822\n",
            "Epoch 118/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9952 - mean_absolute_error: 1.9952\n",
            "Epoch 118: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9852 - mean_absolute_error: 1.9852 - val_loss: 20.4963 - val_mean_absolute_error: 20.4963\n",
            "Epoch 119/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.0943 - mean_absolute_error: 2.0943\n",
            "Epoch 119: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 2.0868 - mean_absolute_error: 2.0868 - val_loss: 20.6817 - val_mean_absolute_error: 20.6817\n",
            "Epoch 120/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9149 - mean_absolute_error: 1.9149\n",
            "Epoch 120: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 1.9184 - mean_absolute_error: 1.9184 - val_loss: 20.6567 - val_mean_absolute_error: 20.6567\n",
            "Epoch 121/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8982 - mean_absolute_error: 1.8982\n",
            "Epoch 121: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9053 - mean_absolute_error: 1.9053 - val_loss: 20.5660 - val_mean_absolute_error: 20.5660\n",
            "Epoch 122/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8733 - mean_absolute_error: 1.8733\n",
            "Epoch 122: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 1.8725 - mean_absolute_error: 1.8725 - val_loss: 20.6890 - val_mean_absolute_error: 20.6890\n",
            "Epoch 123/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8794 - mean_absolute_error: 1.8794\n",
            "Epoch 123: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 1.8763 - mean_absolute_error: 1.8763 - val_loss: 20.5970 - val_mean_absolute_error: 20.5970\n",
            "Epoch 124/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9278 - mean_absolute_error: 1.9278\n",
            "Epoch 124: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9260 - mean_absolute_error: 1.9260 - val_loss: 20.5238 - val_mean_absolute_error: 20.5238\n",
            "Epoch 125/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.9257 - mean_absolute_error: 1.9257\n",
            "Epoch 125: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.9275 - mean_absolute_error: 1.9275 - val_loss: 20.5888 - val_mean_absolute_error: 20.5888\n",
            "Epoch 126/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 1.8868 - mean_absolute_error: 1.8868\n",
            "Epoch 126: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 1.8836 - mean_absolute_error: 1.8836 - val_loss: 20.6119 - val_mean_absolute_error: 20.6119\n",
            "Epoch 127/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.3783 - mean_absolute_error: 2.3783\n",
            "Epoch 127: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.3643 - mean_absolute_error: 2.3643 - val_loss: 20.6915 - val_mean_absolute_error: 20.6915\n",
            "Epoch 128/128\n",
            "29/30 [============================>.] - ETA: 0s - loss: 2.3084 - mean_absolute_error: 2.3084\n",
            "Epoch 128: val_loss did not improve from 20.31076\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 2.2947 - mean_absolute_error: 2.2947 - val_loss: 20.5730 - val_mean_absolute_error: 20.5730\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d444fb3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wights_file = \"Weights-095--25.13255.hdf5\"\n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "metadata": {
        "id": "EWSzhJRIFORq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(NN_model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkBqv6H9F1c0",
        "outputId": "b05d7e59-6eac-4484-de0f-1725eb4ab437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 34.48306764142209\n",
            "R2: 0.9480675407323224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig1 = px.line(NN_model.history.history['loss'])\n",
        "fig2 = px.line(NN_model.history.history['root_mean_squared_error'])\n",
        "\n",
        "go.Figure(fig1.data + fig2.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "D15ABdqSeyli",
        "outputId": "100c64cf-8ab3-49dc-fa5b-d53e058b8d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1bbe54f2ede8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mfig2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'root_mean_squared_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'loss'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = NN_model.predict(data_test)\n",
        "pred = pred.reshape(1, -1).tolist()[0]\n",
        "pd.DataFrame({'pred': pred}).to_csv('hackcheek.csv', index=False)\n",
        "# pred"
      ],
      "metadata": {
        "id": "8-NfB2MCF6iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## testing"
      ],
      "metadata": {
        "id": "FzhrL1eLLPWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data_train.columns:\n",
        "    plt.plot(i.std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "aE-xwND9L5AU",
        "outputId": "d80f235d-aacc-4461-a1bf-a63abce2bc3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-202-e0dfb73946a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'count_plot'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.plot.hist(stacked=False, bins=20); "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "G-i2HVMxar5y",
        "outputId": "a80a14b5-0a65-48c4-a787-8f30a236fae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost 2"
      ],
      "metadata": {
        "id": "Yvs_RqINgXiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "train = data_train.copy()"
      ],
      "metadata": {
        "id": "tHYrPqoigSu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter outliers\n",
        "train['cnt'].plot.hist(bins=500, figsize=(16, 7)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "p788QPyVhloT",
        "outputId": "604308f6-ad5f-4cac-d19b-cd8a99e22db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f142b54ad50>"
            ]
          },
          "metadata": {},
          "execution_count": 296
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.query(\"cnt > 500\")['cnt'].plot(figsize=(16, 7), style='.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "PA3ZZdJJjZg2",
        "outputId": "5574dfe6-f4b1-4f9b-94ac-908e45ea531b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1429c4e510>"
            ]
          },
          "metadata": {},
          "execution_count": 317
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.query(\"cnt < 600\").copy()"
      ],
      "metadata": {
        "id": "2VJELWZ5kbf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0tVaCGNgwIGT",
        "outputId": "475a5331-77c2-4422-f75b-7ebdce5e48c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             hr      mnth        dy     atemp        yr  windspeed       hum  \\\n",
              "0     -1.672062 -1.319684 -1.652030 -0.949079 -0.622872  -1.566592  0.918266   \n",
              "1     -1.527378 -1.319684 -1.652030 -1.038688 -0.622872  -1.566592  0.868422   \n",
              "2     -1.382694 -1.319684 -1.652030 -1.038688 -0.622872  -1.566592  0.868422   \n",
              "3     -1.238009 -1.319684 -1.652030 -0.949079 -0.622872  -1.566592  0.619206   \n",
              "4     -1.093325 -1.319684 -1.652030 -0.949079 -0.622872  -1.566592  0.619206   \n",
              "...         ...       ...       ...       ...       ...        ...       ...   \n",
              "11994  1.655675 -0.154476  0.406332  1.015838  1.605465  -0.257063 -0.327818   \n",
              "11995 -1.672062 -0.154476  0.520685  0.569560  1.605465  -0.733183 -0.477348   \n",
              "11996 -1.527378 -0.154476  0.520685  0.479951  1.605465  -1.566592 -0.527191   \n",
              "11997 -1.382694 -0.154476  0.520685  0.479951  1.605465  -1.566592 -0.527191   \n",
              "11998 -1.238009 -0.154476  0.520685  0.390932  1.605465  -0.852013 -0.327818   \n",
              "\n",
              "       weekday      temp  cnt season workingday holiday weathersit  \n",
              "0      1.49147 -1.196932   16      1          0       0          1  \n",
              "1      1.49147 -1.302995   40      1          0       0          1  \n",
              "2      1.49147 -1.302995   32      1          0       0          1  \n",
              "3      1.49147 -1.196932   13      1          0       0          1  \n",
              "4      1.49147 -1.196932    1      1          0       0          1  \n",
              "...        ...       ...  ...    ...        ...     ...        ...  \n",
              "11994  1.49147  0.712188  239      2          0       0          1  \n",
              "11995 -1.50078  0.606125  170      2          0       0          1  \n",
              "11996 -1.50078  0.500063  130      2          0       0          1  \n",
              "11997 -1.50078  0.500063   98      2          0       0          1  \n",
              "11998 -1.50078  0.394001   66      2          0       0          1  \n",
              "\n",
              "[11984 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f89e6738-6049-4230-afc2-a27bac81a4a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>dy</th>\n",
              "      <th>atemp</th>\n",
              "      <th>yr</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>hum</th>\n",
              "      <th>weekday</th>\n",
              "      <th>temp</th>\n",
              "      <th>cnt</th>\n",
              "      <th>season</th>\n",
              "      <th>workingday</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weathersit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.672062</td>\n",
              "      <td>-1.319684</td>\n",
              "      <td>-1.652030</td>\n",
              "      <td>-0.949079</td>\n",
              "      <td>-0.622872</td>\n",
              "      <td>-1.566592</td>\n",
              "      <td>0.918266</td>\n",
              "      <td>1.49147</td>\n",
              "      <td>-1.196932</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.527378</td>\n",
              "      <td>-1.319684</td>\n",
              "      <td>-1.652030</td>\n",
              "      <td>-1.038688</td>\n",
              "      <td>-0.622872</td>\n",
              "      <td>-1.566592</td>\n",
              "      <td>0.868422</td>\n",
              "      <td>1.49147</td>\n",
              "      <td>-1.302995</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.382694</td>\n",
              "      <td>-1.319684</td>\n",
              "      <td>-1.652030</td>\n",
              "      <td>-1.038688</td>\n",
              "      <td>-0.622872</td>\n",
              "      <td>-1.566592</td>\n",
              "      <td>0.868422</td>\n",
              "      <td>1.49147</td>\n",
              "      <td>-1.302995</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.238009</td>\n",
              "      <td>-1.319684</td>\n",
              "      <td>-1.652030</td>\n",
              "      <td>-0.949079</td>\n",
              "      <td>-0.622872</td>\n",
              "      <td>-1.566592</td>\n",
              "      <td>0.619206</td>\n",
              "      <td>1.49147</td>\n",
              "      <td>-1.196932</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.093325</td>\n",
              "      <td>-1.319684</td>\n",
              "      <td>-1.652030</td>\n",
              "      <td>-0.949079</td>\n",
              "      <td>-0.622872</td>\n",
              "      <td>-1.566592</td>\n",
              "      <td>0.619206</td>\n",
              "      <td>1.49147</td>\n",
              "      <td>-1.196932</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11994</th>\n",
              "      <td>1.655675</td>\n",
              "      <td>-0.154476</td>\n",
              "      <td>0.406332</td>\n",
              "      <td>1.015838</td>\n",
              "      <td>1.605465</td>\n",
              "      <td>-0.257063</td>\n",
              "      <td>-0.327818</td>\n",
              "      <td>1.49147</td>\n",
              "      <td>0.712188</td>\n",
              "      <td>239</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11995</th>\n",
              "      <td>-1.672062</td>\n",
              "      <td>-0.154476</td>\n",
              "      <td>0.520685</td>\n",
              "      <td>0.569560</td>\n",
              "      <td>1.605465</td>\n",
              "      <td>-0.733183</td>\n",
              "      <td>-0.477348</td>\n",
              "      <td>-1.50078</td>\n",
              "      <td>0.606125</td>\n",
              "      <td>170</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11996</th>\n",
              "      <td>-1.527378</td>\n",
              "      <td>-0.154476</td>\n",
              "      <td>0.520685</td>\n",
              "      <td>0.479951</td>\n",
              "      <td>1.605465</td>\n",
              "      <td>-1.566592</td>\n",
              "      <td>-0.527191</td>\n",
              "      <td>-1.50078</td>\n",
              "      <td>0.500063</td>\n",
              "      <td>130</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11997</th>\n",
              "      <td>-1.382694</td>\n",
              "      <td>-0.154476</td>\n",
              "      <td>0.520685</td>\n",
              "      <td>0.479951</td>\n",
              "      <td>1.605465</td>\n",
              "      <td>-1.566592</td>\n",
              "      <td>-0.527191</td>\n",
              "      <td>-1.50078</td>\n",
              "      <td>0.500063</td>\n",
              "      <td>98</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11998</th>\n",
              "      <td>-1.238009</td>\n",
              "      <td>-0.154476</td>\n",
              "      <td>0.520685</td>\n",
              "      <td>0.390932</td>\n",
              "      <td>1.605465</td>\n",
              "      <td>-0.852013</td>\n",
              "      <td>-0.327818</td>\n",
              "      <td>-1.50078</td>\n",
              "      <td>0.394001</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11984 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f89e6738-6049-4230-afc2-a27bac81a4a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f89e6738-6049-4230-afc2-a27bac81a4a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f89e6738-6049-4230-afc2-a27bac81a4a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axis = plt.subplots(7, 2, figsize=(16, 20))\n",
        "train.hist(ax=axis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tfQPbp9Fk93H",
        "outputId": "f95b5d01-49b9-4f3f-e07b-4f8f20b806be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f1429baedd0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f142f974f90>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f1429b89ed0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1429b4d410>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f1429b01910>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1429ab9e10>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f1429a7d350>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1429a32910>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f14299e8d90>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f14299ac310>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f1429963890>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1429918d90>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f14298df2d0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f14298937d0>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 319
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x1440 with 14 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop('cnt', axis=1)\n",
        "y = train.cnt\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vSK7DAkdwj0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGBR = xgb.XGBRegressor(\n",
        "    colsample_bylevel=0.8,\n",
        "    colsample_bytree=0.9, \n",
        "    learning_rate=0.01,\n",
        "    max_depth=10, \n",
        "    n_estimators=1000, \n",
        "    seed=20, \n",
        "    subsample=0.6\n",
        ")\n",
        "\n",
        "XGBR.fit(train.drop('cnt', axis=1), train.cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN38WmBQxWLd",
        "outputId": "dda40a2e-67b0-42cb-a929-010c5408a728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06:00:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(colsample_bylevel=0.8, colsample_bytree=0.9, learning_rate=0.01,\n",
              "             max_depth=10, n_estimators=1000, seed=20, subsample=0.6)"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(XGBR, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAkUsPEtypDP",
        "outputId": "c078c28b-6229-437c-826c-699914e6a75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "RMSE: 11.995280237377703\n",
            "R2: 0.9926393575808716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = XGBR.predict(data_test)\n",
        "pd.DataFrame({'pred': pred}).to_csv('hackcheek.csv', index=False)"
      ],
      "metadata": {
        "id": "7Ta59-42zQT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep learning really"
      ],
      "metadata": {
        "id": "UqpYKW8tS2PI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7G6VyPkZS1hh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}